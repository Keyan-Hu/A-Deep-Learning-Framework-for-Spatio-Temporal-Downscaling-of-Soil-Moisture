{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据准备"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 获取降水数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "source_zarr = \"D:\\\\Data_Store\\\\Dataset\\\\Original_Data\\\\0.1_Data.zarr\"\n",
    "output_npy_path = \"D:\\\\Data_Store\\\\Dataset\\\\Reconstruction\\\\37SM\\\\hourlyPrecipRateGC.npy\"\n",
    "ds = xr.open_zarr(source_zarr)\n",
    "hourlyPrecipRateGC = ds['hourlyPrecipRateGC'].values\n",
    "np.save(output_npy_path, hourlyPrecipRateGC)\n",
    "\n",
    "data = np.load(\"D:\\\\Data_Store\\\\Dataset\\\\Reconstruction\\\\37SM\\\\hourlyPrecipRateGC.npy\")\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (弃用) 站点指标cal（单网络）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(r\"C:\\Users\\Administrator\\Desktop\\code\\ST-Conv\")\n",
    "import argparse\n",
    "import torch\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "from Original_Point_Data.tool.origin_utils import OriginUtils\n",
    "from tool.utils import Util\n",
    "from metrics.metrics import CustomMetricCollection\n",
    "\n",
    "def cal_point_data(cal_time, mode, csv_path, reconstruct_path, start_time,find_mode='0.1'):\n",
    "    # 点位信息\n",
    "    ali_list = OriginUtils.find_grid_points(mode, csv_path,find_mode=find_mode)\n",
    "    point_list = [[item[2], item[1], item[0]] for item in ali_list]  # 重新排列为 [Latitude, Longitude, 'Point Name']\n",
    "    point_list.sort(key=lambda x: (-x[0], x[1])) # 纬度降序，经度升序\n",
    "\n",
    "    # 指标字典\n",
    "    metrics_dict = {}\n",
    "    # 数据列表\n",
    "    point_data_list = []\n",
    "    for lat, lon, point in point_list:      ## ！！！！这里似乎逻辑有问题\n",
    "        # 点位数据和lon_ind, lat_ind\n",
    "        point_data = OriginUtils.cal_point_data_specific(ali_list, point)\n",
    "        _, (lat_ind, lon_ind) = OriginUtils.find_grid_coordinates(lon, lat)\n",
    "\n",
    "        # 重建数据并进行时间对齐\n",
    "        reconstruct_data = np.load(reconstruct_path)[:, lat_ind, lon_ind]\n",
    "        reconstruct_data = np.where(reconstruct_data < 0, 0, reconstruct_data)\n",
    "        aligned_data = OriginUtils.align_time_series(point_data, reconstruct_data, start_time)\n",
    "        if cal_time == '12':\n",
    "            aligned_data['Time'] = pd.to_datetime(aligned_data['Time'])\n",
    "            aligned_data = aligned_data[aligned_data['Time'].dt.hour.isin([6, 18])]\n",
    "        point_data_list.append(aligned_data)\n",
    "\n",
    "        # 评价指标\n",
    "        metrics_collector = CustomMetricCollection()\n",
    "        if not aligned_data.empty:\n",
    "            valid_data = aligned_data.dropna(subset=['Point Data SM', 'Reconstructed SM'])\n",
    "            if not valid_data.empty:\n",
    "                output = torch.tensor(valid_data['Reconstructed SM'].values)\n",
    "                target = torch.tensor(valid_data['Point Data SM'].values)\n",
    "                metrics_collector.update(output, target)\n",
    "\n",
    "                metrics_dict[f\"{point}_{lon}_{lat}\"] = metrics_collector.calculate_averages()\n",
    "            else:\n",
    "                metrics_dict[f\"{point}_{lon}_{lat}\"] = {metric: np.nan for metric in CustomMetricCollection().metrics.keys()}\n",
    "        metrics_collector.reset()\n",
    "    return point_list, point_data_list , metrics_dict\n",
    "\n",
    "def main(args):\n",
    "    Original_Point_path_argu = Util.load_config(args.Original_Point_path_config)\n",
    "    path_argu = Util.load_config(args.path_config_path)\n",
    "    return Original_Point_path_argu, path_argu\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--Original_Point_path_config', type=str, default='../Original_Point_Data/config/path_config.yaml')\n",
    "    parser.add_argument('--path_config_path', type=str, default='../config/path_config.yaml')\n",
    "    args = parser.parse_known_args()[0]\n",
    "    Original_Point_path_argu, path_argu = main(args)\n",
    "    \n",
    "    # 初始指标\n",
    "    start_time = '2016-01-01 03:00:00'\n",
    "    mode = 'Shiquanhe'  #（Shiquanhe、Ali、Maqu、Naqu、CTP）\n",
    "    cal_time = '3'\n",
    "    csv_path = Original_Point_path_argu['info']\n",
    "\n",
    "    #reconstruct_path = r\"D:\\Data_Store\\Dataset\\Reconstruction\\RF\\RF_reconstructed_data.npy\"\n",
    "    reconstruct_path = r\"D:\\Data_Store\\Dataset\\Reconstruction\\LSTM\\LSTM1.npy\"\n",
    "    point_list, point_data_list , metrics_dict = cal_point_data(cal_time, mode, csv_path, reconstruct_path, start_time,find_mode='0.1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (弃用) 查看偏移是否更优"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(r\"C:\\Users\\Administrator\\Desktop\\code\\ST-Conv\")\n",
    "import argparse\n",
    "import torch\n",
    "import os\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "from Original_Point_Data.tool.origin_utils import OriginUtils\n",
    "from tool.utils import Util\n",
    "from metrics.metrics import CustomMetricCollection\n",
    "\n",
    "def tensor_to_scalar(value):\n",
    "    \"\"\"如果值是tensor，则转换为标量\"\"\"\n",
    "    if isinstance(value, torch.Tensor):\n",
    "        return value.item()\n",
    "    return value\n",
    "\n",
    "def find_best_shift(output, target, metrics_collector, max_shift=1600):\n",
    "    best_metrics = None\n",
    "    best_shift = 0\n",
    "    for shift in range(max_shift + 1):\n",
    "        if shift > 0:\n",
    "            shifted_output = output[shift:]\n",
    "            shifted_target = target[:len(target) - shift]\n",
    "        else:\n",
    "            shifted_output = output\n",
    "            shifted_target = target\n",
    "        metrics_collector.reset()\n",
    "        metrics_collector.update(shifted_output, shifted_target)\n",
    "        current_metrics = metrics_collector.calculate_averages()\n",
    "        if best_metrics is None or current_metrics['PearsonR'] > best_metrics['PearsonR']:  # 假设以RMSE为优化指标\n",
    "            best_metrics = current_metrics\n",
    "            best_shift = shift\n",
    "    print(f\"Best Shift: {best_shift}\")  # 打印最佳偏移量\n",
    "    return best_metrics, best_shift\n",
    "\n",
    "def cal_all_modes_data(start_time, modes, csv_path, reconstruct_path, cal_time='3', find_mode='0.1', str_param='RF'):\n",
    "    all_metrics_dict = {}\n",
    "    for mode in modes:\n",
    "        ali_list = OriginUtils.find_grid_points(mode, csv_path, find_mode=find_mode)\n",
    "        point_list = [[item[2], item[1], item[0]] for item in ali_list]\n",
    "        point_list.sort(key=lambda x: (-x[0], x[1]))\n",
    "\n",
    "        metrics_dict = {}\n",
    "        for lat, lon, point in point_list:\n",
    "            point_data = OriginUtils.cal_point_data_specific(ali_list, point)\n",
    "            _, (lat_ind, lon_ind) = OriginUtils.find_grid_coordinates(lon, lat)\n",
    "            reconstruct_data = np.load(reconstruct_path)[:, lat_ind, lon_ind]\n",
    "            reconstruct_data = np.where(reconstruct_data < 0, 0, reconstruct_data)\n",
    "            aligned_data = OriginUtils.align_time_series(point_data, reconstruct_data, start_time)\n",
    "            if cal_time == '12':\n",
    "                aligned_data['Time'] = pd.to_datetime(aligned_data['Time'])\n",
    "                aligned_data = aligned_data[aligned_data['Time'].dt.hour.isin([6, 18])]\n",
    "\n",
    "            metrics_collector = CustomMetricCollection()\n",
    "            if not aligned_data.empty:\n",
    "                valid_data = aligned_data.dropna(subset=['Point Data SM', 'Reconstructed SM'])\n",
    "                if not valid_data.empty:\n",
    "                    output = torch.tensor(valid_data['Reconstructed SM'].values, dtype=torch.float32)\n",
    "                    target = torch.tensor(valid_data['Point Data SM'].values, dtype=torch.float32)\n",
    "                    best_metrics, best_shift = find_best_shift(output, target, metrics_collector)\n",
    "                    metrics_dict[f\"{point}_{lon}_{lat}\"] = {k: tensor_to_scalar(v) for k, v in best_metrics.items()}\n",
    "                else:\n",
    "                    metrics_dict[f\"{point}_{lon}_{lat}\"] = {metric: np.nan for metric in CustomMetricCollection().metrics.keys()}\n",
    "            metrics_collector.reset()\n",
    "\n",
    "        all_metrics_dict[mode] = metrics_dict\n",
    "\n",
    "    folder_path = os.path.dirname(reconstruct_path)\n",
    "    filename = f\"{str_param}_station_metrics_cor.xlsx\"\n",
    "    full_path = os.path.join(folder_path, filename)\n",
    "\n",
    "    with pd.ExcelWriter(full_path) as writer:\n",
    "        for mode, metrics in all_metrics_dict.items():\n",
    "            df = pd.DataFrame.from_dict(metrics, orient='index').reset_index()\n",
    "            df.columns = ['站点名'] + list(metrics[next(iter(metrics))].keys())  # 重置列名以包括站点名\n",
    "            df.to_excel(writer, sheet_name=mode, index=False)\n",
    "\n",
    "def main(args):\n",
    "    Original_Point_path_argu = Util.load_config(args.Original_Point_path_config)\n",
    "    path_argu = Util.load_config(args.path_config_path)\n",
    "    return Original_Point_path_argu, path_argu\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--Original_Point_path_config', type=str, default='../Original_Point_Data/config/path_config.yaml')\n",
    "    parser.add_argument('--path_config_path', type=str, default='../config/path_config.yaml')\n",
    "    args = parser.parse_known_args()[0]\n",
    "    Original_Point_path_argu, path_argu = main(args)\n",
    "    \n",
    "    start_time = '2016-01-01 03:00:00'\n",
    "    modes = ['Shiquanhe', 'Ali', 'Maqu', 'Naqu', 'CTP']  # 所有模式\n",
    "    cal_time = '3'\n",
    "    csv_path = Original_Point_path_argu['info']\n",
    "    reconstruct_path_lstm = r\"D:\\Data_Store\\Dataset\\Reconstruction\\LSTM\\LSTM1.npy\"\n",
    "    reconstruct_path_rf = r\"D:\\Data_Store\\Dataset\\Reconstruction\\RF\\RF_reconstructed_data.npy\"\n",
    "    file_path1 = \"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RES\\spatial_resnet34_temporal_usage_position_embedding_init256.npy\"\n",
    "    file_path2 = \"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RES\\spatial_unet_temporal_usage_position_embedding_init256.npy\"\n",
    "    file_path3 = \"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RES\\ST_resnet34_temporal_usage_position_embedding_init256.npy\"\n",
    "    file_path4 = \"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RES\\ST_unet_temporal_usage_position_embedding_init256.npy\"\n",
    "    file_path5 = \"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RES\\ST_ViT_serial_temporal_usage_position_embedding_individual_init256.n16.npy\"\n",
    "    file_path6 = r\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RES\\temporal_temporal_usage_position_embedding.npy\"\n",
    "    # 调用函数处理所有模式并保存数据\n",
    "    cal_all_modes_data(start_time, modes, csv_path, reconstruct_path_lstm, cal_time, find_mode='0.1',str_param='LSTM')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 站点指标cal（所有网络）-获取全网络指标.XLSX文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(r\"C:\\Users\\Administrator\\Desktop\\code\\ST-Conv\")\n",
    "import argparse\n",
    "import torch\n",
    "import os\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "from Original_Point_Data.tool.origin_utils import OriginUtils\n",
    "from tool.utils import Util\n",
    "from metrics.metrics import CustomMetricCollection\n",
    "\n",
    "def tensor_to_scalar(value):\n",
    "    \"\"\"如果值是tensor，则转换为标量\"\"\"\n",
    "    if isinstance(value, torch.Tensor):\n",
    "        return value.item()\n",
    "    return value\n",
    "\n",
    "def cal_all_modes_data(start_time, modes, csv_path, reconstruct_path, cal_time='3', find_mode='0.1', str_param='RF'):\n",
    "    all_metrics_dict = {}\n",
    "    for mode in modes:  # 迭代处理每个模式\n",
    "        # 点位信息\n",
    "        ali_list = OriginUtils.find_grid_points(mode, csv_path, find_mode=find_mode)\n",
    "        point_list = [[item[2], item[1], item[0]] for item in ali_list]\n",
    "        point_list.sort(key=lambda x: (-x[0], x[1]))  # 纬度降序，经度升序\n",
    "\n",
    "        metrics_dict = {}  # 对每个模式重置指标字典\n",
    "\n",
    "        for lat, lon, point in point_list:\n",
    "            point_data = OriginUtils.cal_point_data_specific(ali_list, point)\n",
    "            _, (lat_ind, lon_ind) = OriginUtils.find_grid_coordinates(lon, lat, find_mode=find_mode)\n",
    "\n",
    "            reconstruct_data = np.load(reconstruct_path)[:, lat_ind, lon_ind]\n",
    "            reconstruct_data = np.where(reconstruct_data < 0, 0, reconstruct_data)\n",
    "            aligned_data = OriginUtils.align_time_series(point_data, reconstruct_data, start_time)\n",
    "            if cal_time == '12':\n",
    "                aligned_data['Time'] = pd.to_datetime(aligned_data['Time'])\n",
    "                aligned_data = aligned_data[aligned_data['Time'].dt.hour.isin([6, 18])]\n",
    "\n",
    "            metrics_collector = CustomMetricCollection()\n",
    "            if not aligned_data.empty:\n",
    "                valid_data = aligned_data.dropna(subset=['Point Data SM', 'Reconstructed SM'])\n",
    "                if not valid_data.empty:\n",
    "                    output = torch.tensor(valid_data['Reconstructed SM'].values)\n",
    "                    target = torch.tensor(valid_data['Point Data SM'].values)\n",
    "                    metrics_collector.update(output, target)\n",
    "\n",
    "                    # 将所有tensor对象转换为标量值\n",
    "                    metrics = metrics_collector.calculate_averages()\n",
    "                    metrics = {k: tensor_to_scalar(v) for k, v in metrics.items()}\n",
    "                    metrics_dict[f\"{point}_{lon}_{lat}\"] = metrics\n",
    "                else:\n",
    "                    metrics_dict[f\"{point}_{lon}_{lat}\"] = {metric: np.nan for metric in CustomMetricCollection().metrics.keys()}\n",
    "            metrics_collector.reset()\n",
    "\n",
    "        all_metrics_dict[mode] = metrics_dict  # 将当前模式的metrics字典保存到总字典中\n",
    "\n",
    "    folder_path = os.path.dirname(reconstruct_path)\n",
    "\n",
    "    # 保存到CSV\n",
    "    filename = f\"{str_param}_station_metrics.xlsx\"  # 构造文件名\n",
    "    full_path = os.path.join(folder_path, filename)  # 将文件夹路径和文件名拼接起来\n",
    "\n",
    "    with pd.ExcelWriter(full_path) as writer:\n",
    "        for mode, metrics in all_metrics_dict.items():\n",
    "            # 调整DataFrame以确保站点名称作为第一列显示\n",
    "            df = pd.DataFrame(metrics).T.reset_index()\n",
    "            df.rename(columns={'index': '站点名'}, inplace=True)\n",
    "            df.to_excel(writer, sheet_name=mode, index=False)  # 每个模式一个子表\n",
    "\n",
    "def main(args):\n",
    "    Original_Point_path_argu = Util.load_config(args.Original_Point_path_config)\n",
    "    path_argu = Util.load_config(args.path_config_path)\n",
    "    return Original_Point_path_argu, path_argu\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--Original_Point_path_config', type=str, default='../Original_Point_Data/config/path_config.yaml')\n",
    "    parser.add_argument('--path_config_path', type=str, default='../config/path_config.yaml')\n",
    "    args = parser.parse_known_args()[0]\n",
    "    Original_Point_path_argu, path_argu = main(args)\n",
    "    \n",
    "    start_time = '2016-01-01 03:00:00'\n",
    "    modes = ['Shiquanhe', 'Ali', 'Maqu', 'Naqu', 'CTP']  # 所有模式\n",
    "    cal_time = '3'\n",
    "    csv_path = Original_Point_path_argu['info']\n",
    "    Original_path = r\"D:\\Data_Store\\Dataset\\Reconstruction\\37SM\\soil_moisture.npy\"\n",
    "    reconstruct_path_lstm = r\"D:\\Data_Store\\Dataset\\Reconstruction\\LSTM\\LSTM1.npy\"\n",
    "    reconstruct_path_rf = r\"D:\\Data_Store\\Dataset\\Reconstruction\\RF\\RF_reconstructed_data.npy\"\n",
    "    reconstruct_path_STPSCFENet = r\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\ST_PSC-FENet.npy\"\n",
    "    reconstruct_path_SPPSCFENet = r\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\SP_PSC-FENet.npy\"\n",
    "    reconstruct_path_STPSCFENet36 = r\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\ST_PSC-FENet36.npy\"\n",
    "    reconstruct_path_SPPSCFENet36 = r\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\SP_PSC-FENet36.npy\"\n",
    "    reconstruct_path_SPRESNET = r\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\SP_ResNet.npy\"\n",
    "    reconstruct_path_SPUNET = r\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\SP_UNet.npy\"\n",
    "    reconstruct_path_STRESNET = r\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\ST_ResNet.npy\"\n",
    "    reconstruct_path_STUNET = r\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\ST_UNet.npy\"\n",
    "    reconstruct_path_STVIT = r\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\ST_ViT.npy\"\n",
    "    reconstruct_path_SPVIT = r\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\SP_ViT.npy\"\n",
    "    reconstruct_path_TEMPORAL = r\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\Temporal.npy\"\n",
    "\n",
    "    reconstruction_paths = {\n",
    "    'PSC-FENet-SP': r\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\SP_PSC-FENet.npy\",\n",
    "    'PSC-FENet-ST': r\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\ST_PSC-FENet.npy\",\n",
    "    'PSC-FENet-SP36': r\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\SP_PSC-FENet36.npy\",\n",
    "    'PSC-FENet-ST36': r\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\ST_PSC-FENet36.npy\",\n",
    "    'LSTM': r\"D:\\Data_Store\\Dataset\\Reconstruction\\LSTM\\LSTM1.npy\",\n",
    "    'RF': r\"D:\\Data_Store\\Dataset\\Reconstruction\\RF\\RF_reconstructed_data.npy\",\n",
    "    'ResNet-SP': r\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\SP_ResNet.npy\",\n",
    "    'ResNet-ST': r\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\ST_ResNet.npy\",\n",
    "    'UNet-SP': r\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\SP_UNet.npy\",\n",
    "    'UNet-ST': r\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\ST_UNet.npy\",\n",
    "    'VIT-SP': r\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\SP_ViT.npy\",\n",
    "    'VIT-ST': r\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\ST_ViT.npy\",\n",
    "    'Temporal': r\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\Temporal.npy\"\n",
    "}\n",
    "\n",
    "    # KPNET重建数据\n",
    "    #cal_all_modes_data(start_time, modes, csv_path, reconstruct_path_STPSCFENet, cal_time, find_mode='0.1', str_param='STPSC-FENet')\n",
    "    #cal_all_modes_data(start_time, modes, csv_path, reconstruct_path_SPPSCFENet, cal_time, find_mode='0.1', str_param='SPPSC-FENet')\n",
    "    cal_all_modes_data(start_time, modes, csv_path, reconstruct_path_STPSCFENet36, cal_time, find_mode='0.1', str_param='STPSC-FENet36')\n",
    "    cal_all_modes_data(start_time, modes, csv_path, reconstruct_path_SPPSCFENet36, cal_time, find_mode='0.1', str_param='SPPSC-FENet36')\n",
    "    # RF重建数据\n",
    "    #cal_all_modes_data(start_time, modes, csv_path, reconstruct_path_rf, cal_time, find_mode='0.1', str_param='RF')\n",
    "    # LSTM重建数据\n",
    "    #cal_all_modes_data(start_time, modes, csv_path, reconstruct_path_lstm, cal_time, find_mode='0.1', str_param='LSTM')\n",
    "    # RESNET重建数据\n",
    "    #cal_all_modes_data(start_time, modes, csv_path, reconstruct_path_SPRESNET, cal_time, find_mode='0.1', str_param='SPRESNET')\n",
    "    #cal_all_modes_data(start_time, modes, csv_path, reconstruct_path_STRESNET, cal_time, find_mode='0.1', str_param='STRESNET')\n",
    "    # UNET重建数据\n",
    "    #cal_all_modes_data(start_time, modes, csv_path, reconstruct_path_SPUNET, cal_time, find_mode='0.1', str_param='SPUNET')\n",
    "    #cal_all_modes_data(start_time, modes, csv_path, reconstruct_path_STUNET, cal_time, find_mode='0.1', str_param='STUNET')\n",
    "    # VIT重建数据\n",
    "    #cal_all_modes_data(start_time, modes, csv_path, reconstruct_path_STVIT, cal_time, find_mode='0.1', str_param='STVIT')\n",
    "    #cal_all_modes_data(start_time, modes, csv_path, reconstruct_path_SPVIT, cal_time, find_mode='0.1', str_param='SPVIT')\n",
    "    # TEMPORAL重建数据\n",
    "    #cal_all_modes_data(start_time, modes, csv_path, reconstruct_path_TEMPORAL, cal_time, find_mode='0.1', str_param='TEMPORAL')\n",
    "    # 卫星数据\n",
    "    #cal_all_modes_data(start_time, modes, csv_path, Original_path, cal_time, find_mode='0.37', str_param='Original')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 计算降尺度指标Gdown、GEFFI、GPREC、GACCU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(r\"C:\\Users\\Administrator\\Desktop\\code\\ST-Conv\")\n",
    "import argparse\n",
    "import torch\n",
    "import os\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "from Original_Point_Data.tool.origin_utils import OriginUtils\n",
    "from tool.utils import Util\n",
    "from metrics.metrics import SoilMoistureMetrics\n",
    "import copy\n",
    "\n",
    "def tensor_to_scalar(value):\n",
    "    \"\"\"如果值是tensor，则转换为标量\"\"\"\n",
    "    if isinstance(value, torch.Tensor):\n",
    "        return value.item()\n",
    "    return value\n",
    "\n",
    "def cal_all_modes_data(start_time, modes, csv_path, Original_path, reconstruct_path, cal_time='3', find_mode='0.1', str_param='RF'):\n",
    "    all_metrics_dict = {}\n",
    "    for mode in modes:  # 迭代处理每个模式\n",
    "        # 点位信息\n",
    "        ali_list = OriginUtils.find_grid_points(mode, csv_path, find_mode=find_mode)\n",
    "        point_list = [[item[2], item[1], item[0]] for item in ali_list]\n",
    "        point_list.sort(key=lambda x: (-x[0], x[1]))  # 纬度降序，经度升序\n",
    "\n",
    "        metrics_dict = {}  # 对每个模式重置指标字典\n",
    "\n",
    "        for lat, lon, point in point_list:\n",
    "            point_data = OriginUtils.cal_point_data_specific(ali_list, point)\n",
    "            _, (lat_ind, lon_ind) = OriginUtils.find_grid_coordinates(lon, lat, find_mode=find_mode)\n",
    "            _, (lat_ind_37, lon_ind_37) = OriginUtils.find_grid_coordinates(lon, lat, find_mode='0.37')\n",
    "\n",
    "            reconstruct_data = np.load(reconstruct_path)[:, lat_ind, lon_ind]\n",
    "            reconstruct_data = np.where(reconstruct_data < 0, 0, reconstruct_data)\n",
    "\n",
    "            Original_data = np.load(Original_path)[:, lat_ind_37, lon_ind_37]\n",
    "            Original_data = np.where(Original_data < 0, 0, Original_data)\n",
    "\n",
    "            aligned_data = OriginUtils.align_time_series(point_data, reconstruct_data, start_time)\n",
    "            aligned_Original_data = OriginUtils.align_time_series(point_data, Original_data, start_time)\n",
    "\n",
    "            aligned_Original_data.rename(columns={'Reconstructed SM': 'Original SM'}, inplace=True)\n",
    "            merged_data = pd.merge(aligned_data, aligned_Original_data[['Time', 'Original SM']], on='Time', how='outer')\n",
    "\n",
    "            if not merged_data.empty:\n",
    "                merged_data.dropna(subset=['Point Data SM', 'Reconstructed SM', 'Original SM'], inplace=True)\n",
    "                metrics_collector = SoilMoistureMetrics()\n",
    "                if not merged_data.empty:\n",
    "                    hr = merged_data['Reconstructed SM'].values.astype(np.float32)\n",
    "                    lr = merged_data['Original SM'].values.astype(np.float32)\n",
    "                    is_data = merged_data['Point Data SM'].values.astype(np.float32)\n",
    "\n",
    "                    metrics_collector.update(hr, lr, is_data)\n",
    "                    metrics = metrics_collector.get_metrics()\n",
    "                    metrics_dict[f\"{point}_{lon}_{lat}\"] = {key: (val[0] if isinstance(val, list) and len(val) == 1 else np.nan if not val else val) \n",
    "                                                           for key, val in copy.deepcopy(metrics).items()}\n",
    "\n",
    "                metrics_collector.reset()\n",
    "\n",
    "        all_metrics_dict[mode] = metrics_dict\n",
    "\n",
    "    folder_path = os.path.dirname(reconstruct_path)\n",
    "    filename = f\"{str_param}_station_G_metrics.xlsx\"\n",
    "    full_path = os.path.join(folder_path, filename)\n",
    "\n",
    "    with pd.ExcelWriter(full_path) as writer:\n",
    "        for mode, metrics in all_metrics_dict.items():\n",
    "            # 调整DataFrame以确保站点名称作为第一列显示\n",
    "            df = pd.DataFrame(metrics).T.reset_index()\n",
    "            df.rename(columns={'index': '站点名'}, inplace=True)\n",
    "            df.to_excel(writer, sheet_name=mode, index=False)\n",
    "\n",
    "def main(args):\n",
    "    Original_Point_path_argu = Util.load_config(args.Original_Point_path_config)\n",
    "    path_argu = Util.load_config(args.path_config_path)\n",
    "    return Original_Point_path_argu, path_argu\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--Original_Point_path_config', type=str, default='../Original_Point_Data/config/path_config.yaml')\n",
    "    parser.add_argument('--path_config_path', type=str, default='../config/path_config.yaml')\n",
    "    args = parser.parse_known_args()[0]\n",
    "    Original_Point_path_argu, path_argu = main(args)\n",
    "    \n",
    "    start_time = '2016-01-01 03:00:00'\n",
    "    modes = ['Shiquanhe', 'Ali', 'Maqu', 'Naqu', 'CTP']  # 所有模式\n",
    "    cal_time = '3'\n",
    "    csv_path = Original_Point_path_argu['info']\n",
    "    Original_path = r\"D:\\Data_Store\\Dataset\\Reconstruction\\37SM\\soil_moisture.npy\"\n",
    "    reconstruct_path_lstm = r\"D:\\Data_Store\\Dataset\\Reconstruction\\LSTM\\LSTM1.npy\"\n",
    "    reconstruct_path_rf = r\"D:\\Data_Store\\Dataset\\Reconstruction\\RF\\RF_reconstructed_data.npy\"\n",
    "    reconstruct_path_STPSCFENet = r\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\ST_PSC-FENet.npy\"\n",
    "    reconstruct_path_SPPSCFENet = r\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\SP_PSC-FENet.npy\"\n",
    "    reconstruct_path_STPSCFENet36 = r\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\ST_PSC-FENet36.npy\"\n",
    "    reconstruct_path_SPPSCFENet36 = r\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\SP_PSC-FENet36.npy\"\n",
    "    reconstruct_path_SPRESNET = r\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\SP_ResNet.npy\"\n",
    "    reconstruct_path_SPUNET = r\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\SP_UNet.npy\"\n",
    "    reconstruct_path_STRESNET = r\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\ST_ResNet.npy\"\n",
    "    reconstruct_path_STUNET = r\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\ST_UNet.npy\"\n",
    "    reconstruct_path_STVIT = r\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\ST_ViT.npy\"\n",
    "    reconstruct_path_SPVIT = r\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\SP_ViT.npy\"\n",
    "    reconstruct_path_TEMPORAL = r\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\Temporal.npy\"\n",
    "\n",
    "    # KPNET重建数据\n",
    "    #cal_all_modes_data(start_time, modes, csv_path, Original_path, reconstruct_path_STPSCFENet, cal_time, find_mode='0.1', str_param='STPSC-FENet')\n",
    "    #cal_all_modes_data(start_time, modes, csv_path, Original_path, reconstruct_path_SPPSCFENet, cal_time, find_mode='0.1', str_param='SPPSC-FENet')\n",
    "    cal_all_modes_data(start_time, modes, csv_path, Original_path, reconstruct_path_STPSCFENet36, cal_time, find_mode='0.1', str_param='STPSC-FENet36')\n",
    "    cal_all_modes_data(start_time, modes, csv_path, Original_path, reconstruct_path_SPPSCFENet36, cal_time, find_mode='0.1', str_param='SPPSC-FENet36')\n",
    "    # RF重建数据\n",
    "    #cal_all_modes_data(start_time, modes, csv_path, Original_path, reconstruct_path_rf, cal_time, find_mode='0.1', str_param='RF')\n",
    "    # LSTM重建数据\n",
    "    #cal_all_modes_data(start_time, modes, csv_path, Original_path, reconstruct_path_lstm, cal_time, find_mode='0.1', str_param='LSTM')\n",
    "    # RESNET重建数据\n",
    "    #cal_all_modes_data(start_time, modes, csv_path, Original_path, reconstruct_path_SPRESNET, cal_time, find_mode='0.1', str_param='SPRESNET')\n",
    "    #cal_all_modes_data(start_time, modes, csv_path, Original_path, reconstruct_path_STRESNET, cal_time, find_mode='0.1', str_param='STRESNET')\n",
    "    # UNET重建数据\n",
    "    #cal_all_modes_data(start_time, modes, csv_path, Original_path, reconstruct_path_SPUNET, cal_time, find_mode='0.1', str_param='SPUNET')\n",
    "    #cal_all_modes_data(start_time, modes, csv_path, Original_path, reconstruct_path_STUNET, cal_time, find_mode='0.1', str_param='STUNET')\n",
    "    # VIT重建数据\n",
    "    #cal_all_modes_data(start_time, modes, csv_path, Original_path, reconstruct_path_STVIT, cal_time, find_mode='0.1', str_param='STVIT')\n",
    "    #cal_all_modes_data(start_time, modes, csv_path, Original_path, reconstruct_path_SPVIT, cal_time, find_mode='0.1', str_param='SPVIT')\n",
    "    # TEMPORAL重建数据\n",
    "    #cal_all_modes_data(start_time, modes, csv_path, Original_path, reconstruct_path_TEMPORAL, cal_time, find_mode='0.1', str_param='TEMPORAL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(r\"C:\\Users\\Administrator\\Desktop\\code\\ST-Conv\")\n",
    "import argparse\n",
    "import torch\n",
    "import os\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "from Original_Point_Data.tool.origin_utils import OriginUtils\n",
    "from tool.utils import Util\n",
    "from metrics.metrics import CustomMetricCollection\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "def main():\n",
    "    # 加载数据\n",
    "    data_path = r\"D:\\Data_Store\\Dataset\\Reconstruction\\37SM\\soil_moisture.npy\"\n",
    "    data = np.load(data_path)\n",
    "\n",
    "    # 计算非NaN的数据数目\n",
    "    non_nan_counts = np.sum(~np.isnan(data), axis=0)  # 在时间维度上求和非NaN值\n",
    "\n",
    "    # 绘制非NaN数据数目\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    im = plt.imshow(non_nan_counts, cmap='viridis', origin='lower')\n",
    "    plt.colorbar(im, label='Non-NaN Count')\n",
    "    plt.title('Non-NaN Data Counts at Each Latitude and Longitude')\n",
    "    plt.xlabel('Longitude Index')\n",
    "    plt.ylabel('Latitude Index')\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (弃用，多时间尺度指标计算中已算过) 计算12hour的时频数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(r\"C:\\Users\\Administrator\\Desktop\\code\\ST-Conv\")\n",
    "import argparse\n",
    "import torch\n",
    "import os\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "from Original_Point_Data.tool.origin_utils import OriginUtils\n",
    "from tool.utils import Util\n",
    "from metrics.metrics import CustomMetricCollection\n",
    "\n",
    "def tensor_to_scalar(value):\n",
    "    \"\"\"如果值是tensor，则转换为标量\"\"\"\n",
    "    if isinstance(value, torch.Tensor):\n",
    "        return value.item()\n",
    "    return value\n",
    "\n",
    "def cal_all_modes_data(start_time, modes, csv_path, reconstruct_path, cal_time='3', find_mode='0.1', str_param='RF'):\n",
    "    all_metrics_dict = {}\n",
    "    for mode in modes:  # 迭代处理每个模式\n",
    "        # 点位信息\n",
    "        ali_list = OriginUtils.find_grid_points(mode, csv_path, find_mode=find_mode)\n",
    "        point_list = [[item[2], item[1], item[0]] for item in ali_list]\n",
    "        point_list.sort(key=lambda x: (-x[0], x[1]))  # 纬度降序，经度升序\n",
    "\n",
    "        metrics_dict = {}  # 对每个模式重置指标字典\n",
    "\n",
    "        for lat, lon, point in point_list:\n",
    "            point_data = OriginUtils.cal_point_data_specific(ali_list, point)\n",
    "            _, (lat_ind, lon_ind) = OriginUtils.find_grid_coordinates(lon, lat, find_mode=find_mode)\n",
    "\n",
    "            reconstruct_data = np.load(reconstruct_path)[:, lat_ind, lon_ind]\n",
    "            reconstruct_data = np.where(reconstruct_data < 0, 0, reconstruct_data)\n",
    "            aligned_data = OriginUtils.align_time_series(point_data, reconstruct_data, start_time)\n",
    "            if cal_time == '12':\n",
    "                aligned_data['Time'] = pd.to_datetime(aligned_data['Time'])\n",
    "                aligned_data = aligned_data[aligned_data['Time'].dt.hour.isin([6, 18])]\n",
    "\n",
    "            metrics_collector = CustomMetricCollection()\n",
    "            if not aligned_data.empty:\n",
    "                valid_data = aligned_data.dropna(subset=['Point Data SM', 'Reconstructed SM'])\n",
    "                if not valid_data.empty:\n",
    "                    output = torch.tensor(valid_data['Reconstructed SM'].values)\n",
    "                    target = torch.tensor(valid_data['Point Data SM'].values)\n",
    "                    metrics_collector.update(output, target)\n",
    "\n",
    "                    # 将所有tensor对象转换为标量值\n",
    "                    metrics = metrics_collector.calculate_averages()\n",
    "                    metrics = {k: tensor_to_scalar(v) for k, v in metrics.items()}\n",
    "                    metrics_dict[f\"{point}_{lon}_{lat}\"] = metrics\n",
    "                else:\n",
    "                    metrics_dict[f\"{point}_{lon}_{lat}\"] = {metric: np.nan for metric in CustomMetricCollection().metrics.keys()}\n",
    "            metrics_collector.reset()\n",
    "\n",
    "        all_metrics_dict[mode] = metrics_dict  # 将当前模式的metrics字典保存到总字典中\n",
    "\n",
    "    folder_path = os.path.dirname(reconstruct_path)\n",
    "\n",
    "    # 保存到CSV\n",
    "    filename = f\"{str_param}_station_metrics_12.xlsx\"  # 构造文件名\n",
    "    full_path = os.path.join(folder_path, filename)  # 将文件夹路径和文件名拼接起来\n",
    "\n",
    "    with pd.ExcelWriter(full_path) as writer:\n",
    "        for mode, metrics in all_metrics_dict.items():\n",
    "            # 调整DataFrame以确保站点名称作为第一列显示\n",
    "            df = pd.DataFrame(metrics).T.reset_index()\n",
    "            df.rename(columns={'index': '站点名'}, inplace=True)\n",
    "            df.to_excel(writer, sheet_name=mode, index=False)  # 每个模式一个子表\n",
    "\n",
    "def main(args):\n",
    "    Original_Point_path_argu = Util.load_config(args.Original_Point_path_config)\n",
    "    path_argu = Util.load_config(args.path_config_path)\n",
    "    return Original_Point_path_argu, path_argu\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--Original_Point_path_config', type=str, default='../Original_Point_Data/config/path_config.yaml')\n",
    "    parser.add_argument('--path_config_path', type=str, default='../config/path_config.yaml')\n",
    "    args = parser.parse_known_args()[0]\n",
    "    Original_Point_path_argu, path_argu = main(args)\n",
    "    \n",
    "    start_time = '2016-01-01 03:00:00'\n",
    "    modes = ['Shiquanhe', 'Ali', 'Maqu', 'Naqu', 'CTP']  # 所有模式\n",
    "    cal_time = '12'\n",
    "    csv_path = Original_Point_path_argu['info']\n",
    "    Original_path = r\"D:\\Data_Store\\Dataset\\Reconstruction\\37SM\\soil_moisture.npy\"\n",
    "    reconstruct_path_lstm = r\"D:\\Data_Store\\Dataset\\Reconstruction\\LSTM\\LSTM1.npy\"\n",
    "    reconstruct_path_rf = r\"D:\\Data_Store\\Dataset\\Reconstruction\\RF\\RF_reconstructed_data.npy\"\n",
    "    reconstruct_path_STPSCFENet = r\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\ST_PSC-FENet.npy\"\n",
    "    reconstruct_path_SPPSCFENet = r\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\SP_PSC-FENet.npy\"\n",
    "    reconstruct_path_SPRESNET = r\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\SP_ResNet.npy\"\n",
    "    reconstruct_path_SPUNET = r\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\SP_UNet.npy\"\n",
    "    reconstruct_path_STRESNET = r\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\ST_ResNet.npy\"\n",
    "    reconstruct_path_STUNET = r\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\ST_UNet.npy\"\n",
    "    reconstruct_path_STVIT = r\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\ST_ViT.npy\"\n",
    "    reconstruct_path_SPVIT = r\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\SP_ViT.npy\"\n",
    "    reconstruct_path_TEMPORAL = r\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\Temporal.npy\"\n",
    "\n",
    "    # KPNET重建数据\n",
    "    cal_all_modes_data(start_time, modes, csv_path, reconstruct_path_STPSCFENet, cal_time, find_mode='0.1', str_param='STPSCFENet')\n",
    "    cal_all_modes_data(start_time, modes, csv_path, reconstruct_path_SPPSCFENet, cal_time, find_mode='0.1', str_param='SPPSCFENet')\n",
    "    # RF重建数据\n",
    "    cal_all_modes_data(start_time, modes, csv_path, reconstruct_path_rf, cal_time, find_mode='0.1', str_param='RF')\n",
    "    # LSTM重建数据\n",
    "    cal_all_modes_data(start_time, modes, csv_path, reconstruct_path_lstm, cal_time, find_mode='0.1', str_param='LSTM')\n",
    "    # RESNET重建数据\n",
    "    cal_all_modes_data(start_time, modes, csv_path, reconstruct_path_SPRESNET, cal_time, find_mode='0.1', str_param='SPRESNET')\n",
    "    cal_all_modes_data(start_time, modes, csv_path, reconstruct_path_STRESNET, cal_time, find_mode='0.1', str_param='STRESNET')\n",
    "    # UNET重建数据\n",
    "    cal_all_modes_data(start_time, modes, csv_path, reconstruct_path_SPUNET, cal_time, find_mode='0.1', str_param='SPUNET')\n",
    "    cal_all_modes_data(start_time, modes, csv_path, reconstruct_path_STUNET, cal_time, find_mode='0.1', str_param='STUNET')\n",
    "    # VIT重建数据\n",
    "    cal_all_modes_data(start_time, modes, csv_path, reconstruct_path_STVIT, cal_time, find_mode='0.1', str_param='STVIT')\n",
    "    cal_all_modes_data(start_time, modes, csv_path, reconstruct_path_SPVIT, cal_time, find_mode='0.1', str_param='SPVIT')\n",
    "    # TEMPORAL重建数据\n",
    "    cal_all_modes_data(start_time, modes, csv_path, reconstruct_path_TEMPORAL, cal_time, find_mode='0.1', str_param='TEMPORAL')\n",
    "    # 卫星数据 \n",
    "    cal_all_modes_data(start_time, modes, csv_path, Original_path, cal_time, find_mode='0.37', str_param='Original')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(r\"C:\\Users\\Administrator\\Desktop\\code\\ST-Conv\")\n",
    "import argparse\n",
    "import torch\n",
    "import os\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "from Original_Point_Data.tool.origin_utils import OriginUtils\n",
    "from tool.utils import Util\n",
    "from metrics.metrics import CustomMetricCollection\n",
    "\n",
    "def tensor_to_scalar(value):\n",
    "    \"\"\"如果值是tensor，则转换为标量\"\"\"\n",
    "    if isinstance(value, torch.Tensor):\n",
    "        return value.item()\n",
    "    return value\n",
    "\n",
    "def cal_all_modes_data(start_time, modes, csv_path, reconstruct_path, cal_time='3', find_mode='0.1', str_param='RF'):\n",
    "    all_metrics_dict = {}\n",
    "    for mode in modes:  # 迭代处理每个模式\n",
    "        # 点位信息\n",
    "        ali_list = OriginUtils.find_grid_points(mode, csv_path, find_mode=find_mode)\n",
    "        point_list = [[item[2], item[1], item[0]] for item in ali_list]\n",
    "        point_list.sort(key=lambda x: (-x[0], x[1]))  # 纬度降序，经度升序\n",
    "\n",
    "        metrics_dict = {}  # 对每个模式重置指标字典\n",
    "\n",
    "        for lat, lon, point in point_list:\n",
    "            point_data = OriginUtils.cal_point_data_specific(ali_list, point)\n",
    "            _, (lat_ind, lon_ind) = OriginUtils.find_grid_coordinates(lon, lat, find_mode=find_mode)\n",
    "\n",
    "            reconstruct_data = np.load(reconstruct_path)[:, lat_ind, lon_ind]\n",
    "            reconstruct_data = np.where(reconstruct_data < 0, 0, reconstruct_data)\n",
    "            aligned_data = OriginUtils.align_time_series(point_data, reconstruct_data, start_time)\n",
    "            if cal_time == '12':\n",
    "                aligned_data['Time'] = pd.to_datetime(aligned_data['Time'])\n",
    "                aligned_data = aligned_data[aligned_data['Time'].dt.hour.isin([6, 18])]\n",
    "\n",
    "            metrics_collector = CustomMetricCollection()\n",
    "            if not aligned_data.empty:\n",
    "                valid_data = aligned_data.dropna(subset=['Point Data SM', 'Reconstructed SM'])\n",
    "                if not valid_data.empty:\n",
    "                    output = torch.tensor(valid_data['Reconstructed SM'].values)\n",
    "                    target = torch.tensor(valid_data['Point Data SM'].values)\n",
    "                    metrics_collector.update(output, target)\n",
    "\n",
    "                    # 将所有tensor对象转换为标量值\n",
    "                    metrics = metrics_collector.calculate_averages()\n",
    "                    metrics = {k: tensor_to_scalar(v) for k, v in metrics.items()}\n",
    "                    metrics_dict[f\"{point}_{lon}_{lat}\"] = metrics\n",
    "                else:\n",
    "                    metrics_dict[f\"{point}_{lon}_{lat}\"] = {metric: np.nan for metric in CustomMetricCollection().metrics.keys()}\n",
    "            metrics_collector.reset()\n",
    "\n",
    "        all_metrics_dict[mode] = metrics_dict  # 将当前模式的metrics字典保存到总字典中\n",
    "\n",
    "    folder_path = os.path.dirname(reconstruct_path)\n",
    "\n",
    "    # 保存到CSV\n",
    "    filename = f\"{str_param}_station_metrics.xlsx\"  # 构造文件名\n",
    "    full_path = os.path.join(folder_path, filename)  # 将文件夹路径和文件名拼接起来\n",
    "\n",
    "    with pd.ExcelWriter(full_path) as writer:\n",
    "        for mode, metrics in all_metrics_dict.items():\n",
    "            # 调整DataFrame以确保站点名称作为第一列显示\n",
    "            df = pd.DataFrame(metrics).T.reset_index()\n",
    "            df.rename(columns={'index': '站点名'}, inplace=True)\n",
    "            df.to_excel(writer, sheet_name=mode, index=False)  # 每个模式一个子表\n",
    "\n",
    "def main(args):\n",
    "    Original_Point_path_argu = Util.load_config(args.Original_Point_path_config)\n",
    "    path_argu = Util.load_config(args.path_config_path)\n",
    "    return Original_Point_path_argu, path_argu\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--Original_Point_path_config', type=str, default='../Original_Point_Data/config/path_config.yaml')\n",
    "    parser.add_argument('--path_config_path', type=str, default='../config/path_config.yaml')\n",
    "    args = parser.parse_known_args()[0]\n",
    "    Original_Point_path_argu, path_argu = main(args)\n",
    "    \n",
    "    start_time = '2016-01-01 03:00:00'\n",
    "    modes = ['Shiquanhe', 'Ali', 'Maqu', 'Naqu', 'CTP']  # 所有模式\n",
    "    cal_time = '3'\n",
    "    csv_path = Original_Point_path_argu['info']\n",
    "    Original_path = r\"D:\\Data_Store\\Dataset\\Reconstruction\\37SM\\soil_moisture.npy\"\n",
    "    reconstruct_path_lstm = r\"D:\\Data_Store\\Dataset\\Reconstruction\\LSTM\\LSTM1.npy\"\n",
    "    reconstruct_path_rf = r\"D:\\Data_Store\\Dataset\\Reconstruction\\RF\\RF_reconstructed_data.npy\"\n",
    "    reconstruct_path_KPNET = r\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RES\\ST_kpnet_Encoder_lite_serial_temporal_usage_position_embedding_individual_init256_11_121_131_dropout_rate0.3168.npy\"\n",
    "    reconstruct_path_SPKPNET = r\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RES\\spatial_kpnet_Encoder_lite_serial_temporal_usage_position_embedding_individual_init256168.npy\"   \n",
    "    reconstruct_path_SPRESNET = r\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RES\\spatial_resnet34_temporal_usage_position_embedding_init256168.npy\"\n",
    "    reconstruct_path_SPUNET = r\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RES\\spatial_unet_temporal_usage_position_embedding_init256168.npy\"\n",
    "    reconstruct_path_STRESNET = r\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RES\\ST_resnet34_temporal_usage_position_embedding_init256168.npy\"\n",
    "    reconstruct_path_STUNET = r\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RES\\ST_unet_temporal_usage_position_embedding_init256168.npy\"\n",
    "    reconstruct_path_STVIT = r\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RES\\ST_ViT_serial_temporal_usage_position_embedding_individual_init256168.npy\"\n",
    "    reconstruct_path_SPVIT = r\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RES\\spatial_VIT_serial_temporal_usage_position_embedding_individual_init256168.npy\"  \n",
    "    reconstruct_path_TEMPORAL = r\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RES\\temporal_temporal_usage_position_embedding168.npy\"\n",
    "\n",
    "    # KPNET重建数据\n",
    "    cal_all_modes_data(start_time, modes, csv_path, reconstruct_path_KPNET, cal_time, find_mode='0.1', str_param='STKPNET')\n",
    "    cal_all_modes_data(start_time, modes, csv_path, reconstruct_path_SPKPNET, cal_time, find_mode='0.1', str_param='SPKPNET')\n",
    "    # RF重建数据\n",
    "    cal_all_modes_data(start_time, modes, csv_path, reconstruct_path_rf, cal_time, find_mode='0.1', str_param='RF')\n",
    "    # LSTM重建数据\n",
    "    cal_all_modes_data(start_time, modes, csv_path, reconstruct_path_lstm, cal_time, find_mode='0.1', str_param='LSTM')\n",
    "    # RESNET重建数据\n",
    "    cal_all_modes_data(start_time, modes, csv_path, reconstruct_path_SPRESNET, cal_time, find_mode='0.1', str_param='SPRESNET')\n",
    "    cal_all_modes_data(start_time, modes, csv_path, reconstruct_path_STRESNET, cal_time, find_mode='0.1', str_param='STRESNET')\n",
    "    # UNET重建数据\n",
    "    cal_all_modes_data(start_time, modes, csv_path, reconstruct_path_SPUNET, cal_time, find_mode='0.1', str_param='SPUNET')\n",
    "    cal_all_modes_data(start_time, modes, csv_path, reconstruct_path_STUNET, cal_time, find_mode='0.1', str_param='STUNET')\n",
    "    # VIT重建数据\n",
    "    cal_all_modes_data(start_time, modes, csv_path, reconstruct_path_STVIT, cal_time, find_mode='0.1', str_param='STVIT')\n",
    "    cal_all_modes_data(start_time, modes, csv_path, reconstruct_path_SPVIT, cal_time, find_mode='0.1', str_param='SPVIT')\n",
    "    # TEMPORAL重建数据\n",
    "    cal_all_modes_data(start_time, modes, csv_path, reconstruct_path_TEMPORAL, cal_time, find_mode='0.1', str_param='TEMPORAL')\n",
    "    # 卫星数据 \n",
    "    cal_all_modes_data(start_time, modes, csv_path, Original_path, cal_time, find_mode='0.37', str_param='Original')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 站点多时间尺度指标（获取全网络全时刻指标.xlsx文件）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(r\"C:\\Users\\Administrator\\Desktop\\code\\ST-Conv\")\n",
    "import argparse\n",
    "import torch\n",
    "import os\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "from Original_Point_Data.tool.origin_utils import OriginUtils\n",
    "from tool.utils import Util\n",
    "from metrics.metrics import CustomMetricCollection\n",
    "\n",
    "def tensor_to_scalar(value):\n",
    "    \"\"\"如果值是tensor，则转换为标量\"\"\"\n",
    "    if isinstance(value, torch.Tensor):\n",
    "        return value.item()\n",
    "    return value\n",
    "\n",
    "def cal_all_modes_data_time_specific(start_time, modes, csv_path, reconstruct_path, find_mode='0.1', str_param='RF'):\n",
    "    # 定义时间点\n",
    "    time_points = [0, 3, 6, 9, 12, 15, 18, 21]\n",
    "    all_metrics_dict = {}\n",
    "    \n",
    "    for mode in modes:\n",
    "        ali_list = OriginUtils.find_grid_points(mode, csv_path, find_mode='0.1')\n",
    "        point_list = [[item[2], item[1], item[0]] for item in ali_list]\n",
    "        point_list.sort(key=lambda x: (-x[0], x[1]))\n",
    "\n",
    "        for time_point in time_points:\n",
    "            metrics_dict = {}\n",
    "            \n",
    "            for lat, lon, point in point_list:\n",
    "                point_data = OriginUtils.cal_point_data_specific(ali_list, point)\n",
    "                _, (lat_ind, lon_ind) = OriginUtils.find_grid_coordinates(lon, lat, find_mode='0.1')\n",
    "\n",
    "                reconstruct_data = np.load(reconstruct_path)[:, lat_ind, lon_ind]\n",
    "                reconstruct_data = np.where(reconstruct_data < 0, 0, reconstruct_data)\n",
    "                aligned_data = OriginUtils.align_time_series(point_data, reconstruct_data, start_time)\n",
    "\n",
    "                # 提取特定时间点的数据\n",
    "                aligned_data['Time'] = pd.to_datetime(aligned_data['Time'])\n",
    "                specific_time_data = aligned_data[aligned_data['Time'].dt.hour.isin([time_point])]\n",
    "\n",
    "                metrics_collector = CustomMetricCollection()\n",
    "                if not specific_time_data.empty:\n",
    "                    valid_data = specific_time_data.dropna(subset=['Point Data SM', 'Reconstructed SM'])\n",
    "                    if not valid_data.empty:\n",
    "                        output = torch.tensor(valid_data['Reconstructed SM'].values)\n",
    "                        target = torch.tensor(valid_data['Point Data SM'].values)\n",
    "                        metrics_collector.update(output, target)\n",
    "\n",
    "                        metrics = metrics_collector.calculate_averages()\n",
    "                        metrics = {k: tensor_to_scalar(v) for k, v in metrics.items()}\n",
    "                        metrics_dict[f\"{point}_{lon}_{lat}\"] = metrics\n",
    "                    else:\n",
    "                        metrics_dict[f\"{point}_{lon}_{lat}\"] = {metric: np.nan for metric in CustomMetricCollection().metrics.keys()}\n",
    "                metrics_collector.reset()\n",
    "\n",
    "            all_metrics_dict[f\"{mode}_{time_point}\"] = metrics_dict\n",
    "\n",
    "    folder_path = os.path.dirname(reconstruct_path)\n",
    "\n",
    "    # 保存到Excel\n",
    "    filename = f\"{str_param}_station_time_metrics.xlsx\"\n",
    "    full_path = os.path.join(folder_path, filename)\n",
    "\n",
    "    with pd.ExcelWriter(full_path) as writer:\n",
    "        for time_point, metrics in all_metrics_dict.items():\n",
    "            df = pd.DataFrame(metrics).T.reset_index()\n",
    "            df.rename(columns={'index': '站点名'}, inplace=True)\n",
    "            df.to_excel(writer, sheet_name=f\"{time_point}h\", index=False)\n",
    "\n",
    "    return all_metrics_dict\n",
    "\n",
    "def main(args):\n",
    "    Original_Point_path_argu = Util.load_config(args.Original_Point_path_config)\n",
    "    path_argu = Util.load_config(args.path_config_path)\n",
    "    return Original_Point_path_argu, path_argu\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--Original_Point_path_config', type=str, default='../Original_Point_Data/config/path_config.yaml')\n",
    "    parser.add_argument('--path_config_path', type=str, default='../config/path_config.yaml')\n",
    "    args = parser.parse_known_args()[0]\n",
    "    Original_Point_path_argu, path_argu = main(args)\n",
    "    \n",
    "    start_time = '2016-01-01 03:00:00'\n",
    "    modes = ['Shiquanhe', 'Ali', 'Maqu', 'Naqu', 'CTP']  # 所有模式\n",
    "    cal_time = '3'\n",
    "    csv_path = Original_Point_path_argu['info']\n",
    "    Original_path = r\"D:\\Data_Store\\Dataset\\Reconstruction\\37SM\\soil_moisture.npy\"\n",
    "    reconstruct_path_lstm = r\"D:\\Data_Store\\Dataset\\Reconstruction\\LSTM\\LSTM1.npy\"\n",
    "    reconstruct_path_rf = r\"D:\\Data_Store\\Dataset\\Reconstruction\\RF\\RF_reconstructed_data.npy\"\n",
    "    reconstruct_path_STPSCFENet = r\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\ST_PSC-FENet.npy\"\n",
    "    reconstruct_path_SPPSCFENet = r\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\SP_PSC-FENet.npy\"\n",
    "    reconstruct_path_STPSCFENet36 = r\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\ST_PSC-FENet36.npy\"\n",
    "    reconstruct_path_SPPSCFENet36 = r\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\SP_PSC-FENet36.npy\"\n",
    "    reconstruct_path_SPRESNET = r\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\SP_ResNet.npy\"\n",
    "    reconstruct_path_SPUNET = r\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\SP_UNet.npy\"\n",
    "    reconstruct_path_STRESNET = r\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\ST_ResNet.npy\"\n",
    "    reconstruct_path_STUNET = r\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\ST_UNet.npy\"\n",
    "    reconstruct_path_STVIT = r\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\ST_ViT.npy\"\n",
    "    reconstruct_path_SPVIT = r\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\SP_ViT.npy\"\n",
    "    reconstruct_path_TEMPORAL = r\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\Temporal.npy\"\n",
    "\n",
    "\n",
    "    # KPNET重建数据\n",
    "    #cal_all_modes_data_time_specific(start_time, modes, csv_path, reconstruct_path_STPSCFENet, find_mode='0.1', str_param='STPSCFENet')\n",
    "    #cal_all_modes_data_time_specific(start_time, modes, csv_path, reconstruct_path_SPPSCFENet, find_mode='0.1', str_param='SPPSCFENet')\n",
    "    cal_all_modes_data_time_specific(start_time, modes, csv_path, reconstruct_path_STPSCFENet36, find_mode='0.1', str_param='STPSCFENet36')\n",
    "    cal_all_modes_data_time_specific(start_time, modes, csv_path, reconstruct_path_SPPSCFENet36, find_mode='0.1', str_param='SPPSCFENet36')\n",
    "    # RF重建数据\n",
    "    #cal_all_modes_data_time_specific(start_time, modes, csv_path, reconstruct_path_rf, find_mode='0.1', str_param='RF')\n",
    "    # LSTM重建数据\n",
    "    #cal_all_modes_data_time_specific(start_time, modes, csv_path, reconstruct_path_lstm, find_mode='0.1', str_param='LSTM')\n",
    "    # RESNET重建数据\n",
    "    #cal_all_modes_data_time_specific(start_time, modes, csv_path, reconstruct_path_SPRESNET, find_mode='0.1', str_param='SPRESNET')\n",
    "    #cal_all_modes_data_time_specific(start_time, modes, csv_path, reconstruct_path_STRESNET, find_mode='0.1', str_param='STRESNET')\n",
    "    # UNET重建数据\n",
    "    #cal_all_modes_data_time_specific(start_time, modes, csv_path, reconstruct_path_SPUNET, find_mode='0.1', str_param='SPUNET')\n",
    "    #cal_all_modes_data_time_specific(start_time, modes, csv_path, reconstruct_path_STUNET, find_mode='0.1', str_param='STUNET')\n",
    "    # VIT重建数据\n",
    "    #cal_all_modes_data_time_specific(start_time, modes, csv_path, reconstruct_path_STVIT, find_mode='0.1', str_param='STVIT')\n",
    "    #cal_all_modes_data_time_specific(start_time, modes, csv_path, reconstruct_path_SPVIT, find_mode='0.1', str_param='SPVIT')\n",
    "    # TEMPORAL重建数据\n",
    "    #cal_all_modes_data_time_specific(start_time, modes, csv_path, reconstruct_path_TEMPORAL, find_mode='0.1', str_param='TEMPORAL')\n",
    "    # 卫星数据 \n",
    "    #cal_all_modes_data_time_specific(start_time, modes, csv_path, Original_path, find_mode='0.37', str_param='Original')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 绘制散点密度图(未用到)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import gaussian_kde\n",
    "from sklearn.metrics import explained_variance_score,r2_score,median_absolute_error,mean_squared_error,mean_absolute_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from matplotlib.font_manager import FontProperties\n",
    "from scipy import stats\n",
    "from matplotlib import rcParams\n",
    "from statistics import mean\n",
    "from scipy.stats import pearsonr,t\n",
    "\n",
    "def preprocess_and_merge_data(all_networks_data):\n",
    "    merged_data = pd.DataFrame()\n",
    "\n",
    "    for network, data in all_networks_data.items():\n",
    "        relevant_data = data[['Original_Data'] + [col for col in data.columns if col not in ['Original_Data', 'Precipitation']]]\n",
    "        relevant_data = relevant_data.dropna(subset=['Original_Data'])\n",
    "        if merged_data.empty:\n",
    "            merged_data = relevant_data\n",
    "        else:\n",
    "            merged_data = merged_data.reset_index(drop=True)\n",
    "            relevant_data = relevant_data.reset_index(drop=True)\n",
    "            merged_data = pd.concat([merged_data, relevant_data], axis=0, ignore_index=True)\n",
    "    return merged_data\n",
    "\n",
    "\n",
    "\n",
    "def plot_density_scatter(x_data, y_data, title, xlabel, ylabel, text_fontsize=14):\n",
    "    # 设置字体\n",
    "    font = FontProperties()\n",
    "    font.set_family('Times New Roman')\n",
    "    font.set_size(text_fontsize)\n",
    "    font_prop = FontProperties(family='Times New Roman', size=14)\n",
    "    # 准备数据\n",
    "    df = pd.DataFrame({'x': x_data, 'y': y_data}).dropna()\n",
    "    x, y = df['x'], df['y']\n",
    "    xy = np.vstack([x, y])\n",
    "    z = gaussian_kde(xy)(xy)\n",
    "    idx = z.argsort()\n",
    "    x, y, z = x.iloc[idx], y.iloc[idx], z[idx]\n",
    "\n",
    "    # 计算拟合线参数\n",
    "    def slope(xs, ys):\n",
    "        m = (((np.mean(xs) * np.mean(ys)) - np.mean(xs * ys)) /\n",
    "             ((np.mean(xs) * np.mean(xs)) - np.mean(xs * xs)))\n",
    "        b = np.mean(ys) - m * np.mean(xs)\n",
    "        return m, b\n",
    "    k, b = slope(x, y)\n",
    "\n",
    "    # 构建拟合线和95%置信区间线\n",
    "    regression_line = k * x + b\n",
    "\n",
    "    # 计算统计指标\n",
    "    BIAS = np.mean(x - y)\n",
    "    MSE = mean_squared_error(x, y)\n",
    "    RMSE = np.sqrt(MSE)\n",
    "    ubRMSE = np.sqrt(np.sum((y - np.mean(y))**2)/ len(y))\n",
    "    R2 = pearsonr(x, y).statistic\n",
    "    n = len(y)\n",
    "    # 95置信带\n",
    "    n = 1  # 数据点的数量\n",
    "    t_value = 1.96  # 95%置信区间对应的t值\n",
    "    y_fit = k * x + b  # 拟合的y值\n",
    "    std_err = np.std(y - y_fit) / np.sqrt(n)  # 标准误差\n",
    "    margin_of_error = t_value * std_err  # 误差幅度\n",
    "\n",
    "    # 计算置信区间\n",
    "    lower_confidence_bound = y_fit - margin_of_error\n",
    "    upper_confidence_bound = y_fit + margin_of_error\n",
    "\n",
    "    # 计算x轴的最小值和最大值及其padding\n",
    "    min_val_x = x.min()\n",
    "    max_val_x = x.max()\n",
    "    padding_x = (max_val_x - min_val_x) * 0.01  # 添加10%的padding以避免数据点紧贴边缘\n",
    "\n",
    "    # 计算y轴的最小值和最大值及其padding\n",
    "    min_val_y = y.min()\n",
    "    max_val_y = y.max()\n",
    "    padding_y = (max_val_y - min_val_y) * 0.01  # 添加10%的padding以避免数据点紧贴边缘\n",
    "\n",
    "    # 绘图\n",
    "    fig, ax = plt.subplots(figsize=(8, 6), dpi=300)\n",
    "    #ax.plot(x, lower_confidence_bound, linestyle='--', color='black', label='95% Prediction Band')\n",
    "    #ax.plot(x, upper_confidence_bound, linestyle='--', color='black')\n",
    "\n",
    "    # 使用蓝色色系的颜色映射来表示数据点的密度\n",
    "    scatter = ax.scatter(x, y, marker='o', c=z, edgecolors=None, s=15, cmap='RdBu_r',  alpha=0.8)\n",
    "    cbar = plt.colorbar(scatter, shrink=1, orientation='vertical', extend='both', pad=0.015, aspect=30, label='Frequency')\n",
    "\n",
    "    # 绘制回归线和1:1线\n",
    "    ax.plot(x, regression_line, 'black', lw=1.5, label='Regression Line')\n",
    "    ax.plot([min_val_x - padding_x, max_val_x + padding_x], [min_val_y - padding_y, max_val_y + padding_y], 'red', lw=1.5, linestyle='--', label='1:1 Line')\n",
    "\n",
    "    # 设置图形的范围\n",
    "    ax.axis([min_val_x - padding_x, max_val_x + padding_x, min_val_y - padding_y, max_val_y + padding_y])\n",
    "\n",
    "    # 在右下角添加统计指标文本\n",
    "    text_position_x = max_val_x - padding_x * 2\n",
    "    text_position_y = min_val_y + padding_y * 2\n",
    "\n",
    "    step = 0.018\n",
    "\n",
    "    # 使用FontProperties实例绘制文本，去除$符号避免斜体\n",
    "    ax.text(text_position_x, text_position_y, f'R²={R2:.3f}', fontproperties=font_prop, horizontalalignment='right', verticalalignment='bottom')\n",
    "    ax.text(text_position_x, text_position_y + step, f'RMSE={RMSE:.3f}', fontproperties=font_prop, horizontalalignment='right', verticalalignment='bottom')\n",
    "    ax.text(text_position_x, text_position_y + 2*step, f'ubRMSE={ubRMSE:.3f}', fontproperties=font_prop, horizontalalignment='right', verticalalignment='bottom')\n",
    "    ax.text(text_position_x, text_position_y + 3*step, f'bias={BIAS:.3f}', fontproperties=font_prop, horizontalalignment='right', verticalalignment='bottom')\n",
    "    ax.text(text_position_x, text_position_y + 4*step, f'N={n}', fontproperties=font_prop, horizontalalignment='right', verticalalignment='bottom')\n",
    "\n",
    "    #ax.set_title(title, fontproperties=font,fontsize=16)\n",
    "    ax.set_xlabel(xlabel, fontproperties=font,fontsize=14)\n",
    "    ax.set_ylabel(ylabel, fontproperties=font,fontsize=14)\n",
    "\n",
    "    for label in ax.get_xticklabels() + ax.get_yticklabels():\n",
    "        label.set_fontsize(12)\n",
    "    ax.legend(loc='upper left', frameon=False)\n",
    "    ax.grid(True, linestyle='--', alpha=0.2)\n",
    "    plt.legend(prop=font_prop, loc='upper left', fancybox=False, shadow=False,frameon=False)\n",
    "    plt.show()\n",
    "processed_data = preprocess_and_merge_data(all_networks_data)\n",
    "for method in processed_data.columns:\n",
    "    if method != 'Satellite_Data':# 'Satellite_Data''Original_Data'\n",
    "        plot_density_scatter(processed_data['Satellite_Data'], processed_data[method], f'{method}', 'Satellite_Data($m^3/m^{-3}$)', f'{method}($m^3/m^{-3}$)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SMAP卫星原位验证图-密度散点图指标图、二维指标图、重建结果图"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (数据处理) 0.1 降尺度产品下采样 -得到与卫星数据一致的数据lon，lat，times\n",
    "为确保数据匹配准确性，每次绘图之前需正确设置selected_model_names，在后续中selected_model_names可能会多次修改\n",
    "二维指标图只绘制 KPNET-SP、LSTM、RF、Temporal "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "\n",
    "def adjust_boundaries(lat_min, lat_max, lon_min, lon_max, lat_points, lon_points):\n",
    "    lat_step = (lat_max - lat_min) / (lat_points - 1)\n",
    "    lon_step = (lon_max - lon_min) / (lon_points - 1)\n",
    "\n",
    "    lat_min_adjusted = lat_min - lat_step / 2\n",
    "    lat_max_adjusted = lat_max + lat_step / 2\n",
    "    lon_min_adjusted = lon_min - lon_step / 2\n",
    "    lon_max_adjusted = lon_max + lon_step / 2\n",
    "\n",
    "    return lat_min_adjusted, lat_max_adjusted, lon_min_adjusted, lon_max_adjusted\n",
    "def map_high_res_to_low_res(high_res_params, low_res_params):\n",
    "    high_lat_min, high_lat_max, high_lon_min, high_lon_max, high_lat_points, high_lon_points = high_res_params\n",
    "    low_lat_min, low_lat_max, low_lon_min, low_lon_max, low_lat_points, low_lon_points = low_res_params\n",
    "    \n",
    "    high_lat_linspace = np.linspace(high_lat_min, high_lat_max, high_lat_points)\n",
    "    high_lon_linspace = np.linspace(high_lon_min, high_lon_max, high_lon_points)\n",
    "    high_lon_grid, high_lat_grid = np.meshgrid(high_lon_linspace, high_lat_linspace)\n",
    "    \n",
    "    low_lat_linspace = np.linspace(low_lat_min, low_lat_max, low_lat_points)\n",
    "    low_lon_linspace = np.linspace(low_lon_min, low_lon_max, low_lon_points)\n",
    "    low_lon_grid, low_lat_grid = np.meshgrid(low_lon_linspace, low_lat_linspace)\n",
    "    \n",
    "    mapping_matrix = np.zeros((high_lat_points, high_lon_points, 2), dtype=int)\n",
    "\n",
    "    for i in range(high_lat_points):\n",
    "        for j in range(high_lon_points):\n",
    "            high_lon, high_lat = high_lon_grid[i, j], high_lat_grid[i, j]\n",
    "            dist = (low_lon_grid - high_lon) ** 2 + (low_lat_grid - high_lat) ** 2\n",
    "            lat_ind, lon_ind = np.unravel_index(np.argmin(dist, axis=None), dist.shape)\n",
    "            mapping_matrix[i, j] = [lat_ind, lon_ind]\n",
    "    \n",
    "    return mapping_matrix\n",
    "\n",
    "def aggregate_to_low_res2(high_res_data, mapping_matrix, low_res_shape):\n",
    "    low_res_data = np.zeros((high_res_data.shape[0], low_res_shape[0], low_res_shape[1]))\n",
    "    \n",
    "    for t in range(high_res_data.shape[0]):\n",
    "        accumulator = np.zeros(low_res_shape)\n",
    "        counter = np.zeros(low_res_shape, dtype=int)\n",
    "        for i in range(mapping_matrix.shape[0]):\n",
    "            for j in range(mapping_matrix.shape[1]):\n",
    "                low_i, low_j = mapping_matrix[i, j]\n",
    "                accumulator[low_i, low_j] += high_res_data[t, i, j]\n",
    "                counter[low_i, low_j] += 1\n",
    "\n",
    "        low_res_data[t] = np.where(counter > 0, accumulator / counter, 0)\n",
    "    \n",
    "    return low_res_data\n",
    "def aggregate_to_low_res(high_res_data, mapping_matrix, low_res_shape):\n",
    "    \"\"\"\n",
    "    将高分辨率数据汇总到低分辨率网格。\n",
    "    :param high_res_data: 高分辨率数据，形状为 (time, lat, lon)\n",
    "    :param mapping_matrix: 映射矩阵，形状为 (high_lat, high_lon, 2)\n",
    "    :param low_res_shape: 低分辨率数据的形状 (lat, lon)\n",
    "    :return: 聚合后的低分辨率数据\n",
    "    \"\"\"\n",
    "    low_res_data = np.zeros((high_res_data.shape[0], low_res_shape[0], low_res_shape[1]))\n",
    "    low_i_indices, low_j_indices = mapping_matrix[:, :, 0], mapping_matrix[:, :, 1]\n",
    "\n",
    "    for t in range(high_res_data.shape[0]):\n",
    "        np.add.at(low_res_data[t], (low_i_indices, low_j_indices), high_res_data[t])\n",
    "        counter = np.zeros(low_res_shape, dtype=int)\n",
    "        np.add.at(counter, (low_i_indices, low_j_indices), 1)\n",
    "        low_res_data[t] = np.where(counter > 0, low_res_data[t] / counter, 0)\n",
    "\n",
    "    return low_res_data\n",
    "\n",
    "def generate_clipping_mask(lon_grid, lat_grid, shp_path):\n",
    "    \"\"\"\n",
    "    使用.shp文件生成数据掩码：在边界外的数据掩码为0，在边界内的数据掩码为1。\n",
    "    \"\"\"\n",
    "    gdf = gpd.read_file(shp_path)\n",
    "    poly = gdf.unary_union\n",
    "    mask = np.zeros_like(lon_grid, dtype=bool)\n",
    "\n",
    "    for i in range(lon_grid.shape[0]):\n",
    "        for j in range(lon_grid.shape[1]):\n",
    "            point = Point(lon_grid[i, j], lat_grid[i, j])\n",
    "            if point.within(poly):\n",
    "                mask[i, j] = True\n",
    "\n",
    "    return mask\n",
    "\n",
    "# 建立高分数据到低分数据映射矩阵\n",
    "high_res_params = (25.550117, 40.450184, 66.95031, 104.95048, 150, 381)\n",
    "low_res_params = (25.293533, 40.604774, 66.84705, 105.31184, 42, 104)\n",
    "high_lat_min_adj, high_lat_max_adj, high_lon_min_adj, high_lon_max_adj = adjust_boundaries(*high_res_params)\n",
    "low_lat_min_adj, low_lat_max_adj, low_lon_min_adj, low_lon_max_adj = adjust_boundaries(*low_res_params)\n",
    "high_res_params_adj = (high_lat_min_adj, high_lat_max_adj, high_lon_min_adj, high_lon_max_adj, high_res_params[-2], high_res_params[-1])\n",
    "low_res_params_adj = (low_lat_min_adj, low_lat_max_adj, low_lon_min_adj, low_lon_max_adj, low_res_params[-2], low_res_params[-1])\n",
    "mapping_matrix = map_high_res_to_low_res(high_res_params_adj, low_res_params_adj)\n",
    "\n",
    "reconstruction_paths = {\n",
    "    'PSC-FENet-SP': r\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\SP_PSC-FENet.npy\",\n",
    "    'PSC-FENet-ST': r\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\ST_PSC-FENet.npy\",\n",
    "    'PSC-FENet-SP32': r\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\SP_PSC-FENet36.npy\",\n",
    "    'PSC-FENet-ST32': r\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\ST_PSC-FENet36.npy\",\n",
    "    'LSTM': r\"D:\\Data_Store\\Dataset\\Reconstruction\\LSTM\\LSTM1.npy\",\n",
    "    'RF': r\"D:\\Data_Store\\Dataset\\Reconstruction\\RF\\RF_reconstructed_data.npy\",\n",
    "    'ResNet-SP': r\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\SP_ResNet.npy\",\n",
    "    'ResNet-ST': r\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\ST_ResNet.npy\",\n",
    "    'UNet-SP': r\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\SP_UNet.npy\",\n",
    "    'UNet-ST': r\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\ST_UNet.npy\",\n",
    "    'VIT-SP': r\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\SP_ViT.npy\",\n",
    "    'VIT-ST': r\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\ST_ViT.npy\",\n",
    "    'Temporal': r\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\Temporal.npy\"\n",
    "}\n",
    "\n",
    "dates = pd.date_range(start='2016-01-01 03:00', periods=8768, freq='3H')\n",
    "filter_mask = ((dates.hour == 6) | (dates.hour == 18))\n",
    "filtered_indices = np.where(filter_mask)[0]\n",
    "\n",
    "# 第一张图\n",
    "#selected_model_names = ['PSC-FENet-ST','PSC-FENet-ST32', 'ResNet-ST', 'UNet-ST', 'VIT-ST', 'PSC-FENet-SP','PSC-FENet-SP32', 'ResNet-SP', 'UNet-SP', 'VIT-SP']\n",
    "# 第二张图\n",
    "#selected_model_names = ['PSC-FENet-ST','PSC-FENet-ST36', 'LSTM', 'RF', 'Temporal']\n",
    "selected_model_names = ['PSC-FENet-ST', 'LSTM', 'RF', 'Temporal']\n",
    "\n",
    "selected_paths = [reconstruction_paths[name] for name in selected_model_names if name in reconstruction_paths]\n",
    "model_data = [np.load(path)[filtered_indices, :, :] for path in selected_paths]\n",
    "\n",
    "# 计算均值\n",
    "low_res_shape = (42, 104)\n",
    "low_res_model_data = [aggregate_to_low_res(data, mapping_matrix, low_res_shape) for data in model_data]\n",
    "\n",
    "# 使用掩码矩阵-划分TP\n",
    "shp_path = r\"D:\\\\Data_Store\\\\TPBoundary_new(2021)\\\\TPBoundary_new(2021).shp\"\n",
    "\n",
    "settings = {\n",
    "    0.37: (25.293533, 40.604774, 66.84705, 105.31184, 42, 104)\n",
    "}\n",
    "\n",
    "masks = {}\n",
    "mode = 0.37\n",
    "for mode, (lat_min, lat_max, lon_min, lon_max, lat_points, lon_points) in settings.items():\n",
    "    lon_step = (lon_max - lon_min) / (lon_points - 1)\n",
    "    lat_step = (lat_max - lat_min) / (lat_points - 1)\n",
    "    new_lon_min = lon_min - lon_step / 2\n",
    "    new_lon_max = lon_max + lon_step / 2\n",
    "    new_lat_min = lat_min - lat_step / 2\n",
    "    new_lat_max = lat_max + lat_step / 2\n",
    "    lon_range, lat_range = np.linspace(new_lon_min, new_lon_max, lon_points), np.linspace(new_lat_min, new_lat_max, lat_points)\n",
    "    lon_grid, lat_grid = np.meshgrid(lon_range, lat_range)\n",
    "    masks[mode] = generate_clipping_mask(lon_grid, lat_grid, shp_path)\n",
    "\n",
    "mask = masks[mode]\n",
    "mask_expanded = mask[np.newaxis, :, :]\n",
    "low_res_model_data = [np.where(mask_expanded, data, np.nan) for data in low_res_model_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "def cal_data(Original_path, low_res_model_data):\n",
    "    num_models = len(low_res_model_data)\n",
    "    # 定义时间筛选\n",
    "    dates = pd.date_range(start='2016-01-01 03:00', periods=8768, freq='3H')\n",
    "    filter_mask = ((dates.hour == 6) | (dates.hour == 18))\n",
    "    filtered_indices = np.where(filter_mask)[0]\n",
    "    Original_data_low_res_full = np.load(Original_path) \n",
    "    Original_data_low_res = Original_data_low_res_full[filtered_indices, :, :]\n",
    "    \n",
    "    Original_mean = np.nanmean(Original_data_low_res, axis=0)\n",
    "    \n",
    "    # 初始化r_values数组来存储每个模型数据和原始数据之间的相关系数\n",
    "    r_values = np.zeros((Original_data_low_res.shape[1], Original_data_low_res.shape[2], num_models)) * np.nan\n",
    "    \n",
    "    for k, model_data in enumerate(low_res_model_data):\n",
    "        for i in range(Original_data_low_res.shape[1]):  # 遍历纬度\n",
    "            for j in range(Original_data_low_res.shape[2]):  # 遍历经度\n",
    "                original_series = Original_data_low_res[:, i, j]\n",
    "                model_series = model_data[:, i, j]\n",
    "                mask = ~np.isnan(original_series) & ~np.isnan(model_series)\n",
    "                original_series = original_series[mask]\n",
    "                model_series = model_series[mask]\n",
    "                if not len(original_series) or not len(model_series):\n",
    "                    continue\n",
    "                r, _ = pearsonr(original_series, model_series)\n",
    "                r_values[i, j, k] = r\n",
    "\n",
    "    return num_models, Original_mean, r_values\n",
    "Original_path = r\"D:\\Data_Store\\Dataset\\Reconstruction\\37SM\\soil_moisture.npy\"\n",
    "num_models, Original_mean, r_values = cal_data(Original_path, low_res_model_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (数据处理) 0.37 卫星和重建数据图和指标 -得到与卫星数据一致的数据lon，lat，times\n",
    "select_model_names和reconstruction_paths 自动匹配\n",
    "后续实验中没有生成0.37的数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import pearsonr\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "from matplotlib.colors import Normalize\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from matplotlib.collections import PolyCollection\n",
    "\n",
    "low_res_params = (25.293533, 40.604774, 66.84705, 105.31184, 42, 104)\n",
    "def cal_data(selected_model_names, Original_path, reconstruction_paths):\n",
    "    dates = pd.date_range(start='2016-01-01 03:00', periods=8768, freq='3H')\n",
    "    filter_mask = ((dates.hour == 6) | (dates.hour == 18))\n",
    "    filtered_indices = np.where(filter_mask)[0]\n",
    "    Original_data = np.load(Original_path)[filtered_indices, :, :]\n",
    "\n",
    "    #选择模型数据\n",
    "    selected_paths = [reconstruction_paths[name] for name in selected_model_names if name in reconstruction_paths]\n",
    "    model_data = [np.load(path)[filtered_indices, :, :] for path in selected_paths]\n",
    "    num_models = len(model_data)\n",
    "\n",
    "    Original_mean = np.nanmean(Original_data, axis=0)\n",
    "    \n",
    "    # 初始化r_values数组来存储每个模型数据和原始数据之间的相关系数\n",
    "    r_values = np.zeros((Original_data.shape[1], Original_data.shape[2], num_models)) * np.nan\n",
    "    for k,model_data in enumerate(model_data):\n",
    "        for i in range(Original_data.shape[1]):\n",
    "            for j in range(Original_data.shape[2]):\n",
    "                original_series = Original_data[:, i, j]\n",
    "                model_series = model_data[:, i, j]\n",
    "                mask = ~np.isnan(original_series) & ~np.isnan(model_series)\n",
    "                original_series = original_series[mask]\n",
    "                model_series = model_series[mask]\n",
    "                if np.isnan(original_series).all() or np.isnan(model_series).all():\n",
    "                    continue\n",
    "                r, _ = pearsonr(original_series, model_series)\n",
    "                r_values[i, j, k] = r\n",
    "    return num_models, Original_mean, r_values\n",
    "\n",
    "reconstruction_paths = {\n",
    "    'PSC-FENet-SP': r\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\SP_PSC-FENet.npy\",\n",
    "    'PSC-FENet-ST': r\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\ST_PSC-FENet.npy\",\n",
    "    'LSTM': r\"D:\\Data_Store\\Dataset\\Reconstruction\\LSTM\\LSTM1.npy\",\n",
    "    'RF': r\"D:\\Data_Store\\Dataset\\Reconstruction\\RF\\RF_reconstructed_data.npy\",\n",
    "    'ResNet-SP': r\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\SP_ResNet.npy\",\n",
    "    'ResNet-ST': r\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\ST_ResNet.npy\",\n",
    "    'UNet-SP': r\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\SP_UNet.npy\",\n",
    "    'UNet-ST': r\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\ST_UNet.npy\",\n",
    "    'VIT-SP': r\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\SP_ViT.npy\",\n",
    "    'VIT-ST': r\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\ST_ViT.npy\",\n",
    "    'Temporal': r\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\Temporal.npy\"\n",
    "}\n",
    "\n",
    "model_names = list(reconstruction_paths.keys())\n",
    "Original_path = r\"D:\\Data_Store\\Dataset\\Reconstruction\\37SM\\soil_moisture.npy\"\n",
    "\n",
    "# 模型选择\n",
    "# 第一张图\n",
    "selected_model_names = ['PSC-FENet-ST', 'ResNet-ST', 'UNet-ST', 'VIT-ST', 'PSC-FENet-SP', 'ResNet-SP', 'UNet-SP', 'VIT-SP']\n",
    "# 第二张图\n",
    "#selected_model_names = ['KPNet-SP', 'LSTM', 'RF', 'Temporal']\n",
    "\n",
    "num_models, Original_mean, r_values = cal_data(selected_model_names, Original_path, reconstruction_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (数据处理) 0.37数据密度散点图指标计算 R-bias-RMSE-ubRMSE \n",
    "使用0.37数据和原始数据计算，先连接所有数据，然后统一计算，每个指标一个numpyarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def load_data(Original_path, reconstruction_paths):\n",
    "    # 加载原始数据\n",
    "    dates = pd.date_range(start='2016-01-01 03:00', periods=8768, freq='3H')\n",
    "    filter_mask = ((dates.hour == 6) | (dates.hour == 18))\n",
    "    filtered_indices = np.where(filter_mask)[0]\n",
    "    Original_data = np.load(Original_path)[filtered_indices, :, :]\n",
    "    model_datas = [np.load(path)[filtered_indices, :, :] for path in reconstruction_paths.values()]\n",
    "\n",
    "    return Original_data, model_datas\n",
    "\n",
    "def filter_data(Original_data, model_data):\n",
    "    # 扁平化数据\n",
    "    Original_flat = Original_data.flatten()\n",
    "    model_flat = model_data.flatten()\n",
    "    \n",
    "    # 获取非NaN的索引\n",
    "    valid_idx = ~np.isnan(Original_flat) & ~np.isnan(model_flat)\n",
    "    \n",
    "    # 过滤出所有非NaN的数据点\n",
    "    filtered_original = Original_flat[valid_idx]\n",
    "    filtered_model = model_flat[valid_idx]\n",
    "    \n",
    "    return filtered_original, filtered_model\n",
    "\n",
    "def calculate_metrics(Original_data, model_datas, model_names):\n",
    "    metrics = {\n",
    "        'R': [],\n",
    "        'Bias': [],\n",
    "        'RMSE': [],\n",
    "        'ubRMSE': []\n",
    "    }\n",
    "    \n",
    "    for model_name, model_data in zip(model_names, model_datas):\n",
    "        # 过滤出有效的数据点\n",
    "        filtered_original, filtered_model = filter_data(Original_data, model_data)\n",
    "        \n",
    "        # 计算相关系数 R\n",
    "        R = np.corrcoef(filtered_original, filtered_model)[0, 1]\n",
    "        metrics['R'].append(R)\n",
    "        \n",
    "        # 计算偏差 Bias\n",
    "        Bias = np.mean(filtered_model - filtered_original)\n",
    "        metrics['Bias'].append(Bias)\n",
    "        \n",
    "        # 计算均方根误差 RMSE\n",
    "        RMSE = np.sqrt(np.mean((filtered_model - filtered_original) ** 2))\n",
    "        metrics['RMSE'].append(RMSE)\n",
    "        \n",
    "        # 计算无偏均方根误差 ubRMSE\n",
    "        mean_original = np.mean(filtered_original)\n",
    "        mean_model = np.mean(filtered_model)\n",
    "        deviations_original = filtered_original - mean_original\n",
    "        deviations_model = filtered_model - mean_model\n",
    "        ubRMSE = np.sqrt(np.mean((deviations_model - deviations_original) ** 2))\n",
    "        metrics['ubRMSE'].append(ubRMSE)\n",
    "        \n",
    "    return metrics\n",
    "\n",
    "#selected_model_names 前有定义\n",
    "selected_reconstruction_paths = {name: paths for name, paths in reconstruction_paths.items() if name in selected_model_names}\n",
    "Original_data, reconstruction_data = load_data(Original_path, selected_reconstruction_paths)\n",
    "metrics = calculate_metrics(Original_data, reconstruction_data, selected_model_names)\n",
    "\n",
    "# 转为numpy数组\n",
    "for key in metrics:\n",
    "    metrics[key] = np.array(metrics[key])\n",
    "for key, value in metrics.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "np.save(r\"C:\\Users\\Administrator\\Desktop\\Draw\\reconstruction_result.npy\",metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (数据处理) 0.1数据密度散点图指标计算 R-bias-RMSE-ubRMSE \n",
    "绘图步骤中第一步的后续步骤哦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(Original_data, model_datas, model_names):\n",
    "    metrics = {\n",
    "        'R': [],\n",
    "        'Bias': [],\n",
    "        'RMSE': [],\n",
    "        'ubRMSE': []\n",
    "    }\n",
    "    \n",
    "    for model_name, model_data in zip(model_names, model_datas):\n",
    "        # 过滤出有效的数据点\n",
    "        filtered_original, filtered_model = filter_data(Original_data, model_data)\n",
    "        \n",
    "        # 计算相关系数 R\n",
    "        R = np.corrcoef(filtered_original, filtered_model)[0, 1]\n",
    "        metrics['R'].append(R)\n",
    "        \n",
    "        # 计算偏差 Bias\n",
    "        Bias = np.mean(filtered_model - filtered_original)\n",
    "        metrics['Bias'].append(Bias)\n",
    "        \n",
    "        # 计算均方根误差 RMSE\n",
    "        RMSE = np.sqrt(np.mean((filtered_model - filtered_original) ** 2))\n",
    "        metrics['RMSE'].append(RMSE)\n",
    "        \n",
    "        # 计算无偏均方根误差 ubRMSE\n",
    "        mean_original = np.mean(filtered_original)\n",
    "        mean_model = np.mean(filtered_model)\n",
    "        deviations_original = filtered_original - mean_original\n",
    "        deviations_model = filtered_model - mean_model\n",
    "        ubRMSE = np.sqrt(np.mean((deviations_model - deviations_original) ** 2))\n",
    "        metrics['ubRMSE'].append(ubRMSE)\n",
    "        \n",
    "    return metrics\n",
    "def filter_data(Original_data, model_data):\n",
    "    # 扁平化数据\n",
    "    Original_flat = Original_data.flatten()\n",
    "    model_flat = model_data.flatten()\n",
    "    \n",
    "    # 获取非NaN的索引\n",
    "    valid_idx = ~np.isnan(Original_flat) & ~np.isnan(model_flat)\n",
    "    \n",
    "    # 过滤出所有非NaN的数据点\n",
    "    filtered_original = Original_flat[valid_idx]\n",
    "    filtered_model = model_flat[valid_idx]\n",
    "    \n",
    "    return filtered_original, filtered_model\n",
    "def load_data(Original_path):\n",
    "    # 加载原始数据\n",
    "    dates = pd.date_range(start='2016-01-01 03:00', periods=8768, freq='3H')\n",
    "    filter_mask = ((dates.hour == 6) | (dates.hour == 18))\n",
    "    filtered_indices = np.where(filter_mask)[0]\n",
    "    Original_data = np.load(Original_path)[filtered_indices, :, :]\n",
    "\n",
    "    return Original_data\n",
    "\n",
    "Original_data = load_data(Original_path)\n",
    "metrics = calculate_metrics(Original_data, low_res_model_data, selected_model_names)\n",
    "\n",
    "# 将结果转换为numpy数组\n",
    "for key in metrics:\n",
    "    metrics[key] = np.array(metrics[key])\n",
    "\n",
    "# 打印结果\n",
    "for key, value in metrics.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "np.save(r\"C:\\Users\\Administrator\\Desktop\\Draw\\reconstruction_result.npy\",metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (中间过程查看) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 绘制图像DPI=300\n",
    "plt.figure(dpi=150)\n",
    "plt.imshow(Original_data[3], cmap='viridis', origin='lower',vmin=0,vmax=0.5)\n",
    "plt.colorbar(label='Value')\n",
    "plt.title(f\"Data at Time Index: {5}\")\n",
    "plt.xlabel('Longitude')\n",
    "plt.ylabel('Latitude')\n",
    "plt.show()\n",
    "# 绘制图像DPI=300\n",
    "plt.figure(dpi=150)\n",
    "plt.imshow(low_res_model_data[0][3], cmap='viridis', origin='lower',vmin=0,vmax=0.5)\n",
    "plt.colorbar(label='Value')\n",
    "plt.title(f\"Data at Time Index: {5}\")\n",
    "plt.xlabel('Longitude')\n",
    "plt.ylabel('Latitude')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstruction_paths = {\n",
    "    #'KPNet-SP': r\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RES1616\\spatial_kpnet_Encoder_lite_serial_temporal_usage_position_embedding_individual_init256371616.npy\" ,\n",
    "    'KPNet-ST': r\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RES1616\\ST_kpnet_Encoder_lite_serial_temporal_usage_position_embedding_individual_init256_11_121_131_dropout_rate0.3371616.npy\",\n",
    "    'LSTM': r\"D:\\Data_Store\\Dataset\\Reconstruction\\LSTM\\LSTM_0.37.npy\",\n",
    "    'RF': r\"D:\\Data_Store\\Dataset\\Reconstruction\\RF\\RF_reconstructed_data_0.37.npy\",\n",
    "    'ResNet-SP': r\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RES1616\\spatial_resnet34_temporal_usage_position_embedding_init256371616.npy\",\n",
    "    #'ResNet-ST': r\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RES1616\\ST_resnet34_temporal_usage_position_embedding_init256371616.npy\",\n",
    "    'UNet-SP': r\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RES37\\spatial_unet_temporal_usage_position_embedding_init25637168.npy\",\n",
    "    #'UNet-ST': r\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RES1616\\ST_unet_temporal_usage_position_embedding_init256371616.npy\",\n",
    "    'VIT-SP': r\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RES1616\\spatial_VIT_serial_temporal_usage_position_embedding_individual_init256371616.npy\",\n",
    "    #'VIT-ST': r\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RES1616\\ST_ViT_serial_temporal_usage_position_embedding_individual_init256371616.npy\",\n",
    "    'Temporal': r\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RES1616\\temporal_temporal_usage_position_embedding371616.npy\"\n",
    "}\n",
    "Original_path = r\"D:\\Data_Store\\Dataset\\Reconstruction\\37SM\\soil_moisture.npy\"\n",
    "model_names = ['KPNet-ST', 'LSTM', 'RF', 'ResNet-SP', 'UNet-SP', 'VIT-SP', 'Temporal']\n",
    "data = np.load(r\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RES1616\\ST_kpnet_Encoder_lite_serial_temporal_usage_position_embedding_individual_init256_11_121_131_dropout_rate0.3371616.npy\")\n",
    "plt.figure(dpi=150)\n",
    "plt.imshow(data[3], cmap='viridis', origin='lower',vmin=0,vmax=0.5)\n",
    "plt.colorbar(label='Value')\n",
    "plt.title(f\"Data at Time Index: {5}\")\n",
    "plt.xlabel('Longitude')\n",
    "plt.ylabel('Latitude')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = low_res_model_data[0]\n",
    "plt.figure(dpi=150)\n",
    "plt.imshow(data[0], cmap='viridis', origin='lower',vmin=0,vmax=0.5)\n",
    "plt.colorbar(label='Value')\n",
    "plt.title(f\"Data at Time Index: {5}\")\n",
    "plt.xlabel('Longitude')\n",
    "plt.ylabel('Latitude')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "data = np.load(r\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\ST_PSC-FENet.npy\")\n",
    "plt.figure(dpi=150)\n",
    "plt.imshow(data[3], cmap='viridis', origin='lower',vmin=0,vmax=0.5)\n",
    "plt.colorbar(label='Value')\n",
    "plt.title(f\"Data at Time Index: {5}\")\n",
    "plt.xlabel('Longitude')\n",
    "plt.ylabel('Latitude')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "data = np.load(r\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\ST_PSC-FENet.npy\")\n",
    "plt.figure(dpi=150)\n",
    "plt.imshow(data[3], cmap='viridis', origin='lower',vmin=0,vmax=0.5)\n",
    "plt.colorbar(label='Value')\n",
    "plt.title(f\"Data at Time Index: {5}\")\n",
    "plt.xlabel('Longitude')\n",
    "plt.ylabel('Latitude')\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (绘图) V1.0 3-4 密度散点图-完整绘制\n",
    "SM不按均值区间分割"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import pearsonr\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "from matplotlib.colors import Normalize\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from matplotlib.collections import PolyCollection\n",
    "import matplotlib.colors as mcolors\n",
    "from scipy.stats import gaussian_kde\n",
    "from matplotlib.colors import to_rgba\n",
    "from matplotlib.font_manager import FontProperties\n",
    "\n",
    "def clean_axis(ax):\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['bottom'].set_visible(False)\n",
    "    ax.spines['left'].set_visible(False)\n",
    "    ax.tick_params(axis='x', which='both', length=0, labelbottom=False)\n",
    "    ax.tick_params(axis='y', which='both', length=0, labelleft=False)\n",
    "    ax.grid(False)\n",
    "def create_subplots(num_models, Original_mean,r_values, lon_range, lat_range, model_names,  metric, plt_name=None):\n",
    "    # 创建图形对象,绘制顺序依赖于fig定义顺序\n",
    "    #colors = ['#f8d1b4', '#f69b72', '#99f2d1', '#315c4f', '#b096b6', '#3e2248', '#555555']['#acebe2', '#9fe6dd', '#93e1d8' ,'#6d87bc','#5673ae', '#3e5fa0', '#274b91']\n",
    "    colors = ['#1f7ea1', '#1f7ea1', '#1f7ea1' ,'#274b91','#274b91', '#274b91', '#274b91']\n",
    "    fig = plt.figure(figsize=(18, 24))\n",
    "\n",
    "    ratio_h = 4 # 竖直比例\n",
    "    ratio_v = 3 # 水平比例\n",
    "\n",
    "    absolute_dis_h = 0.45/ratio_h # 中央子图绝对大小（竖直）0.15\n",
    "    absolute_dis_v = 0.45/ratio_v # 中央子图绝对大小（水平）0.2\n",
    "    absolute_dis_v_third = 0.45/ratio_v*2.5 # 第三层子图绝对大小（水平）0.2\n",
    "\n",
    "    edge_h = 0.29 # 竖直边缘间隙\n",
    "    edge_v = 0.1 # 水平边缘间隙\n",
    "\n",
    "    mid_h = (1-edge_h*2-absolute_dis_h*3)/2 # 竖直间隙,可以通过竖直边缘间隙调节\n",
    "    mid_v = (1-edge_v*2-absolute_dis_v*4)/3 # 水平间隙……\n",
    "\n",
    "    right_offset_first = 0.10 # 第一层向右偏移量\n",
    "\n",
    "    up_offset = 0.1129 # 上方子图向上偏移\n",
    "    right_offset = 0.1505 # 右方子图向右偏移\n",
    "    up_ratio = 0.12  # 上方子图长度比例\n",
    "    right_ratio = 0.25  # 上方子图宽度比例\n",
    "\n",
    "    # 计算第一层图起点:上层子图  左起点、下起点、宽度比例、高度比例\n",
    "    ax2 = fig.add_axes([edge_v + right_offset_first, 1 - edge_h - absolute_dis_h + up_offset, absolute_dis_v, absolute_dis_h * up_ratio])  # 左上上\n",
    "    ax3 = fig.add_axes([edge_v + right_offset_first + right_offset, 1 - edge_h - absolute_dis_h, absolute_dis_v * right_ratio, absolute_dis_h])  # 左上右\n",
    "    ax1 = fig.add_axes([edge_v + right_offset_first, 1 - edge_h - absolute_dis_h, absolute_dis_v, absolute_dis_h])  # 左上\n",
    "\n",
    "    ax5 = fig.add_axes([edge_v + right_offset_first + absolute_dis_v + mid_v, 1 - edge_h - absolute_dis_h + up_offset, absolute_dis_v, absolute_dis_h * up_ratio])  # 中上上 32\n",
    "    ax6 = fig.add_axes([edge_v + right_offset_first + right_offset + absolute_dis_v + mid_v, 1 - edge_h - absolute_dis_h, absolute_dis_v * right_ratio, absolute_dis_h])  # 中上右 32\n",
    "    ax4 = fig.add_axes([edge_v + right_offset_first + absolute_dis_v + mid_v, 1 - edge_h - absolute_dis_h, absolute_dis_v, absolute_dis_h])  # 中上 32\n",
    "\n",
    "    ax8 = fig.add_axes([edge_v + right_offset_first + 2*(absolute_dis_v + mid_v), 1 - edge_h - absolute_dis_h + up_offset, absolute_dis_v, absolute_dis_h * up_ratio])  # 右上上\n",
    "    ax9 = fig.add_axes([edge_v + right_offset_first + right_offset + 2*(absolute_dis_v + mid_v), 1 - edge_h - absolute_dis_h, absolute_dis_v * right_ratio, absolute_dis_h])  # 右上右\n",
    "    ax7 = fig.add_axes([edge_v + right_offset_first + 2*(absolute_dis_v + mid_v), 1 - edge_h - absolute_dis_h, absolute_dis_v, absolute_dis_h])  # 右上\n",
    "\n",
    "    # 中层子图\n",
    "    ax11 = fig.add_axes([edge_v, 1 - edge_h - 2*absolute_dis_h - mid_h + up_offset, absolute_dis_v, absolute_dis_h * up_ratio])  # 左中上\n",
    "    ax12 = fig.add_axes([edge_v + right_offset, 1 - edge_h - 2*absolute_dis_h - mid_h, absolute_dis_v * right_ratio, absolute_dis_h])  # 左中右\n",
    "    ax10 = fig.add_axes([edge_v , 1 - edge_h - 2*absolute_dis_h - mid_h, absolute_dis_v, absolute_dis_h])  # 左中\n",
    "\n",
    "    ax14 = fig.add_axes([edge_v + absolute_dis_v + mid_v, 1 - edge_h - 2*absolute_dis_h - mid_h + up_offset, absolute_dis_v, absolute_dis_h * up_ratio])  # 中左中上\n",
    "    ax15 = fig.add_axes([edge_v + right_offset + absolute_dis_v + mid_v, 1 - edge_h - 2*absolute_dis_h - mid_h, absolute_dis_v * right_ratio, absolute_dis_h])  # 中左中右\n",
    "    ax13 = fig.add_axes([edge_v + absolute_dis_v + mid_v, 1 - edge_h - 2*absolute_dis_h - mid_h, absolute_dis_v, absolute_dis_h])  # 中左中\n",
    "\n",
    "    ax17 = fig.add_axes([edge_v + 2*(absolute_dis_v + mid_v), 1 - edge_h - 2*absolute_dis_h - mid_h + up_offset, absolute_dis_v, absolute_dis_h * up_ratio])  # 中右中上\n",
    "    ax18 = fig.add_axes([edge_v + right_offset + 2*(absolute_dis_v + mid_v), 1 - edge_h - 2*absolute_dis_h - mid_h, absolute_dis_v * right_ratio, absolute_dis_h])  # 中右中右\n",
    "    ax16 = fig.add_axes([edge_v + 2*(absolute_dis_v + mid_v), 1 - edge_h - 2*absolute_dis_h - mid_h, absolute_dis_v, absolute_dis_h])  # 中右中\n",
    "\n",
    "    ax20 = fig.add_axes([edge_v + 3*(absolute_dis_v + mid_v), 1 - edge_h - 2*absolute_dis_h - mid_h + up_offset, absolute_dis_v, absolute_dis_h * up_ratio])  # 右中上\n",
    "    ax21 = fig.add_axes([edge_v + right_offset + 3*(absolute_dis_v + mid_v), 1 - edge_h - 2*absolute_dis_h - mid_h, absolute_dis_v * right_ratio, absolute_dis_h])  # 右中右\n",
    "    ax19 = fig.add_axes([edge_v + 3*(absolute_dis_v + mid_v), 1 - edge_h - 2*absolute_dis_h - mid_h, absolute_dis_v, absolute_dis_h])  # 右中\n",
    "\n",
    "    axes = [ax1, ax2, ax3, ax4, ax5, ax6, ax7, ax8, ax9, ax10, ax11, ax12, ax13, ax14, ax15, ax16, ax17, ax18, ax19, ax20, ax21]\n",
    "\n",
    "    color_scheme1 = ['#c4f5ed', '#1f7ea1']\n",
    "    color_scheme2 = ['#9bafd9', '#103783']\n",
    "\n",
    "    r_set = metric['R']\n",
    "    bias_set = metric['Bias']\n",
    "    rmse_set = metric['RMSE']\n",
    "    ubrmse_set = metric['ubRMSE']\n",
    "\n",
    "    for i in range(num_models):\n",
    "        if i < 3:\n",
    "            color_scheme = color_scheme1\n",
    "        else:\n",
    "            color_scheme = color_scheme2\n",
    "        draw_subplot(axes[3*i], Original_mean, r_values[:,:,i], model_names[i], color_scheme,r_set[i],bias_set[i],rmse_set[i],ubrmse_set[i])\n",
    "        draw_subplot_up(axes[3*i+1], Original_mean, r_values[:,:,i], colors[i])\n",
    "        draw_subplot_left(axes[3*i+2], Original_mean, r_values[:,:,i], colors[i])  \n",
    "    fig.savefig(r\"C:\\Users\\Administrator\\Desktop\\Draw\\sat_original.jpeg\", dpi=1000, format='jpeg', bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def draw_subplot(ax, Original_mean, r_value, model_name, color_scheme, r, bias, rmse, ubrmse):\n",
    "    cmap = mcolors.LinearSegmentedColormap.from_list('custom_cmap', color_scheme)\n",
    "    model_names = ['KPNet-ST', 'LSTM', 'RF', 'ResNet-SP', 'UNet-SP', 'VIT-SP', 'Temporal']\n",
    "    font_prop = FontProperties(family='Times New Roman', size=14)\n",
    "    r_value = r_value.flatten()\n",
    "    Original_mean = Original_mean.flatten()\n",
    "    mask = ~np.isnan(r_value) & ~np.isnan(Original_mean)\n",
    "    x = Original_mean[mask]\n",
    "    y = r_value[mask]\n",
    "\n",
    "    xy = np.vstack([x, y])\n",
    "    z = gaussian_kde(xy)(xy)\n",
    "\n",
    "    scatter = ax.scatter(x, y, c=z, s=12, cmap=cmap, edgecolor='face', linewidth=0, alpha=1)\n",
    "    sns.regplot(x=x, y=y, ax=ax, order=3, scatter=False, color='#522226', line_kws={'label':f'Quad Fit: {model_name}'}, ci=95)\n",
    "    for artist in ax.findobj(PolyCollection):\n",
    "        artist.set_alpha(0.4)\n",
    "\n",
    "    x_min,x_max = np.percentile(x, 1), np.percentile(x, 99)\n",
    "    y_min,y_max = np.percentile(y, 1), np.percentile(y, 99)\n",
    "\n",
    "    padding_x = (x_max - x_min) * 0.01\n",
    "    padding_y = (0.9999 - y_min) * 0.01\n",
    "    # 指标文本\n",
    "    x_range = np.arange(0,4)*6\n",
    "    text_position_x = x_max - padding_x * 0.5\n",
    "    text_position_y_r = y_min + padding_y * x_range[3] + padding_y * 0.5\n",
    "    text_position_y_bias = y_min + padding_y * x_range[2] + padding_y * 0.5\n",
    "    text_position_y_rmse = y_min + padding_y * x_range[1] + padding_y * 0.5\n",
    "    text_position_y_ubrmse = y_min + padding_y * x_range[0] + padding_y * 0.5\n",
    "    # 使用FontProperties实例绘制文本，去除$符号避免斜体\n",
    "    ax.text(text_position_x, text_position_y_r, f'R={r:.3f}', fontproperties=font_prop, horizontalalignment='right', verticalalignment='bottom')\n",
    "    ax.text(text_position_x, text_position_y_rmse, f'RMSE={rmse:.3f}', fontproperties=font_prop, horizontalalignment='right', verticalalignment='bottom')\n",
    "    ax.text(text_position_x, text_position_y_bias, f'bias={bias:.3f}', fontproperties=font_prop, horizontalalignment='right', verticalalignment='bottom')\n",
    "    ax.text(text_position_x, text_position_y_ubrmse, f'ubRMSE={ubrmse:.3f}', fontproperties=font_prop, horizontalalignment='right', verticalalignment='bottom')\n",
    "    prefix = \"(\" + chr(ord('a') + model_names.index(model_name)) + \")\"\n",
    "    ax.text(0.01, 0.11, f'{prefix} {model_name}', transform=ax.transAxes, fontsize=18, fontname='Times New Roman', verticalalignment='top')\n",
    "\n",
    "    # 设置刻度\n",
    "    ax.set_xlim(x_min,x_max)\n",
    "    ax.set_ylim(y_min,0.9999)\n",
    "    ax.xaxis.set_major_locator(MaxNLocator(nbins=4))\n",
    "    ax.yaxis.set_major_locator(MaxNLocator(nbins=4))\n",
    "    ax.tick_params(axis='x', which='both', colors='black', length=3, width=1.5, labelsize=14, direction='out', pad=2, bottom=True, top=False, labelbottom=True,labeltop=False)\n",
    "    ax.tick_params(axis='y', which='both', colors='black', length=3, width=1.5, labelsize=14, direction='out', pad=6, left=True, right=False, labelleft=True,labelright=False)\n",
    "    # 设置轴标签\n",
    "    ax.set_xlabel('Original mean SM (m³/m³)', size=16, fontname='Times New Roman')\n",
    "    ax.set_ylabel('Pearson R',size=16)\n",
    "    plt.setp(ax.get_xticklabels(), rotation=0,rotation_mode=\"anchor\")\n",
    "    plt.setp(ax.get_yticklabels(), rotation=90,ha=\"center\", rotation_mode=\"anchor\")\n",
    "    # 设置轴参数\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['bottom'].set_color('black')\n",
    "    ax.spines['bottom'].set_linewidth(1.5)\n",
    "    ax.spines['left'].set_color('black')\n",
    "    ax.spines['left'].set_linewidth(1.5)\n",
    "    ax.grid(True, color='black', linestyle='--',alpha=0.5,zorder=0.1)\n",
    "\n",
    "def draw_subplot1(ax, Original_mean, r_value, model_name, color_scheme,r,bias,rmse,ubrmse):\n",
    "    cmap = mcolors.LinearSegmentedColormap.from_list('custom_cmap', color_scheme)\n",
    "    model_names = ['KPNet-ST', 'LSTM', 'RF', 'ResNet-SP', 'UNet-SP', 'VIT-SP', 'Temporal']\n",
    "    font_prop = FontProperties(family='Times New Roman', size=14)\n",
    "    r_value = r_value.flatten()\n",
    "    Original_mean = Original_mean.flatten()\n",
    "    mask = ~np.isnan(r_value) & ~np.isnan(Original_mean)\n",
    "    x = Original_mean[mask]\n",
    "    y = r_value[mask]\n",
    "\n",
    "    # ax.scatter(x, y, color=color, alpha=0.5)\n",
    "    # 分割Original_mean并计算每个区间的r_value均值\n",
    "    bins = np.arange(np.min(x), np.max(x) + 0.0001, 0.0001)\n",
    "    digitized = np.digitize(x, bins)\n",
    "    bin_means = np.array([y[digitized == i].mean() for i in range(1, len(bins))])\n",
    "    bin_centers = (bins[:-1] + bins[1:]) / 2\n",
    "\n",
    "    valid_mask = ~np.isnan(bin_means)\n",
    "    bin_centers_clean = bin_centers[valid_mask]\n",
    "    bin_means_clean = bin_means[valid_mask]\n",
    "\n",
    "    xy_means = np.vstack([bin_centers_clean, bin_means_clean])\n",
    "    z_means = gaussian_kde(xy_means)(xy_means)\n",
    "\n",
    "    scatter = ax.scatter(bin_centers_clean, bin_means_clean, c=z_means, s=12, cmap=cmap, edgecolor='face', linewidth=0, alpha=1)\n",
    "    sns.regplot(x=bin_centers_clean, y=bin_means_clean, ax=ax, order=3, scatter=False, color='#522226', line_kws={'label':f'Quad Fit: {model_name}'}, ci=95)\n",
    "    for artist in ax.findobj(PolyCollection):\n",
    "        artist.set_alpha(0.4)\n",
    "\n",
    "    x_min,x_max = np.percentile(x, 1), np.percentile(x, 99)\n",
    "    y_min,y_max = np.percentile(y, 5), np.percentile(y, 95)\n",
    "\n",
    "    padding_x = (x_max - x_min) * 0.01\n",
    "    padding_y = (0.9999 - y_min) * 0.01\n",
    "    # 指标文本\n",
    "    x_range = np.arange(0,4)*6\n",
    "    text_position_x = x_max - padding_x * 0.5\n",
    "    text_position_y_r = y_min + padding_y * x_range[3] + padding_y * 0.5\n",
    "    text_position_y_bias = y_min + padding_y * x_range[2] + padding_y * 0.5\n",
    "    text_position_y_rmse = y_min + padding_y * x_range[1] + padding_y * 0.5\n",
    "    text_position_y_ubrmse = y_min + padding_y * x_range[0] + padding_y * 0.5\n",
    "    # 使用FontProperties实例绘制文本，去除$符号避免斜体\n",
    "    ax.text(text_position_x, text_position_y_r, f'R={r:.3f}', fontproperties=font_prop, horizontalalignment='right', verticalalignment='bottom')\n",
    "    ax.text(text_position_x, text_position_y_rmse, f'RMSE={rmse:.3f}', fontproperties=font_prop, horizontalalignment='right', verticalalignment='bottom')\n",
    "    ax.text(text_position_x, text_position_y_bias, f'bias={bias:.3f}', fontproperties=font_prop, horizontalalignment='right', verticalalignment='bottom')\n",
    "    ax.text(text_position_x, text_position_y_ubrmse, f'ubRMSE={ubrmse:.3f}', fontproperties=font_prop, horizontalalignment='right', verticalalignment='bottom')\n",
    "    prefix = \"(\" + chr(ord('a') + model_names.index(model_name)) + \")\"\n",
    "    ax.text(0.01, 0.11, f'{prefix} {model_name}', transform=ax.transAxes, fontsize=18, fontname='Times New Roman', verticalalignment='top')\n",
    "\n",
    "    # 设置刻度\n",
    "    ax.set_xlim(x_min,x_max)\n",
    "    ax.set_ylim(y_min,0.9999)\n",
    "    ax.xaxis.set_major_locator(MaxNLocator(nbins=4))\n",
    "    ax.yaxis.set_major_locator(MaxNLocator(nbins=4))\n",
    "    ax.tick_params(axis='x', which='both', colors='black', length=3, width=1.5, labelsize=14, direction='out', pad=2, bottom=True, top=False, labelbottom=True,labeltop=False)\n",
    "    ax.tick_params(axis='y', which='both', colors='black', length=3, width=1.5, labelsize=14, direction='out', pad=6, left=True, right=False, labelleft=True,labelright=False)\n",
    "    # 设置轴标签\n",
    "    ax.set_xlabel('Original mean SM (m³/m³)', size=16, fontname='Times New Roman')\n",
    "    ax.set_ylabel('Pearson R',size=16)\n",
    "    plt.setp(ax.get_xticklabels(), rotation=0,rotation_mode=\"anchor\")\n",
    "    plt.setp(ax.get_yticklabels(), rotation=90,ha=\"center\", rotation_mode=\"anchor\")\n",
    "    # 设置轴参数\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['bottom'].set_color('black')\n",
    "    ax.spines['bottom'].set_linewidth(1.5)\n",
    "    ax.spines['left'].set_color('black')\n",
    "    ax.spines['left'].set_linewidth(1.5)\n",
    "    ax.grid(True, color='black', linestyle='--',alpha=0.5,zorder=0.1)\n",
    "\n",
    "def draw_subplot_up(ax, Original_mean, r_value, color):\n",
    "    r_value = r_value.flatten()\n",
    "    Original_mean = Original_mean.flatten()\n",
    "    mask = ~np.isnan(r_value) & ~np.isnan(Original_mean)\n",
    "    x = Original_mean[mask]\n",
    "    y = r_value[mask]\n",
    "    x_min,x_max = np.percentile(x, 1), np.percentile(x, 99)\n",
    "\n",
    "    step = 0.001\n",
    "    mask = ~np.isnan(Original_mean) & ~np.isnan(r_value)\n",
    "    Original_mean = Original_mean[mask].flatten()\n",
    "    min_val, max_val = np.min(Original_mean), np.max(Original_mean)\n",
    "    bins = np.arange(min_val, max_val + step, step)\n",
    "    counts, _ = np.histogram(Original_mean, bins=bins)\n",
    "    centers = (bins[:-1] + bins[1:]) / 2\n",
    "    counts_min,counts_max = min(counts),max(counts)\n",
    "    ax.bar(centers, counts, width=step, color=color, edgecolor=color, alpha=0.8)\n",
    "    ax.set_ylim(0,counts_max)\n",
    "    ax.set_xlim(x_min,x_max)\n",
    "    clean_axis(ax)\n",
    "\n",
    "def draw_subplot_left(ax, Original_mean, r_value, color):\n",
    "    r_value = r_value.flatten()\n",
    "    Original_mean = Original_mean.flatten()\n",
    "    mask = ~np.isnan(r_value) & ~np.isnan(Original_mean)\n",
    "    x = Original_mean[mask]\n",
    "    y = r_value[mask]\n",
    "\n",
    "    step = 0.001\n",
    "    mask = ~np.isnan(r_value) & ~np.isnan(Original_mean)\n",
    "    r_value = r_value[mask].flatten()\n",
    "    min_val, max_val = np.min(r_value), np.max(r_value)\n",
    "    bins = np.arange(min_val, max_val + step, step)\n",
    "    counts, _ = np.histogram(r_value, bins=bins)\n",
    "    centers = (bins[:-1] + bins[1:]) / 2\n",
    "    ax.barh(centers, counts, height=step, color=color, edgecolor=color, alpha=0.8)\n",
    "    counts_min,counts_max = min(counts),max(counts)\n",
    "    y_min,y_max = np.percentile(y, 1), np.percentile(y, 99)\n",
    "    ax.set_xlim(0, counts_max)\n",
    "    ax.set_ylim(y_min, 0.9999)\n",
    "    clean_axis(ax)\n",
    "    \n",
    "metric = np.load(r\"C:\\Users\\Administrator\\Desktop\\Draw\\reconstruction_result.npy\",allow_pickle=True).item() \n",
    "create_subplots(num_models, Original_mean,r_values, lon_range, lat_range, model_names, metric, plt_name = 'a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (绘图) V1.0 密度散点图-分割绘制 \n",
    "Mean SM按0.001步长均值然后绘制密度散点"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.colors import Normalize\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from matplotlib.collections import PolyCollection\n",
    "import matplotlib.colors as mcolors\n",
    "from scipy.stats import gaussian_kde\n",
    "from matplotlib.colors import to_rgba\n",
    "from matplotlib.font_manager import FontProperties\n",
    "\n",
    "def clean_axis(ax):\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['bottom'].set_visible(False)\n",
    "    ax.spines['left'].set_visible(False)\n",
    "    ax.tick_params(axis='x', which='both', length=0, labelbottom=False)\n",
    "    ax.tick_params(axis='y', which='both', length=0, labelleft=False)\n",
    "    ax.grid(False)\n",
    "def create_subplots(num_models, Original_mean,r_values, lon_range, lat_range, model_names,  metric, plt_name=None):\n",
    "    # 创建图形对象,绘制顺序依赖于fig定义顺序\n",
    "    colors = ['#1f7ea1', '#1f7ea1', '#1f7ea1' ,'#274b91','#274b91', '#274b91', '#274b91']\n",
    "    fig = plt.figure(figsize=(18, 24))\n",
    "\n",
    "    ratio_h = 4 # 竖直比例\n",
    "    ratio_v = 3 # 水平比例\n",
    "\n",
    "    absolute_dis_h = 0.45/ratio_h # 中央子图绝对大小（竖直）0.15\n",
    "    absolute_dis_v = 0.45/ratio_v # 中央子图绝对大小（水平）0.2\n",
    "\n",
    "    edge_h = 0.29 # 竖直边缘间隙\n",
    "    edge_v = 0.1 # 水平边缘间隙\n",
    "\n",
    "    mid_h = (1-edge_h*2-absolute_dis_h*3)/2 # 竖直间隙,可以通过竖直边缘间隙调节\n",
    "    mid_v = (1-edge_v*2-absolute_dis_v*4)/3 # 水平间隙……\n",
    "\n",
    "    right_offset_first = 0.10 # 第一层向右偏移量\n",
    "\n",
    "    up_offset = 0.1129 # 上方子图向上偏移\n",
    "    right_offset = 0.1505 # 右方子图向右偏移\n",
    "    up_ratio = 0.12  # 上方子图长度比例\n",
    "    right_ratio = 0.25  # 上方子图宽度比例\n",
    "\n",
    "    # 计算第一层图起点:上层子图  左起点、下起点、宽度比例、高度比例\n",
    "    ax2 = fig.add_axes([edge_v + right_offset_first, 1 - edge_h - absolute_dis_h + up_offset, absolute_dis_v, absolute_dis_h * up_ratio])  # 左上上\n",
    "    ax3 = fig.add_axes([edge_v + right_offset_first + right_offset, 1 - edge_h - absolute_dis_h, absolute_dis_v * right_ratio, absolute_dis_h])  # 左上右\n",
    "    ax1 = fig.add_axes([edge_v + right_offset_first, 1 - edge_h - absolute_dis_h, absolute_dis_v, absolute_dis_h])  # 左上\n",
    "\n",
    "    ax5 = fig.add_axes([edge_v + right_offset_first + absolute_dis_v + mid_v, 1 - edge_h - absolute_dis_h + up_offset, absolute_dis_v, absolute_dis_h * up_ratio])  # 中上上 32\n",
    "    ax6 = fig.add_axes([edge_v + right_offset_first + right_offset + absolute_dis_v + mid_v, 1 - edge_h - absolute_dis_h, absolute_dis_v * right_ratio, absolute_dis_h])  # 中上右 32\n",
    "    ax4 = fig.add_axes([edge_v + right_offset_first + absolute_dis_v + mid_v, 1 - edge_h - absolute_dis_h, absolute_dis_v, absolute_dis_h])  # 中上 32\n",
    "\n",
    "    ax8 = fig.add_axes([edge_v + right_offset_first + 2*(absolute_dis_v + mid_v), 1 - edge_h - absolute_dis_h + up_offset, absolute_dis_v, absolute_dis_h * up_ratio])  # 右上上\n",
    "    ax9 = fig.add_axes([edge_v + right_offset_first + right_offset + 2*(absolute_dis_v + mid_v), 1 - edge_h - absolute_dis_h, absolute_dis_v * right_ratio, absolute_dis_h])  # 右上右\n",
    "    ax7 = fig.add_axes([edge_v + right_offset_first + 2*(absolute_dis_v + mid_v), 1 - edge_h - absolute_dis_h, absolute_dis_v, absolute_dis_h])  # 右上\n",
    "\n",
    "    # 中层子图\n",
    "    ax11 = fig.add_axes([edge_v, 1 - edge_h - 2*absolute_dis_h - mid_h + up_offset, absolute_dis_v, absolute_dis_h * up_ratio])  # 左中上\n",
    "    ax12 = fig.add_axes([edge_v + right_offset, 1 - edge_h - 2*absolute_dis_h - mid_h, absolute_dis_v * right_ratio, absolute_dis_h])  # 左中右\n",
    "    ax10 = fig.add_axes([edge_v , 1 - edge_h - 2*absolute_dis_h - mid_h, absolute_dis_v, absolute_dis_h])  # 左中\n",
    "\n",
    "    ax14 = fig.add_axes([edge_v + absolute_dis_v + mid_v, 1 - edge_h - 2*absolute_dis_h - mid_h + up_offset, absolute_dis_v, absolute_dis_h * up_ratio])  # 中左中上\n",
    "    ax15 = fig.add_axes([edge_v + right_offset + absolute_dis_v + mid_v, 1 - edge_h - 2*absolute_dis_h - mid_h, absolute_dis_v * right_ratio, absolute_dis_h])  # 中左中右\n",
    "    ax13 = fig.add_axes([edge_v + absolute_dis_v + mid_v, 1 - edge_h - 2*absolute_dis_h - mid_h, absolute_dis_v, absolute_dis_h])  # 中左中\n",
    "\n",
    "    ax17 = fig.add_axes([edge_v + 2*(absolute_dis_v + mid_v), 1 - edge_h - 2*absolute_dis_h - mid_h + up_offset, absolute_dis_v, absolute_dis_h * up_ratio])  # 中右中上\n",
    "    ax18 = fig.add_axes([edge_v + right_offset + 2*(absolute_dis_v + mid_v), 1 - edge_h - 2*absolute_dis_h - mid_h, absolute_dis_v * right_ratio, absolute_dis_h])  # 中右中右\n",
    "    ax16 = fig.add_axes([edge_v + 2*(absolute_dis_v + mid_v), 1 - edge_h - 2*absolute_dis_h - mid_h, absolute_dis_v, absolute_dis_h])  # 中右中\n",
    "\n",
    "    ax20 = fig.add_axes([edge_v + 3*(absolute_dis_v + mid_v), 1 - edge_h - 2*absolute_dis_h - mid_h + up_offset, absolute_dis_v, absolute_dis_h * up_ratio])  # 右中上\n",
    "    ax21 = fig.add_axes([edge_v + right_offset + 3*(absolute_dis_v + mid_v), 1 - edge_h - 2*absolute_dis_h - mid_h, absolute_dis_v * right_ratio, absolute_dis_h])  # 右中右\n",
    "    ax19 = fig.add_axes([edge_v + 3*(absolute_dis_v + mid_v), 1 - edge_h - 2*absolute_dis_h - mid_h, absolute_dis_v, absolute_dis_h])  # 右中\n",
    "\n",
    "    axes = [ax1, ax2, ax3, ax4, ax5, ax6, ax7, ax8, ax9, ax10, ax11, ax12, ax13, ax14, ax15, ax16, ax17, ax18, ax19, ax20, ax21]\n",
    "\n",
    "    color_scheme1 = ['#c4f5ed', '#1f7ea1']\n",
    "    color_scheme2 = ['#9bafd9', '#103783']\n",
    "\n",
    "    r_set = metric['R']\n",
    "    bias_set = metric['Bias']\n",
    "    rmse_set = metric['RMSE']\n",
    "    ubrmse_set = metric['ubRMSE']\n",
    "\n",
    "    for i in range(num_models):\n",
    "        if i < 3:\n",
    "            color_scheme = color_scheme1\n",
    "        else:\n",
    "            color_scheme = color_scheme2\n",
    "        draw_subplot(axes[3*i], Original_mean, r_values[:,:,i], model_names[i], color_scheme,r_set[i],bias_set[i],rmse_set[i],ubrmse_set[i])\n",
    "        draw_subplot_up(axes[3*i+1], Original_mean, r_values[:,:,i], colors[i])\n",
    "        draw_subplot_left(axes[3*i+2], Original_mean, r_values[:,:,i], colors[i])  \n",
    "    fig.savefig(r\"C:\\Users\\Administrator\\Desktop\\Draw\\sat_original.jpeg\", dpi=500, format='jpeg', bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "def draw_subplot(ax, Original_mean, r_value, model_name, color_scheme,r,bias,rmse,ubrmse):\n",
    "    cmap = mcolors.LinearSegmentedColormap.from_list('custom_cmap', color_scheme)\n",
    "    model_names = ['KPNet-ST', 'LSTM', 'RF', 'ResNet-SP', 'UNet-SP', 'VIT-SP', 'Temporal']\n",
    "    font_prop = FontProperties(family='Times New Roman', size=14)\n",
    "    r_value = r_value.flatten()\n",
    "    Original_mean = Original_mean.flatten()\n",
    "    mask = ~np.isnan(r_value) & ~np.isnan(Original_mean)\n",
    "    x = Original_mean[mask]\n",
    "    y = r_value[mask]\n",
    "\n",
    "    # ax.scatter(x, y, color=color, alpha=0.5)\n",
    "    # 分割Original_mean并计算每个区间的r_value均值\n",
    "    bins = np.arange(np.min(x), np.max(x) + 0.0005, 0.0005)\n",
    "    digitized = np.digitize(x, bins)\n",
    "    bin_means = np.array([y[digitized == i].mean() if not y[digitized == i].size == 0 else np.nan for i in range(1, len(bins))])\n",
    "    bin_centers = (bins[:-1] + bins[1:]) / 2\n",
    "\n",
    "    valid_mask = ~np.isnan(bin_means)\n",
    "    bin_centers_clean = bin_centers[valid_mask]\n",
    "    bin_means_clean = bin_means[valid_mask]\n",
    "\n",
    "    xy_means = np.vstack([bin_centers_clean, bin_means_clean])\n",
    "    z_means = gaussian_kde(xy_means)(xy_means)\n",
    "\n",
    "    scatter = ax.scatter(bin_centers_clean, bin_means_clean, c=z_means, s=20, cmap=cmap, edgecolor='face', linewidth=0, alpha=1)\n",
    "    sns.regplot(x=bin_centers_clean, y=bin_means_clean, ax=ax, order=3, scatter=False, color='#522226', line_kws={'label':f'Quad Fit: {model_name}'}, ci=95)\n",
    "    for artist in ax.findobj(PolyCollection):\n",
    "        artist.set_alpha(0.4)\n",
    "\n",
    "    x_min,x_max = np.percentile(x, 1), np.percentile(x, 99)\n",
    "    y_min,y_max = np.percentile(y, 5), np.percentile(y, 95)\n",
    "\n",
    "    padding_x = (x_max - x_min) * 0.01\n",
    "    padding_y = (0.9999 - y_min) * 0.01\n",
    "    # 指标文本\n",
    "    x_range = np.arange(0,4)*6\n",
    "    text_position_x = x_max - padding_x * 0.5\n",
    "    text_position_y_r = y_min + padding_y * x_range[3] + padding_y * 0.5\n",
    "    text_position_y_bias = y_min + padding_y * x_range[2] + padding_y * 0.5\n",
    "    text_position_y_rmse = y_min + padding_y * x_range[1] + padding_y * 0.5\n",
    "    text_position_y_ubrmse = y_min + padding_y * x_range[0] + padding_y * 0.5\n",
    "    # 使用FontProperties实例绘制文本，去除$符号避免斜体\n",
    "    ax.text(text_position_x, text_position_y_r, f'R={r:.3f}', fontproperties=font_prop, horizontalalignment='right', verticalalignment='bottom')\n",
    "    ax.text(text_position_x, text_position_y_rmse, f'RMSE={rmse:.3f}', fontproperties=font_prop, horizontalalignment='right', verticalalignment='bottom')\n",
    "    ax.text(text_position_x, text_position_y_bias, f'bias={bias:.3f}', fontproperties=font_prop, horizontalalignment='right', verticalalignment='bottom')\n",
    "    ax.text(text_position_x, text_position_y_ubrmse, f'ubRMSE={ubrmse:.3f}', fontproperties=font_prop, horizontalalignment='right', verticalalignment='bottom')\n",
    "    prefix = \"(\" + chr(ord('a') + model_names.index(model_name)) + \")\"\n",
    "    ax.text(0.01, 0.11, f'{prefix} {model_name}', transform=ax.transAxes, fontsize=18, fontname='Times New Roman', verticalalignment='top')\n",
    "\n",
    "    # 设置刻度\n",
    "    ax.set_xlim(x_min,x_max)\n",
    "    ax.set_ylim(y_min,0.9999)\n",
    "    ax.xaxis.set_major_locator(MaxNLocator(nbins=4))\n",
    "    ax.yaxis.set_major_locator(MaxNLocator(nbins=4))\n",
    "    ax.tick_params(axis='x', which='both', colors='black', length=3, width=1.5, labelsize=14, direction='out', pad=2, bottom=True, top=False, labelbottom=True,labeltop=False)\n",
    "    ax.tick_params(axis='y', which='both', colors='black', length=3, width=1.5, labelsize=14, direction='out', pad=6, left=True, right=False, labelleft=True,labelright=False)\n",
    "    # 设置轴标签\n",
    "    ax.set_xlabel('Original mean SM (m³/m³)', size=16, fontname='Times New Roman')\n",
    "    ax.set_ylabel('Pearson R',size=16)\n",
    "    plt.setp(ax.get_xticklabels(), rotation=0,rotation_mode=\"anchor\")\n",
    "    plt.setp(ax.get_yticklabels(), rotation=90,ha=\"center\", rotation_mode=\"anchor\")\n",
    "    # 设置轴参数\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['bottom'].set_color('black')\n",
    "    ax.spines['bottom'].set_linewidth(1.5)\n",
    "    ax.spines['left'].set_color('black')\n",
    "    ax.spines['left'].set_linewidth(1.5)\n",
    "    ax.grid(True, color='black', linestyle='--',alpha=0.5,zorder=0.1)\n",
    "\n",
    "def draw_subplot_up(ax, Original_mean, r_value, color):\n",
    "    r_value = r_value.flatten()\n",
    "    Original_mean = Original_mean.flatten()\n",
    "    mask = ~np.isnan(r_value) & ~np.isnan(Original_mean)\n",
    "    x = Original_mean[mask]\n",
    "    y = r_value[mask]\n",
    "    x_min,x_max = np.percentile(x, 1), np.percentile(x, 99)\n",
    "\n",
    "    step = 0.001\n",
    "    mask = ~np.isnan(Original_mean) & ~np.isnan(r_value)\n",
    "    Original_mean = Original_mean[mask].flatten()\n",
    "    min_val, max_val = np.min(Original_mean), np.max(Original_mean)\n",
    "    bins = np.arange(min_val, max_val + step, step)\n",
    "    counts, _ = np.histogram(Original_mean, bins=bins)\n",
    "    centers = (bins[:-1] + bins[1:]) / 2\n",
    "    counts_min,counts_max = min(counts),max(counts)\n",
    "    ax.bar(centers, counts, width=step, color=color, edgecolor=color, alpha=0.8)\n",
    "    ax.set_ylim(0,counts_max)\n",
    "    ax.set_xlim(x_min,x_max)\n",
    "    clean_axis(ax)\n",
    "\n",
    "def draw_subplot_left(ax, Original_mean, r_value, color):\n",
    "    r_value = r_value.flatten()\n",
    "    Original_mean = Original_mean.flatten()\n",
    "    mask = ~np.isnan(r_value) & ~np.isnan(Original_mean)\n",
    "    x = Original_mean[mask]\n",
    "    y = r_value[mask]\n",
    "\n",
    "    step = 0.001\n",
    "    mask = ~np.isnan(r_value) & ~np.isnan(Original_mean)\n",
    "    r_value = r_value[mask].flatten()\n",
    "    min_val, max_val = np.min(r_value), np.max(r_value)\n",
    "    bins = np.arange(min_val, max_val + step, step)\n",
    "    counts, _ = np.histogram(r_value, bins=bins)\n",
    "    centers = (bins[:-1] + bins[1:]) / 2\n",
    "    ax.barh(centers, counts, height=step, color=color, edgecolor=color, alpha=0.8)\n",
    "    counts_min,counts_max = min(counts),max(counts)\n",
    "    y_min,y_max = np.percentile(y, 5), np.percentile(y, 95)\n",
    "    ax.set_xlim(0, counts_max)\n",
    "    ax.set_ylim(y_min, 0.9999)\n",
    "    clean_axis(ax)\n",
    "metric = np.load(r\"C:\\Users\\Administrator\\Desktop\\Draw\\reconstruction_result.npy\",allow_pickle=True).item() \n",
    "create_subplots(num_models, Original_mean,r_values, lon_range, lat_range, model_names, metric, plt_name = 'a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (绘图) V2.0 4*2 密度散点图 不同模型间对比 -完整绘制\n",
    "SM不按均值区间分割--在v1版本上添加密度刻度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.collections import PolyCollection\n",
    "import matplotlib.colors as mcolors\n",
    "from scipy.stats import gaussian_kde\n",
    "from matplotlib.colors import to_rgba\n",
    "from matplotlib.font_manager import FontProperties\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "def clean_axis(ax):\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['bottom'].set_visible(False)\n",
    "    ax.spines['left'].set_visible(False)\n",
    "    ax.tick_params(axis='x', which='both', length=0, labelbottom=False)\n",
    "    ax.tick_params(axis='y', which='both', length=0, labelleft=False)\n",
    "    ax.grid(False)\n",
    "\n",
    "def create_subplots(num_models, Original_mean, r_values, model_names,  metric, plt_name=None):\n",
    "    sns.set(style=\"whitegrid\", font='Times New Roman')\n",
    "    matplotlib.rcParams['font.family'] = 'Times New Roman'\n",
    "    matplotlib.rcParams['axes.labelweight'] = 'bold'\n",
    "\n",
    "    # 创建图形对象\n",
    "    fig = plt.figure(figsize=(18, 24))\n",
    "\n",
    "    ratio_h = 4 # 竖直比例\n",
    "    ratio_v = 3 # 水平比例\n",
    "\n",
    "    absolute_dis_h = 0.45/ratio_h # 中央子图绝对大小（竖直）0.15\n",
    "    absolute_dis_v = 0.45/ratio_v # 中央子图绝对大小（水平）0.2\n",
    "\n",
    "    edge_h = 0.29 # 竖直边缘间隙\n",
    "    edge_v = 0.1 # 水平边缘间隙\n",
    "\n",
    "    mid_h = (1-edge_h*2-absolute_dis_h*3)/2 # 竖直间隙,可以通过竖直边缘间隙调节\n",
    "    mid_v = (1-edge_v*2-absolute_dis_v*4)/3 # 水平间隙……\n",
    "\n",
    "    right_offset_first = 0 # 第一层向右偏移量\n",
    "\n",
    "    up_offset = 0.1129 # 上方子图向上偏移\n",
    "    right_offset = 0.1505 # 右方子图向右偏移\n",
    "    up_ratio = 0.12  # 上方子图长度比例\n",
    "    right_ratio = 0.25  # 上方子图宽度比例\n",
    "\n",
    "    # 计算第一层图起点:上层子图  左起点、下起点、宽度比例、高度比例\n",
    "    ax2 = fig.add_axes([edge_v + right_offset_first, 1 - edge_h - absolute_dis_h + up_offset, absolute_dis_v, absolute_dis_h * up_ratio])\n",
    "    ax3 = fig.add_axes([edge_v + right_offset_first + right_offset, 1 - edge_h - absolute_dis_h, absolute_dis_v * right_ratio, absolute_dis_h])\n",
    "    ax1 = fig.add_axes([edge_v + right_offset_first, 1 - edge_h - absolute_dis_h, absolute_dis_v, absolute_dis_h])\n",
    "\n",
    "    ax5 = fig.add_axes([edge_v + right_offset_first + absolute_dis_v + mid_v, 1 - edge_h - absolute_dis_h + up_offset, absolute_dis_v, absolute_dis_h * up_ratio])\n",
    "    ax6 = fig.add_axes([edge_v + right_offset_first + right_offset + absolute_dis_v + mid_v, 1 - edge_h - absolute_dis_h, absolute_dis_v * right_ratio, absolute_dis_h])\n",
    "    ax4 = fig.add_axes([edge_v + right_offset_first + absolute_dis_v + mid_v, 1 - edge_h - absolute_dis_h, absolute_dis_v, absolute_dis_h])\n",
    "\n",
    "    ax8 = fig.add_axes([edge_v + right_offset_first + 2*(absolute_dis_v + mid_v), 1 - edge_h - absolute_dis_h + up_offset, absolute_dis_v, absolute_dis_h * up_ratio])\n",
    "    ax9 = fig.add_axes([edge_v + right_offset_first + right_offset + 2*(absolute_dis_v + mid_v), 1 - edge_h - absolute_dis_h, absolute_dis_v * right_ratio, absolute_dis_h])\n",
    "    ax7 = fig.add_axes([edge_v + right_offset_first + 2*(absolute_dis_v + mid_v), 1 - edge_h - absolute_dis_h, absolute_dis_v, absolute_dis_h])\n",
    "\n",
    "    ax11 = fig.add_axes([edge_v + right_offset_first + 3*(absolute_dis_v + mid_v), 1 - edge_h - absolute_dis_h + up_offset, absolute_dis_v, absolute_dis_h * up_ratio])\n",
    "    ax12 = fig.add_axes([edge_v + right_offset_first + right_offset + 3*(absolute_dis_v + mid_v), 1 - edge_h - absolute_dis_h, absolute_dis_v * right_ratio, absolute_dis_h])\n",
    "    ax10 = fig.add_axes([edge_v + right_offset_first + 3*(absolute_dis_v + mid_v) , 1 - edge_h - absolute_dis_h, absolute_dis_v, absolute_dis_h])\n",
    "\n",
    "    # 中层子图\n",
    "    ax14 = fig.add_axes([edge_v, 1 - edge_h - 2*absolute_dis_h - mid_h + up_offset, absolute_dis_v, absolute_dis_h * up_ratio])\n",
    "    ax15 = fig.add_axes([edge_v + right_offset, 1 - edge_h - 2*absolute_dis_h - mid_h, absolute_dis_v * right_ratio, absolute_dis_h]) \n",
    "    ax13 = fig.add_axes([edge_v , 1 - edge_h - 2*absolute_dis_h - mid_h, absolute_dis_v, absolute_dis_h])\n",
    "\n",
    "    ax17 = fig.add_axes([edge_v + absolute_dis_v + mid_v, 1 - edge_h - 2*absolute_dis_h - mid_h + up_offset, absolute_dis_v, absolute_dis_h * up_ratio])\n",
    "    ax18 = fig.add_axes([edge_v + right_offset + absolute_dis_v + mid_v, 1 - edge_h - 2*absolute_dis_h - mid_h, absolute_dis_v * right_ratio, absolute_dis_h])\n",
    "    ax16 = fig.add_axes([edge_v + absolute_dis_v + mid_v, 1 - edge_h - 2*absolute_dis_h - mid_h, absolute_dis_v, absolute_dis_h])\n",
    "\n",
    "    ax20 = fig.add_axes([edge_v + 2*(absolute_dis_v + mid_v), 1 - edge_h - 2*absolute_dis_h - mid_h + up_offset, absolute_dis_v, absolute_dis_h * up_ratio])\n",
    "    ax21 = fig.add_axes([edge_v + right_offset + 2*(absolute_dis_v + mid_v), 1 - edge_h - 2*absolute_dis_h - mid_h, absolute_dis_v * right_ratio, absolute_dis_h])\n",
    "    ax19 = fig.add_axes([edge_v + 2*(absolute_dis_v + mid_v), 1 - edge_h - 2*absolute_dis_h - mid_h, absolute_dis_v, absolute_dis_h])\n",
    "\n",
    "    ax23 = fig.add_axes([edge_v + 3*(absolute_dis_v + mid_v), 1 - edge_h - 2*absolute_dis_h - mid_h + up_offset, absolute_dis_v, absolute_dis_h * up_ratio])\n",
    "    ax24 = fig.add_axes([edge_v + right_offset + 3*(absolute_dis_v + mid_v), 1 - edge_h - 2*absolute_dis_h - mid_h, absolute_dis_v * right_ratio, absolute_dis_h])\n",
    "    ax22 = fig.add_axes([edge_v + 3*(absolute_dis_v + mid_v), 1 - edge_h - 2*absolute_dis_h - mid_h, absolute_dis_v, absolute_dis_h])\n",
    "\n",
    "    axes = [ax1, ax2, ax3, ax4, ax5, ax6, ax7, ax8, ax9, ax10, ax11, ax12, ax13, ax14, ax15, ax16, ax17, ax18, ax19, ax20, ax21, ax22, ax23, ax24]\n",
    "\n",
    "    # 上左色盘 \n",
    "    colors = ['#36134A', '#103783', '#003C50', '#004D40', '#36134A', '#103783', '#003C50', '#004D40']\n",
    "\n",
    "    # 中间渐变色色条\n",
    "    color_scheme1 = ['#B7A0C4', '#36134A']\n",
    "    color_scheme2 = ['#9bafd9', '#103783']\n",
    "    color_scheme3 = ['#75CCD8', '#00505F']\n",
    "    color_scheme4 = ['#80CBC4', '#004D40']\n",
    "    color_schemes = [color_scheme1, color_scheme2, color_scheme3, color_scheme4]\n",
    "\n",
    "    r_set = metric['R']\n",
    "    bias_set = metric['Bias']\n",
    "    rmse_set = metric['RMSE']\n",
    "    ubrmse_set = metric['ubRMSE']\n",
    "    \n",
    "    for i in range(num_models):\n",
    "        color_scheme = color_schemes[i % 4]\n",
    "        draw_subplot(axes[3*i], Original_mean, r_values[:,:,i], model_names[i], color_scheme,r_set[i],bias_set[i],rmse_set[i],ubrmse_set[i])\n",
    "        draw_subplot_up(axes[3*i+1], Original_mean, r_values[:,:,i], colors[i])\n",
    "        draw_subplot_left(axes[3*i+2], Original_mean, r_values[:,:,i], colors[i])  \n",
    "    \n",
    "    fig.savefig(r\"C:\\Users\\Administrator\\Desktop\\Draw\\sat_original.jpeg\", dpi=1000, format='jpeg', bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def draw_subplot(ax, Original_mean, r_value, model_name, color_scheme, r, bias, rmse, ubrmse):\n",
    "    # 使用了selected_model_names\n",
    "    cmap = mcolors.LinearSegmentedColormap.from_list('custom_cmap', color_scheme)\n",
    "    font_prop = FontProperties(family='Times New Roman', size=14)\n",
    "    r_value = r_value.flatten()\n",
    "    Original_mean = Original_mean.flatten()\n",
    "    mask = ~np.isnan(r_value) & ~np.isnan(Original_mean)\n",
    "    x = Original_mean[mask]\n",
    "    y = r_value[mask]\n",
    "\n",
    "    xy = np.vstack([x, y])\n",
    "    z = gaussian_kde(xy)(xy)\n",
    "\n",
    "    scatter = ax.scatter(x, y, c=z, s=8, cmap=cmap, edgecolor='face', linewidth=0, alpha=1)\n",
    "    sns.regplot(x=x, y=y, ax=ax, order=3, scatter=False, color='#522226', line_kws={'label':f'Quad Fit: {model_name}'}, ci=99)\n",
    "    for artist in ax.findobj(PolyCollection):\n",
    "        artist.set_alpha(0.4)\n",
    "\n",
    "    x_min,x_max = np.percentile(x, 1), np.percentile(x, 99)\n",
    "    y_min,y_max = np.percentile(y, 1), np.percentile(y, 99)\n",
    "\n",
    "    padding_x = (x_max - x_min) * 0.01\n",
    "    padding_y = (0.9999 - y_min) * 0.01\n",
    "\n",
    "    # 密度散点图bar\n",
    "    norm = matplotlib.colors.Normalize(vmin=np.min(z), vmax=np.max(z))\n",
    "    sm = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\n",
    "    sm.set_array([])\n",
    "\n",
    "    cax = ax.inset_axes([0.05, 0.95, 0.6, 0.04], transform=ax.transAxes)\n",
    "    cbar = plt.colorbar(sm, cax=cax, orientation='horizontal', format='%d', extend='both', alpha=1)\n",
    "\n",
    "    cbar.outline.set_visible(False)\n",
    "    cbar.ax.tick_params(colors='black', length=2, width=1.5, labelsize=12, direction='out', pad=2, bottom=True, top=False, labelbottom=True, labeltop=False)\n",
    "\n",
    "    cbar.ax.xaxis.set_major_locator(MaxNLocator(nbins=4))\n",
    "    cbar.solids.set_edgecolor(\"face\")\n",
    "\n",
    "\n",
    "    # 指标文本\n",
    "    x_range = np.arange(0,4)*6\n",
    "    text_position_x = x_max - padding_x * 0.5\n",
    "    text_position_y_r = y_min + padding_y * x_range[3] + padding_y * 0.5\n",
    "    text_position_y_bias = y_min + padding_y * x_range[2] + padding_y * 0.5\n",
    "    text_position_y_rmse = y_min + padding_y * x_range[1] + padding_y * 0.5\n",
    "    text_position_y_ubrmse = y_min + padding_y * x_range[0] + padding_y * 0.5\n",
    "    # 使用FontProperties实例绘制文本，去除$符号避免斜体\n",
    "    ax.text(text_position_x, text_position_y_r, f'R={r:.3f}', fontproperties=font_prop, horizontalalignment='right', verticalalignment='bottom')\n",
    "    ax.text(text_position_x, text_position_y_rmse, f'RMSE={rmse:.3f}', fontproperties=font_prop, horizontalalignment='right', verticalalignment='bottom')\n",
    "    ax.text(text_position_x, text_position_y_bias, f'bias={bias:.3f}', fontproperties=font_prop, horizontalalignment='right', verticalalignment='bottom')\n",
    "    ax.text(text_position_x, text_position_y_ubrmse, f'ubRMSE={ubrmse:.3f}', fontproperties=font_prop, horizontalalignment='right', verticalalignment='bottom')\n",
    "    prefix = \"(\" + chr(ord('a') + selected_model_names.index(model_name)) + \")\"\n",
    "    if len(model_name)>11:\n",
    "        ax.text(0.01, 0.21, f'{prefix}', transform=ax.transAxes, fontsize=18, fontname='Times New Roman', verticalalignment='top')\n",
    "        ax.text(0.01, 0.11, f'{model_name}', transform=ax.transAxes, fontsize=17, fontname='Times New Roman', verticalalignment='top')\n",
    "    else:\n",
    "        ax.text(0.01, 0.11, f'{prefix} {model_name}', transform=ax.transAxes, fontsize=17, fontname='Times New Roman', verticalalignment='top')\n",
    "    # 设置刻度\n",
    "    ax.set_xlim(x_min,x_max)\n",
    "    ax.set_ylim(y_min,0.9999)\n",
    "    ax.xaxis.set_major_locator(MaxNLocator(nbins=4))\n",
    "    ax.yaxis.set_major_locator(MaxNLocator(nbins=4))\n",
    "    ax.tick_params(axis='x', which='both', colors='black', length=3, width=1.5, labelsize=14, direction='out', pad=2, bottom=True, top=False, labelbottom=True,labeltop=False)\n",
    "    ax.tick_params(axis='y', which='both', colors='black', length=3, width=1.5, labelsize=14, direction='out', pad=6, left=True, right=False, labelleft=True,labelright=False)\n",
    "    # 设置轴标签\n",
    "    ax.set_xlabel('Original mean SM (m³/m³)', size=16, fontname='Times New Roman')\n",
    "    ax.set_ylabel('Pearson R',size=16)\n",
    "    plt.setp(ax.get_xticklabels(), rotation=0,rotation_mode=\"anchor\")\n",
    "    plt.setp(ax.get_yticklabels(), rotation=90,ha=\"center\", rotation_mode=\"anchor\")\n",
    "    # 设置轴参数\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['bottom'].set_color('black')\n",
    "    ax.spines['bottom'].set_linewidth(1.5)\n",
    "    ax.spines['left'].set_color('black')\n",
    "    ax.spines['left'].set_linewidth(1.5)\n",
    "    ax.grid(True, color='black', linestyle='--',alpha=0.5,zorder=0.1)\n",
    "\n",
    "def draw_subplot_up(ax, Original_mean, r_value, color):\n",
    "    r_value = r_value.flatten()\n",
    "    Original_mean = Original_mean.flatten()\n",
    "    mask = ~np.isnan(r_value) & ~np.isnan(Original_mean)\n",
    "    x = Original_mean[mask]\n",
    "    y = r_value[mask]\n",
    "    x_min,x_max = np.percentile(x, 1), np.percentile(x, 99)\n",
    "\n",
    "    step = 0.001\n",
    "    mask = ~np.isnan(Original_mean) & ~np.isnan(r_value)\n",
    "    Original_mean = Original_mean[mask].flatten()\n",
    "    min_val, max_val = np.min(Original_mean), np.max(Original_mean)\n",
    "    bins = np.arange(min_val, max_val + step, step)\n",
    "    counts, _ = np.histogram(Original_mean, bins=bins)\n",
    "    centers = (bins[:-1] + bins[1:]) / 2\n",
    "    counts_min,counts_max = min(counts),max(counts)\n",
    "    ax.bar(centers, counts, width=step, color=color, edgecolor=color, alpha=0.8)\n",
    "    ax.set_ylim(0,counts_max)\n",
    "    ax.set_xlim(x_min,x_max)\n",
    "    clean_axis(ax)\n",
    "\n",
    "def draw_subplot_left(ax, Original_mean, r_value, color):\n",
    "    r_value = r_value.flatten()\n",
    "    Original_mean = Original_mean.flatten()\n",
    "    mask = ~np.isnan(r_value) & ~np.isnan(Original_mean)\n",
    "    x = Original_mean[mask]\n",
    "    y = r_value[mask]\n",
    "\n",
    "    step = 0.001\n",
    "    mask = ~np.isnan(r_value) & ~np.isnan(Original_mean)\n",
    "    r_value = r_value[mask].flatten()\n",
    "    min_val, max_val = np.min(r_value), np.max(r_value)\n",
    "    bins = np.arange(min_val, max_val + step, step)\n",
    "    counts, _ = np.histogram(r_value, bins=bins)\n",
    "    centers = (bins[:-1] + bins[1:]) / 2\n",
    "    ax.barh(centers, counts, height=step, color=color, edgecolor=color, alpha=0.8)\n",
    "    counts_min,counts_max = min(counts),max(counts)\n",
    "    y_min,y_max = np.percentile(y, 1), np.percentile(y, 99)\n",
    "    ax.set_xlim(0, counts_max)\n",
    "    ax.set_ylim(y_min, 0.9999)\n",
    "\n",
    "    # 轴绘制\n",
    "    clean_axis(ax)\n",
    "    ax.spines['bottom'].set_visible(True)\n",
    "    ax.spines['bottom'].set_color('black')\n",
    "    ax.spines['bottom'].set_linewidth(1.5)\n",
    "    ax.tick_params(axis='x', which='both', colors='black', length=4, width=1.5, labelsize=14, direction='inout', pad=2, bottom=True, top=False, labelbottom=True, labeltop=False)\n",
    "    ax.set_xticks([counts_max])\n",
    "    ax.set_xticklabels([str(counts_max)])\n",
    "\n",
    "metric = np.load(r\"C:\\Users\\Administrator\\Desktop\\Draw\\reconstruction_result.npy\",allow_pickle=True).item() \n",
    "create_subplots(num_models, Original_mean,r_values, selected_model_names, metric, plt_name = 'a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (绘图) V3.0 5*2 密度散点图 不同模型间对比 -完整绘制\n",
    "SM不按均值区间分割--在v1版本上添加密度刻度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.collections import PolyCollection\n",
    "import matplotlib.colors as mcolors\n",
    "from scipy.stats import gaussian_kde\n",
    "from matplotlib.colors import to_rgba\n",
    "from matplotlib.font_manager import FontProperties\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "def clean_axis(ax):\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['bottom'].set_visible(False)\n",
    "    ax.spines['left'].set_visible(False)\n",
    "    ax.tick_params(axis='x', which='both', length=0, labelbottom=False)\n",
    "    ax.tick_params(axis='y', which='both', length=0, labelleft=False)\n",
    "    ax.grid(False)\n",
    "\n",
    "def create_subplots(num_models, Original_mean, r_values, model_names,  metric, plt_name=None):\n",
    "    sns.set(style=\"whitegrid\", font='Times New Roman')\n",
    "    matplotlib.rcParams['font.family'] = 'Times New Roman'\n",
    "    matplotlib.rcParams['axes.labelweight'] = 'bold'\n",
    "\n",
    "    # 创建图形对象\n",
    "    fig = plt.figure(figsize=(18, 24))\n",
    "\n",
    "    ratio_h = 4 # 竖直比例\n",
    "    ratio_v = 3 # 水平比例\n",
    "\n",
    "    absolute_dis_h = 0.36/ratio_h # 中央子图绝对大小（竖直）0.15\n",
    "    absolute_dis_v = 0.36/ratio_v # 中央子图绝对大小（水平）0.2\n",
    "\n",
    "    edge_h = 0.33 # 竖直边缘间隙\n",
    "    edge_v = 0.1 # 水平边缘间隙\n",
    "\n",
    "    mid_h = (1-edge_h*2-absolute_dis_h*3)/2 # 竖直间隙,可以通过竖直边缘间隙调节\n",
    "    mid_v = (1-edge_v*2-absolute_dis_v*5)/4 # 水平间隙……\n",
    "\n",
    "    right_offset_first = 0 # 第一层向右偏移量\n",
    "\n",
    "    up_offset = 0.0900 # 上方子图向上偏移\n",
    "    right_offset = 0.1200 # 右方子图向右偏移\n",
    "    up_ratio = 0.12  # 上方子图长度比例\n",
    "    right_ratio = 0.25  # 上方子图宽度比例\n",
    "\n",
    "    # 计算第一层图起点:上层子图  左起点、下起点、宽度比例、高度比例\n",
    "    # 上左一\n",
    "    ax2 = fig.add_axes([edge_v + right_offset_first, 1 - edge_h - absolute_dis_h + up_offset, absolute_dis_v, absolute_dis_h * up_ratio])\n",
    "    ax3 = fig.add_axes([edge_v + right_offset_first + right_offset, 1 - edge_h - absolute_dis_h, absolute_dis_v * right_ratio, absolute_dis_h])\n",
    "    ax1 = fig.add_axes([edge_v + right_offset_first, 1 - edge_h - absolute_dis_h, absolute_dis_v, absolute_dis_h])\n",
    "\n",
    "    ax5 = fig.add_axes([edge_v + right_offset_first + absolute_dis_v + mid_v, 1 - edge_h - absolute_dis_h + up_offset, absolute_dis_v, absolute_dis_h * up_ratio])\n",
    "    ax6 = fig.add_axes([edge_v + right_offset_first + right_offset + absolute_dis_v + mid_v, 1 - edge_h - absolute_dis_h, absolute_dis_v * right_ratio, absolute_dis_h])\n",
    "    ax4 = fig.add_axes([edge_v + right_offset_first + absolute_dis_v + mid_v, 1 - edge_h - absolute_dis_h, absolute_dis_v, absolute_dis_h])\n",
    "\n",
    "    ax8 = fig.add_axes([edge_v + right_offset_first + 2*(absolute_dis_v + mid_v), 1 - edge_h - absolute_dis_h + up_offset, absolute_dis_v, absolute_dis_h * up_ratio])\n",
    "    ax9 = fig.add_axes([edge_v + right_offset_first + right_offset + 2*(absolute_dis_v + mid_v), 1 - edge_h - absolute_dis_h, absolute_dis_v * right_ratio, absolute_dis_h])\n",
    "    ax7 = fig.add_axes([edge_v + right_offset_first + 2*(absolute_dis_v + mid_v), 1 - edge_h - absolute_dis_h, absolute_dis_v, absolute_dis_h])\n",
    "\n",
    "    ax11 = fig.add_axes([edge_v + right_offset_first + 3*(absolute_dis_v + mid_v), 1 - edge_h - absolute_dis_h + up_offset, absolute_dis_v, absolute_dis_h * up_ratio])\n",
    "    ax12 = fig.add_axes([edge_v + right_offset_first + right_offset + 3*(absolute_dis_v + mid_v), 1 - edge_h - absolute_dis_h, absolute_dis_v * right_ratio, absolute_dis_h])\n",
    "    ax10 = fig.add_axes([edge_v + right_offset_first + 3*(absolute_dis_v + mid_v) , 1 - edge_h - absolute_dis_h, absolute_dis_v, absolute_dis_h])\n",
    "\n",
    "    ax14 = fig.add_axes([edge_v + right_offset_first + 4*(absolute_dis_v + mid_v), 1 - edge_h - absolute_dis_h + up_offset, absolute_dis_v, absolute_dis_h * up_ratio])\n",
    "    ax15 = fig.add_axes([edge_v + right_offset_first + right_offset + 4*(absolute_dis_v + mid_v), 1 - edge_h - absolute_dis_h, absolute_dis_v * right_ratio, absolute_dis_h])\n",
    "    ax13 = fig.add_axes([edge_v + right_offset_first + 4*(absolute_dis_v + mid_v) , 1 - edge_h - absolute_dis_h, absolute_dis_v, absolute_dis_h])\n",
    "\n",
    "    # 下左一\n",
    "    ax17= fig.add_axes([edge_v, 1 - edge_h - 2*absolute_dis_h - mid_h + up_offset, absolute_dis_v, absolute_dis_h * up_ratio])\n",
    "    ax18 = fig.add_axes([edge_v + right_offset, 1 - edge_h - 2*absolute_dis_h - mid_h, absolute_dis_v * right_ratio, absolute_dis_h]) \n",
    "    ax16 = fig.add_axes([edge_v , 1 - edge_h - 2*absolute_dis_h - mid_h, absolute_dis_v, absolute_dis_h])\n",
    "\n",
    "    ax20 = fig.add_axes([edge_v + absolute_dis_v + mid_v, 1 - edge_h - 2*absolute_dis_h - mid_h + up_offset, absolute_dis_v, absolute_dis_h * up_ratio])\n",
    "    ax21 = fig.add_axes([edge_v + right_offset + absolute_dis_v + mid_v, 1 - edge_h - 2*absolute_dis_h - mid_h, absolute_dis_v * right_ratio, absolute_dis_h])\n",
    "    ax19 = fig.add_axes([edge_v + absolute_dis_v + mid_v, 1 - edge_h - 2*absolute_dis_h - mid_h, absolute_dis_v, absolute_dis_h])\n",
    "\n",
    "    ax23 = fig.add_axes([edge_v + 2*(absolute_dis_v + mid_v), 1 - edge_h - 2*absolute_dis_h - mid_h + up_offset, absolute_dis_v, absolute_dis_h * up_ratio])\n",
    "    ax24 = fig.add_axes([edge_v + right_offset + 2*(absolute_dis_v + mid_v), 1 - edge_h - 2*absolute_dis_h - mid_h, absolute_dis_v * right_ratio, absolute_dis_h])\n",
    "    ax22 = fig.add_axes([edge_v + 2*(absolute_dis_v + mid_v), 1 - edge_h - 2*absolute_dis_h - mid_h, absolute_dis_v, absolute_dis_h])\n",
    "\n",
    "    ax26 = fig.add_axes([edge_v + 3*(absolute_dis_v + mid_v), 1 - edge_h - 2*absolute_dis_h - mid_h + up_offset, absolute_dis_v, absolute_dis_h * up_ratio])\n",
    "    ax27 = fig.add_axes([edge_v + right_offset + 3*(absolute_dis_v + mid_v), 1 - edge_h - 2*absolute_dis_h - mid_h, absolute_dis_v * right_ratio, absolute_dis_h])\n",
    "    ax25 = fig.add_axes([edge_v + 3*(absolute_dis_v + mid_v), 1 - edge_h - 2*absolute_dis_h - mid_h, absolute_dis_v, absolute_dis_h])\n",
    "\n",
    "    ax29 = fig.add_axes([edge_v + 4*(absolute_dis_v + mid_v), 1 - edge_h - 2*absolute_dis_h - mid_h + up_offset, absolute_dis_v, absolute_dis_h * up_ratio])\n",
    "    ax30 = fig.add_axes([edge_v + right_offset + 4*(absolute_dis_v + mid_v), 1 - edge_h - 2*absolute_dis_h - mid_h, absolute_dis_v * right_ratio, absolute_dis_h])\n",
    "    ax28 = fig.add_axes([edge_v + 4*(absolute_dis_v + mid_v), 1 - edge_h - 2*absolute_dis_h - mid_h, absolute_dis_v, absolute_dis_h])\n",
    "\n",
    "    axes = [ax1, ax2, ax3, ax4, ax5, ax6, ax7, ax8, ax9, ax10, ax11, ax12, ax13, ax14, ax15, ax16, ax17, ax18, ax19, ax20, ax21, ax22, ax23, ax24, ax25, ax26, ax27, ax28, ax29, ax30]\n",
    "\n",
    "    # 上左色盘 \n",
    "    colors = ['#36134A', '#103783', '#003C50', '#004D40', '#375700', '#36134A', '#103783', '#003C50', '#004D40', '#375700']\n",
    "\n",
    "    # 中间渐变色色条\n",
    "    color_scheme1 = ['#B7A0C4', '#36134A']\n",
    "    color_scheme2 = ['#9bafd9', '#103783']\n",
    "    color_scheme3 = ['#75CCD8', '#00505F']\n",
    "    color_scheme4 = ['#80CBC4', '#004D40']\n",
    "    color_scheme5 = ['#7AD732', '#375700']\n",
    "    color_schemes = [color_scheme1, color_scheme2, color_scheme3, color_scheme4, color_scheme5]\n",
    "\n",
    "    r_set = metric['R']\n",
    "    bias_set = metric['Bias']\n",
    "    rmse_set = metric['RMSE']\n",
    "    ubrmse_set = metric['ubRMSE']\n",
    "\n",
    "    for i in range(num_models):\n",
    "        color_scheme = color_schemes[i % 5]\n",
    "        draw_subplot(axes[3*i], Original_mean, r_values[:,:,i], model_names[i], color_scheme,r_set[i],bias_set[i],rmse_set[i],ubrmse_set[i])\n",
    "        draw_subplot_up(axes[3*i+1], Original_mean, r_values[:,:,i], colors[i])\n",
    "        draw_subplot_left(axes[3*i+2], Original_mean, r_values[:,:,i], colors[i])  \n",
    "    '''\n",
    "    color_scheme = color_schemes[1 % 5]\n",
    "    draw_subplot(axes[3*1], Original_mean, r_values[:,:,1], model_names[1], color_scheme,r_set[1],bias_set[1],rmse_set[1],ubrmse_set[1])\n",
    "    draw_subplot_up(axes[3*1+1], Original_mean, r_values[:,:,1], colors[1])\n",
    "    draw_subplot_left(axes[3*1+2], Original_mean, r_values[:,:,1], colors[1])  \n",
    "    '''\n",
    "    fig.savefig(r\"C:\\Users\\Administrator\\Desktop\\Draw\\sat_original.jpeg\", dpi=1000, format='jpeg', bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def draw_subplot(ax, Original_mean, r_value, model_name, color_scheme, r, bias, rmse, ubrmse):\n",
    "    # 使用了selected_model_names\n",
    "    cmap = mcolors.LinearSegmentedColormap.from_list('custom_cmap', color_scheme)\n",
    "    font_prop = FontProperties(family='Times New Roman', size=11)\n",
    "    r_value = r_value.flatten()\n",
    "    Original_mean = Original_mean.flatten()\n",
    "    mask = ~np.isnan(r_value) & ~np.isnan(Original_mean)\n",
    "    x = Original_mean[mask]\n",
    "    y = r_value[mask]\n",
    "\n",
    "    xy = np.vstack([x, y])\n",
    "    z = gaussian_kde(xy)(xy)\n",
    "\n",
    "    scatter = ax.scatter(x, y, c=z, s=6, cmap=cmap, edgecolor='face', linewidth=0, alpha=1)\n",
    "    sns.regplot(x=x, y=y, ax=ax, order=3, scatter=False, color='#522226', line_kws={'label':f'Quad Fit: {model_name}'}, ci=99)\n",
    "    for artist in ax.findobj(PolyCollection):\n",
    "        artist.set_alpha(0.4)\n",
    "\n",
    "    x_min,x_max = np.percentile(x, 1), np.percentile(x, 99)\n",
    "    y_min,y_max = np.percentile(y, 1), np.percentile(y, 99)\n",
    "\n",
    "    padding_x = (x_max - x_min) * 0.01\n",
    "    padding_y = (0.9999 - y_min) * 0.01\n",
    "\n",
    "    # 密度散点图bar\n",
    "    norm = matplotlib.colors.Normalize(vmin=np.min(z), vmax=np.max(z))\n",
    "    sm = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\n",
    "    sm.set_array([])\n",
    "\n",
    "    cax = ax.inset_axes([0.05, 0.95, 0.6, 0.04], transform=ax.transAxes)\n",
    "    cbar = plt.colorbar(sm, cax=cax, orientation='horizontal', format='%d', extend='both', alpha=1)\n",
    "\n",
    "    cbar.outline.set_visible(False)\n",
    "    cbar.ax.tick_params(colors='black', length=2, width=1.5, labelsize=12, direction='out', pad=2, bottom=True, top=False, labelbottom=True, labeltop=False)\n",
    "\n",
    "    cbar.ax.xaxis.set_major_locator(MaxNLocator(nbins=4))\n",
    "    cbar.solids.set_edgecolor(\"face\")\n",
    "\n",
    "\n",
    "    # 指标文本\n",
    "    x_range = np.arange(0,4)*6\n",
    "    text_position_x = x_max - padding_x * 0.5\n",
    "    text_position_y_r = y_min + padding_y * x_range[3] + padding_y * 0.5\n",
    "    text_position_y_bias = y_min + padding_y * x_range[2] + padding_y * 0.5\n",
    "    text_position_y_rmse = y_min + padding_y * x_range[1] + padding_y * 0.5\n",
    "    text_position_y_ubrmse = y_min + padding_y * x_range[0] + padding_y * 0.5\n",
    "    # 使用FontProperties实例绘制文本，去除$符号避免斜体\n",
    "    ax.text(text_position_x, text_position_y_r, f'R={r:.3f}', fontproperties=font_prop, horizontalalignment='right', verticalalignment='bottom')\n",
    "    ax.text(text_position_x, text_position_y_rmse, f'RMSE={rmse:.3f}', fontproperties=font_prop, horizontalalignment='right', verticalalignment='bottom')\n",
    "    ax.text(text_position_x, text_position_y_bias, f'bias={bias:.3f}', fontproperties=font_prop, horizontalalignment='right', verticalalignment='bottom')\n",
    "    ax.text(text_position_x, text_position_y_ubrmse, f'ubRMSE={ubrmse:.3f}', fontproperties=font_prop, horizontalalignment='right', verticalalignment='bottom')\n",
    "    prefix = \"(\" + chr(ord('a') + selected_model_names.index(model_name)) + \")\"\n",
    "    if len(model_name)>11:\n",
    "        ax.text(0.01, 0.11, f'{prefix}{model_name}', transform=ax.transAxes, fontsize=11, fontname='Times New Roman', verticalalignment='top')\n",
    "    else:\n",
    "        ax.text(0.01, 0.11, f'{prefix} {model_name}', transform=ax.transAxes, fontsize=14, fontname='Times New Roman', verticalalignment='top')\n",
    "    # 设置刻度\n",
    "    ax.set_xlim(x_min,x_max)\n",
    "    ax.set_ylim(y_min,0.9999)\n",
    "    ax.xaxis.set_major_locator(MaxNLocator(nbins=4))\n",
    "    ax.yaxis.set_major_locator(MaxNLocator(nbins=4))\n",
    "    ax.tick_params(axis='x', which='both', colors='black', length=3, width=1.5, labelsize=12, direction='out', pad=2, bottom=True, top=False, labelbottom=True,labeltop=False)\n",
    "    ax.tick_params(axis='y', which='both', colors='black', length=3, width=1.5, labelsize=12, direction='out', pad=4, left=True, right=False, labelleft=True,labelright=False)\n",
    "    # 设置轴标签\n",
    "    ax.set_xlabel('Original mean SM (m³/m³)', size=14, fontname='Times New Roman')\n",
    "    ax.set_ylabel('Pearson R',size=14, labelpad=1)\n",
    "    plt.setp(ax.get_xticklabels(), rotation=0,rotation_mode=\"anchor\")\n",
    "    plt.setp(ax.get_yticklabels(), rotation=90,ha=\"center\", rotation_mode=\"anchor\")\n",
    "    # 设置轴参数\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['bottom'].set_color('black')\n",
    "    ax.spines['bottom'].set_linewidth(1.5)\n",
    "    ax.spines['left'].set_color('black')\n",
    "    ax.spines['left'].set_linewidth(1.5)\n",
    "    ax.grid(True, color='black', linestyle='--',alpha=0.5,zorder=0.1)\n",
    "\n",
    "def draw_subplot_up(ax, Original_mean, r_value, color):\n",
    "    r_value = r_value.flatten()\n",
    "    Original_mean = Original_mean.flatten()\n",
    "    mask = ~np.isnan(r_value) & ~np.isnan(Original_mean)\n",
    "    x = Original_mean[mask]\n",
    "    y = r_value[mask]\n",
    "    x_min,x_max = np.percentile(x, 1), np.percentile(x, 99)\n",
    "\n",
    "    step = 0.001\n",
    "    mask = ~np.isnan(Original_mean) & ~np.isnan(r_value)\n",
    "    Original_mean = Original_mean[mask].flatten()\n",
    "    min_val, max_val = np.min(Original_mean), np.max(Original_mean)\n",
    "    bins = np.arange(min_val, max_val + step, step)\n",
    "    counts, _ = np.histogram(Original_mean, bins=bins)\n",
    "    centers = (bins[:-1] + bins[1:]) / 2\n",
    "    counts_min,counts_max = min(counts),max(counts)\n",
    "    ax.bar(centers, counts, width=step, color=color, edgecolor=color, alpha=0.8)\n",
    "    ax.set_ylim(0,counts_max)\n",
    "    ax.set_xlim(x_min,x_max)\n",
    "    clean_axis(ax)\n",
    "\n",
    "def draw_subplot_left(ax, Original_mean, r_value, color):\n",
    "    r_value = r_value.flatten()\n",
    "    Original_mean = Original_mean.flatten()\n",
    "    mask = ~np.isnan(r_value) & ~np.isnan(Original_mean)\n",
    "    x = Original_mean[mask]\n",
    "    y = r_value[mask]\n",
    "\n",
    "    step = 0.001\n",
    "    mask = ~np.isnan(r_value) & ~np.isnan(Original_mean)\n",
    "    r_value = r_value[mask].flatten()\n",
    "    min_val, max_val = np.min(r_value), np.max(r_value)\n",
    "    bins = np.arange(min_val, max_val + step, step)\n",
    "    counts, _ = np.histogram(r_value, bins=bins)\n",
    "    centers = (bins[:-1] + bins[1:]) / 2\n",
    "    ax.barh(centers, counts, height=step, color=color, edgecolor=color, alpha=0.8)\n",
    "    counts_min,counts_max = min(counts),max(counts)\n",
    "    y_min,y_max = np.percentile(y, 1), np.percentile(y, 99)\n",
    "    ax.set_xlim(0, counts_max)\n",
    "    ax.set_ylim(y_min, 0.9999)\n",
    "\n",
    "    # 轴绘制\n",
    "    clean_axis(ax)\n",
    "    ax.spines['bottom'].set_visible(True)\n",
    "    ax.spines['bottom'].set_color('black')\n",
    "    ax.spines['bottom'].set_linewidth(1.5)\n",
    "    ax.tick_params(axis='x', which='both', colors='black', length=4, width=1.5, labelsize=14, direction='inout', pad=2, bottom=True, top=False, labelbottom=True, labeltop=False)\n",
    "    ax.set_xticks([counts_max])\n",
    "    ax.set_xticklabels([str(counts_max)])\n",
    "\n",
    "metric = np.load(r\"C:\\Users\\Administrator\\Desktop\\Draw\\reconstruction_result.npy\",allow_pickle=True).item() \n",
    "create_subplots(num_models, Original_mean,r_values, selected_model_names, metric, plt_name = 'a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (绘图) V2.0 4*1 密度散点图 不同模型间对比 -完整绘制 \n",
    "SM不按均值区间分割--在v1版本上添加密度刻度\n",
    "使用前注意数据输入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.collections import PolyCollection\n",
    "import matplotlib.colors as mcolors\n",
    "from scipy.stats import gaussian_kde\n",
    "from matplotlib.colors import to_rgba\n",
    "from matplotlib.font_manager import FontProperties\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "def clean_axis(ax):\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['bottom'].set_visible(False)\n",
    "    ax.spines['left'].set_visible(False)\n",
    "    ax.tick_params(axis='x', which='both', length=0, labelbottom=False)\n",
    "    ax.tick_params(axis='y', which='both', length=0, labelleft=False)\n",
    "    ax.grid(False)\n",
    "\n",
    "def create_subplots(num_models, Original_mean, r_values, model_names,  metric, plt_name=None):\n",
    "    sns.set(style=\"whitegrid\", font='Times New Roman')\n",
    "    matplotlib.rcParams['font.family'] = 'Times New Roman'\n",
    "    matplotlib.rcParams['axes.labelweight'] = 'bold'\n",
    "\n",
    "    # 创建图形对象\n",
    "    fig = plt.figure(figsize=(18, 24))\n",
    "\n",
    "    ratio_h = 4 # 竖直比例\n",
    "    ratio_v = 3 # 水平比例\n",
    "\n",
    "    absolute_dis_h = 0.45/ratio_h # 中央子图绝对大小（竖直）0.15\n",
    "    absolute_dis_v = 0.45/ratio_v # 中央子图绝对大小（水平）0.2\n",
    "\n",
    "    edge_h = 0.29 # 竖直边缘间隙\n",
    "    edge_v = 0.1 # 水平边缘间隙\n",
    "\n",
    "    mid_h = 0   # 竖直间隙\n",
    "    mid_v = (1-edge_v*2-absolute_dis_v*4)/3 # 水平间隙……\n",
    "\n",
    "    up_offset = 0.1129 # 上方子图向上偏移\n",
    "    right_offset = 0.1505 # 右方子图向右偏移\n",
    "    up_ratio = 0.12  # 上方子图长度比例\n",
    "    right_ratio = 0.25  # 上方子图宽度比例\n",
    "\n",
    "    # 计算第一层图起点:上层子图  左起点、下起点、宽度比例、高度比例\n",
    "    ax2 = fig.add_axes([edge_v, 1 - edge_h - absolute_dis_h + up_offset, absolute_dis_v, absolute_dis_h * up_ratio])\n",
    "    ax3 = fig.add_axes([edge_v + right_offset, 1 - edge_h - absolute_dis_h, absolute_dis_v * right_ratio, absolute_dis_h])\n",
    "    ax1 = fig.add_axes([edge_v, 1 - edge_h - absolute_dis_h, absolute_dis_v, absolute_dis_h])\n",
    "\n",
    "    ax5 = fig.add_axes([edge_v + absolute_dis_v + mid_v, 1 - edge_h - absolute_dis_h + up_offset, absolute_dis_v, absolute_dis_h * up_ratio])\n",
    "    ax6 = fig.add_axes([edge_v + right_offset + absolute_dis_v + mid_v, 1 - edge_h - absolute_dis_h, absolute_dis_v * right_ratio, absolute_dis_h])\n",
    "    ax4 = fig.add_axes([edge_v + absolute_dis_v + mid_v, 1 - edge_h - absolute_dis_h, absolute_dis_v, absolute_dis_h])\n",
    "\n",
    "    ax8 = fig.add_axes([edge_v + 2*(absolute_dis_v + mid_v), 1 - edge_h - absolute_dis_h + up_offset, absolute_dis_v, absolute_dis_h * up_ratio])\n",
    "    ax9 = fig.add_axes([edge_v + right_offset + 2*(absolute_dis_v + mid_v), 1 - edge_h - absolute_dis_h, absolute_dis_v * right_ratio, absolute_dis_h])\n",
    "    ax7 = fig.add_axes([edge_v + 2*(absolute_dis_v + mid_v), 1 - edge_h - absolute_dis_h, absolute_dis_v, absolute_dis_h])\n",
    "\n",
    "    ax11 = fig.add_axes([edge_v + 3*(absolute_dis_v + mid_v), 1 - edge_h - absolute_dis_h + up_offset, absolute_dis_v, absolute_dis_h * up_ratio])\n",
    "    ax12 = fig.add_axes([edge_v + right_offset + 3*(absolute_dis_v + mid_v), 1 - edge_h - absolute_dis_h, absolute_dis_v * right_ratio, absolute_dis_h])\n",
    "    ax10 = fig.add_axes([edge_v + 3*(absolute_dis_v + mid_v) , 1 - edge_h - absolute_dis_h, absolute_dis_v, absolute_dis_h])\n",
    "\n",
    "\n",
    "    axes = [ax1, ax2, ax3, ax4, ax5, ax6, ax7, ax8, ax9, ax10, ax11, ax12]\n",
    "\n",
    "    # 上左色盘 \n",
    "    colors = ['#36134A', '#103783', '#003C50', '#004D40', '#36134A', '#103783', '#003C50', '#004D40']\n",
    "\n",
    "    # 中间渐变色色条\n",
    "    color_scheme1 = ['#B7A0C4', '#36134A']\n",
    "    color_scheme2 = ['#9bafd9', '#103783']\n",
    "    color_scheme3 = ['#75CCD8', '#00505F']\n",
    "    color_scheme4 = ['#80CBC4', '#004D40']\n",
    "    color_schemes = [color_scheme1, color_scheme2, color_scheme3, color_scheme4]\n",
    "\n",
    "    r_set = metric['R']\n",
    "    bias_set = metric['Bias']\n",
    "    rmse_set = metric['RMSE']\n",
    "    ubrmse_set = metric['ubRMSE']\n",
    "\n",
    "    for i in range(num_models):\n",
    "        color_scheme = color_schemes[i % 4]\n",
    "        draw_subplot(axes[3*i], Original_mean, r_values[:,:,i], model_names[i], color_scheme,r_set[i],bias_set[i],rmse_set[i],ubrmse_set[i])\n",
    "        draw_subplot_up(axes[3*i+1], Original_mean, r_values[:,:,i], colors[i])\n",
    "        draw_subplot_left(axes[3*i+2], Original_mean, r_values[:,:,i], colors[i])  \n",
    "\n",
    "    fig.savefig(r\"C:\\Users\\Administrator\\Desktop\\Draw\\sat_original_b.jpeg\", dpi=1000, format='jpeg', bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def draw_subplot(ax, Original_mean, r_value, model_name, color_scheme, r, bias, rmse, ubrmse):\n",
    "    # 使用了selected_model_names\n",
    "    cmap = mcolors.LinearSegmentedColormap.from_list('custom_cmap', color_scheme)\n",
    "    font_prop = FontProperties(family='Times New Roman', size=14)\n",
    "    r_value = r_value.flatten()\n",
    "    Original_mean = Original_mean.flatten()\n",
    "    mask = ~np.isnan(r_value) & ~np.isnan(Original_mean)\n",
    "    x = Original_mean[mask]\n",
    "    y = r_value[mask]\n",
    "\n",
    "    xy = np.vstack([x, y])\n",
    "    z = gaussian_kde(xy)(xy)\n",
    "\n",
    "    scatter = ax.scatter(x, y, c=z, s=8, cmap=cmap, edgecolor='face', linewidth=0, alpha=1)\n",
    "    sns.regplot(x=x, y=y, ax=ax, order=3, scatter=False, color='#522226', line_kws={'label':f'Quad Fit: {model_name}'}, ci=99)\n",
    "    for artist in ax.findobj(PolyCollection):\n",
    "        artist.set_alpha(0.4)\n",
    "\n",
    "    x_min,x_max = np.percentile(x, 1), np.percentile(x, 99)\n",
    "    y_min,y_max = np.percentile(y, 1), np.percentile(y, 99)\n",
    "\n",
    "    padding_x = (x_max - x_min) * 0.01\n",
    "    padding_y = (0.9999 - y_min) * 0.01\n",
    "\n",
    "    # 密度散点图bar\n",
    "    norm = matplotlib.colors.Normalize(vmin=np.min(z), vmax=np.max(z))\n",
    "    sm = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\n",
    "    sm.set_array([])\n",
    "\n",
    "    cax = ax.inset_axes([0.05, 0.95, 0.6, 0.04], transform=ax.transAxes)\n",
    "    cbar = plt.colorbar(sm, cax=cax, orientation='horizontal', format='%d', extend='both', alpha=1)\n",
    "\n",
    "    cbar.outline.set_visible(False)\n",
    "    cbar.ax.tick_params(colors='black', length=2, width=1.5, labelsize=12, direction='out', pad=2, bottom=True, top=False, labelbottom=True, labeltop=False)\n",
    "\n",
    "    cbar.ax.xaxis.set_major_locator(MaxNLocator(nbins=4))\n",
    "    cbar.solids.set_edgecolor(\"face\")\n",
    "\n",
    "\n",
    "    # 指标文本\n",
    "    x_range = np.arange(0,4)*6\n",
    "    text_position_x = x_max - padding_x * 0.5\n",
    "    text_position_y_r = y_min + padding_y * x_range[3] + padding_y * 0.5\n",
    "    text_position_y_bias = y_min + padding_y * x_range[2] + padding_y * 0.5\n",
    "    text_position_y_rmse = y_min + padding_y * x_range[1] + padding_y * 0.5\n",
    "    text_position_y_ubrmse = y_min + padding_y * x_range[0] + padding_y * 0.5\n",
    "    # 使用FontProperties实例绘制文本，去除$符号避免斜体\n",
    "    ax.text(text_position_x, text_position_y_r, f'R={r:.3f}', fontproperties=font_prop, horizontalalignment='right', verticalalignment='bottom')\n",
    "    ax.text(text_position_x, text_position_y_rmse, f'RMSE={rmse:.3f}', fontproperties=font_prop, horizontalalignment='right', verticalalignment='bottom')\n",
    "    ax.text(text_position_x, text_position_y_bias, f'bias={bias:.3f}', fontproperties=font_prop, horizontalalignment='right', verticalalignment='bottom')\n",
    "    ax.text(text_position_x, text_position_y_ubrmse, f'ubRMSE={ubrmse:.3f}', fontproperties=font_prop, horizontalalignment='right', verticalalignment='bottom')\n",
    "    prefix = \"(\" + chr(ord('a') + selected_model_names.index(model_name)) + \")\"\n",
    "    if len(model_name)>11:\n",
    "        ax.text(0.01, 0.21, f'{prefix}', transform=ax.transAxes, fontsize=18, fontname='Times New Roman', verticalalignment='top')\n",
    "        ax.text(0.01, 0.11, f'{model_name}', transform=ax.transAxes, fontsize=17, fontname='Times New Roman', verticalalignment='top')\n",
    "    else:\n",
    "        ax.text(0.01, 0.11, f'{prefix} {model_name}', transform=ax.transAxes, fontsize=17, fontname='Times New Roman', verticalalignment='top')\n",
    "    # 设置刻度\n",
    "    ax.set_xlim(x_min,x_max)\n",
    "    ax.set_ylim(y_min,0.9999)\n",
    "    ax.xaxis.set_major_locator(MaxNLocator(nbins=4))\n",
    "    ax.yaxis.set_major_locator(MaxNLocator(nbins=4))\n",
    "    ax.tick_params(axis='x', which='both', colors='black', length=3, width=1.5, labelsize=14, direction='out', pad=2, bottom=True, top=False, labelbottom=True,labeltop=False)\n",
    "    ax.tick_params(axis='y', which='both', colors='black', length=3, width=1.5, labelsize=14, direction='out', pad=6, left=True, right=False, labelleft=True,labelright=False)\n",
    "    # 设置轴标签\n",
    "    ax.set_xlabel('Original mean SM (m³/m³)', size=16, fontname='Times New Roman')\n",
    "    ax.set_ylabel('Pearson R',size=16)\n",
    "    plt.setp(ax.get_xticklabels(), rotation=0,rotation_mode=\"anchor\")\n",
    "    plt.setp(ax.get_yticklabels(), rotation=90,ha=\"center\", rotation_mode=\"anchor\")\n",
    "    # 设置轴参数\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['bottom'].set_color('black')\n",
    "    ax.spines['bottom'].set_linewidth(1.5)\n",
    "    ax.spines['left'].set_color('black')\n",
    "    ax.spines['left'].set_linewidth(1.5)\n",
    "    ax.grid(True, color='black', linestyle='--',alpha=0.5,zorder=0.1)\n",
    "\n",
    "def draw_subplot_up(ax, Original_mean, r_value, color):\n",
    "    r_value = r_value.flatten()\n",
    "    Original_mean = Original_mean.flatten()\n",
    "    mask = ~np.isnan(r_value) & ~np.isnan(Original_mean)\n",
    "    x = Original_mean[mask]\n",
    "    y = r_value[mask]\n",
    "    x_min,x_max = np.percentile(x, 1), np.percentile(x, 99)\n",
    "\n",
    "    step = 0.001\n",
    "    mask = ~np.isnan(Original_mean) & ~np.isnan(r_value)\n",
    "    Original_mean = Original_mean[mask].flatten()\n",
    "    min_val, max_val = np.min(Original_mean), np.max(Original_mean)\n",
    "    bins = np.arange(min_val, max_val + step, step)\n",
    "    counts, _ = np.histogram(Original_mean, bins=bins)\n",
    "    centers = (bins[:-1] + bins[1:]) / 2\n",
    "    counts_min,counts_max = min(counts),max(counts)\n",
    "    ax.bar(centers, counts, width=step, color=color, edgecolor=color, alpha=0.8)\n",
    "    ax.set_ylim(0,counts_max)\n",
    "    ax.set_xlim(x_min,x_max)\n",
    "    clean_axis(ax)\n",
    "\n",
    "def draw_subplot_left(ax, Original_mean, r_value, color):\n",
    "    r_value = r_value.flatten()\n",
    "    Original_mean = Original_mean.flatten()\n",
    "    mask = ~np.isnan(r_value) & ~np.isnan(Original_mean)\n",
    "    x = Original_mean[mask]\n",
    "    y = r_value[mask]\n",
    "\n",
    "    step = 0.001\n",
    "    mask = ~np.isnan(r_value) & ~np.isnan(Original_mean)\n",
    "    r_value = r_value[mask].flatten()\n",
    "    min_val, max_val = np.min(r_value), np.max(r_value)\n",
    "    bins = np.arange(min_val, max_val + step, step)\n",
    "    counts, _ = np.histogram(r_value, bins=bins)\n",
    "    centers = (bins[:-1] + bins[1:]) / 2\n",
    "    ax.barh(centers, counts, height=step, color=color, edgecolor=color, alpha=0.8)\n",
    "    counts_min,counts_max = min(counts),max(counts)\n",
    "    y_min,y_max = np.percentile(y, 1), np.percentile(y, 99)\n",
    "    ax.set_xlim(0, counts_max)\n",
    "    ax.set_ylim(y_min, 0.9999)\n",
    "\n",
    "    # 轴绘制\n",
    "    clean_axis(ax)\n",
    "    ax.spines['bottom'].set_visible(True)\n",
    "    ax.spines['bottom'].set_color('black')\n",
    "    ax.spines['bottom'].set_linewidth(1.5)\n",
    "    ax.tick_params(axis='x', which='both', colors='black', length=4, width=1.5, labelsize=14, direction='inout', pad=2, bottom=True, top=False, labelbottom=True, labeltop=False)\n",
    "    ax.set_xticks([counts_max])\n",
    "    ax.set_xticklabels([str(counts_max)])\n",
    "    \n",
    "\n",
    "metric = np.load(r\"C:\\Users\\Administrator\\Desktop\\Draw\\reconstruction_result.npy\",allow_pickle=True).item() \n",
    "selected_model_names = ['PSC-FENet-ST', 'LSTM', 'RF', 'Temporal']\n",
    "num_models = len(selected_model_names)\n",
    "create_subplots(num_models, Original_mean,r_values, selected_model_names, metric, plt_name = 'a')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (数据处理) 二维指标计算 \n",
    "在此之前先获取0.1、0.37数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "import pandas as pd\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "def generate_clipping_mask(lon_grid, lat_grid, shp_path):\n",
    "    \"\"\"\n",
    "    使用.shp文件生成数据掩码：在边界外的数据掩码为0，在边界内的数据掩码为1。\n",
    "    \"\"\"\n",
    "    gdf = gpd.read_file(shp_path)\n",
    "    poly = gdf.unary_union\n",
    "    mask = np.zeros_like(lon_grid, dtype=bool)\n",
    "\n",
    "    for i in range(lon_grid.shape[0]):\n",
    "        for j in range(lon_grid.shape[1]):\n",
    "            point = Point(lon_grid[i, j], lat_grid[i, j])\n",
    "            if point.within(poly):\n",
    "                mask[i, j] = True\n",
    "\n",
    "    return mask\n",
    "\n",
    "def cal_ind_data(Original_data_low_res, low_res_model_data):\n",
    "    num_models = len(low_res_model_data)\n",
    "    # 初始化r_values数组来存储每个模型数据和原始数据之间的相关系数\n",
    "    values = {name: np.full((Original_data_low_res.shape[1], Original_data_low_res.shape[2], num_models), np.nan) \n",
    "            for name in ['r_values', 'bias_values', 'RMSE_values', 'ubRMSE_values']}\n",
    "    \n",
    "    for k, model_data in enumerate(low_res_model_data):\n",
    "        for i in range(Original_data_low_res.shape[1]):  # 遍历纬度\n",
    "            for j in range(Original_data_low_res.shape[2]):  # 遍历经度\n",
    "                original_series = Original_data_low_res[:, i, j]\n",
    "                model_series = model_data[:, i, j]\n",
    "                mask = ~np.isnan(original_series) & ~np.isnan(model_series)\n",
    "                original_series = original_series[mask]\n",
    "                model_series = model_series[mask]\n",
    "                if not len(original_series) or not len(model_series):\n",
    "                    continue\n",
    "                values['r_values'][i, j, k], _ = pearsonr(original_series, model_series)\n",
    "                values['bias_values'][i, j, k] = np.mean(model_series - original_series)\n",
    "                values['RMSE_values'][i, j, k] = np.sqrt(np.mean((model_series - original_series)**2))\n",
    "                mean_model = np.mean(model_series)\n",
    "                mean_obs = np.mean(original_series)\n",
    "                values['ubRMSE_values'][i, j, k] = np.sqrt(np.mean((model_series - mean_model - (original_series - mean_obs))**2))\n",
    "\n",
    "    return values['r_values'], values['bias_values'], values['RMSE_values'], values['ubRMSE_values']\n",
    "\n",
    "shp_path = r\"D:\\\\Data_Store\\\\TPBoundary_new(2021)\\\\TPBoundary_new(2021).shp\"\n",
    "lakes_path = r\"D:\\Data_Store\\TPBoundary_new(2021)\\Lake_TP_2010.shp\"\n",
    "\n",
    "selected_model_names = ['PSC-FENet-ST', 'LSTM', 'RF', 'Temporal']\n",
    "#选择模型数据\n",
    "dates = pd.date_range(start='2016-01-01 03:00', periods=8768, freq='3H')\n",
    "filter_mask = ((dates.hour == 6) | (dates.hour == 18))\n",
    "filtered_indices = np.where(filter_mask)[0]\n",
    "Original_data_full = np.load(Original_path)\n",
    "Original_data = Original_data_full[filtered_indices, :, :]\n",
    "\n",
    "# 0.37 \n",
    "#selected_paths = [reconstruction_paths[name] for name in selected_model_names if name in reconstruction_paths]\n",
    "#model_data = [np.load(path)[filtered_indices, :, :] for path in selected_paths]\n",
    "#r_values, bias_values, RMSE_values, ubRMSE_values= cal_ind_data(Original_data, model_data)\n",
    "\n",
    "#0.1 需先计算low_res_model_data\n",
    "r_values, bias_values, RMSE_values, ubRMSE_values = cal_ind_data(Original_data, low_res_model_data)\n",
    "\n",
    "\n",
    "# 裁剪\n",
    "settings = {\n",
    "    0.37: (25.293533, 40.604774, 66.84705, 105.31184, 42, 104)\n",
    "}\n",
    "\n",
    "masks = {}\n",
    "mode = 0.37\n",
    "for mode, (lat_min, lat_max, lon_min, lon_max, lat_points, lon_points) in settings.items():\n",
    "    lon_step = (lon_max - lon_min) / (lon_points - 1)\n",
    "    lat_step = (lat_max - lat_min) / (lat_points - 1)\n",
    "    new_lon_min = lon_min - lon_step / 2\n",
    "    new_lon_max = lon_max + lon_step / 2\n",
    "    new_lat_min = lat_min - lat_step / 2\n",
    "    new_lat_max = lat_max + lat_step / 2\n",
    "    lon_range, lat_range = np.linspace(new_lon_min, new_lon_max, lon_points), np.linspace(new_lat_min, new_lat_max, lat_points)\n",
    "    lon_grid, lat_grid = np.meshgrid(lon_range, lat_range)\n",
    "    masks[mode] = generate_clipping_mask(lon_grid, lat_grid, shp_path)\n",
    "\n",
    "mask = masks[mode]\n",
    "mask_expanded = mask[:, :, np.newaxis]\n",
    "\n",
    "r_values = np.where(mask_expanded, r_values, np.nan)\n",
    "bias_values = np.where(mask_expanded, bias_values, np.nan)\n",
    "RMSE_values = np.where(mask_expanded, RMSE_values, np.nan)\n",
    "ubRMSE_values = np.where(mask_expanded, ubRMSE_values, np.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (绘图) 二维指标图  模型(4)*指标(3) \n",
    "需要修改，不要比例尺，要添加一个比例柱状图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.collections import PolyCollection\n",
    "import matplotlib.colors as mcolors\n",
    "from scipy.stats import gaussian_kde\n",
    "from matplotlib.colors import to_rgba\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "from matplotlib.font_manager import FontProperties\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import matplotlib.cm as mcm\n",
    "import matplotlib.patches as mpatches\n",
    "import geopandas as gpd\n",
    "import matplotlib.font_manager as font_manager\n",
    "\n",
    "\n",
    "sns.set(style=\"whitegrid\", font='Times New Roman')\n",
    "matplotlib.rcParams['font.family'] = 'Times New Roman'\n",
    "legend_font = FontProperties(family='Times New Roman', size=14)\n",
    "font_prop = FontProperties(family='Times New Roman', size=16)\n",
    "def get_cbar_colors(mappable, bins):\n",
    "    mid_points = [(bins[i] + bins[i+1])/2 for i in range(len(bins)-1)]\n",
    "    colors = [mappable.cmap(mappable.norm(mid_point)) for mid_point in mid_points]\n",
    "    return colors\n",
    "\n",
    "def add_north(ax, labelsize=12, loc_x=0.97, loc_y=0.96, width=0.03, height=0.15, pad=0.15):\n",
    "    \"\"\"\n",
    "    画一个比例尺带'N'文字注释\n",
    "    主要参数如下\n",
    "    :param ax: 要画的坐标区域 Axes实例 plt.gca()获取即可\n",
    "    :param labelsize: 显示'N'文字的大小\n",
    "    :param loc_x: 以文字下部为中心的占整个ax横向比例\n",
    "    :param loc_y: 以文字下部为中心的占整个ax纵向比例\n",
    "    :param width: 指南针占ax比例宽度\n",
    "    :param height: 指南针占ax比例高度\n",
    "    :param pad: 文字符号占ax比例间隙\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    minx, maxx = ax.get_xlim()\n",
    "    miny, maxy = ax.get_ylim()\n",
    "    ylen = maxy - miny\n",
    "    xlen = maxx - minx\n",
    "    left = [minx + xlen*(loc_x - width*.5), miny + ylen*(loc_y - pad)]\n",
    "    right = [minx + xlen*(loc_x + width*.5), miny + ylen*(loc_y - pad)]\n",
    "    top = [minx + xlen*loc_x, miny + ylen*(loc_y - pad + height)]\n",
    "    center = [minx + xlen*loc_x, left[1] + (top[1] - left[1])*.4]\n",
    "    triangle = mpatches.Polygon([left, top, right, center], color='k')\n",
    "    ax.text(s='N',\n",
    "            x=minx + xlen*loc_x,\n",
    "            y=miny + ylen*(loc_y - pad + height),\n",
    "            fontsize=labelsize,\n",
    "            horizontalalignment='center',\n",
    "            verticalalignment='bottom')\n",
    "    ax.add_patch(triangle)\n",
    "\n",
    "def add_scalebar(ax,lon0,lat0,length,size=0.45,lw=1.5):\n",
    "    '''\n",
    "    ax: 坐标轴\n",
    "    lon0: 经度\n",
    "    lat0: 纬度\n",
    "    length: 长度\n",
    "    size: 控制粗细和距离的\n",
    "    '''\n",
    "    # style 3\n",
    "    lw = lw\n",
    "    ax.hlines(y=lat0,  xmin = lon0, xmax = lon0+length/111, colors=\"black\", ls=\"-\", lw=lw, label='%d km' % (length))\n",
    "    ax.vlines(x = lon0, ymin = lat0-size, ymax = lat0+size, colors=\"black\", ls=\"-\", lw=lw)\n",
    "    ax.vlines(x = lon0+length/2/111, ymin = lat0-size, ymax = lat0+size, colors=\"black\", ls=\"-\", lw=lw)\n",
    "    ax.vlines(x = lon0+length/111, ymin = lat0-size, ymax = lat0+size, colors=\"black\", ls=\"-\", lw=lw)\n",
    "    ax.text(lon0+length/111,lat0+size+0.12,'%d' % (length),horizontalalignment = 'center',size=12)\n",
    "    ax.text(lon0+length/2/111,lat0+size+0.12,'%d' % (length/2),horizontalalignment = 'center',size=12)\n",
    "    ax.text(lon0,lat0+size+0.12,'0',horizontalalignment = 'center',size=12)\n",
    "    ax.text(lon0+length/111/2*2.5,lat0+size+0.12,'km',horizontalalignment = 'center',size=12)\n",
    "\n",
    "def custom_formatter(x, pos):\n",
    "    if x == 0:\n",
    "        return \"0\"\n",
    "    else:\n",
    "        str_x = \"{:.15g}\".format(x)\n",
    "        if 'e' in str_x:\n",
    "            base, exponent = str_x.split('e')\n",
    "            base = base.rstrip('0').rstrip('.')\n",
    "            exponent = exponent.replace('+', '').lstrip('0')\n",
    "            if exponent:\n",
    "                return f\"{base}e{exponent}\"\n",
    "            else:\n",
    "                return base\n",
    "        else:\n",
    "            return str_x\n",
    "        \n",
    "def add_degree_E(x, pos):\n",
    "    return f\"{int(x)}°E\"\n",
    "\n",
    "def add_degree_N(y, pos):\n",
    "    return f\"{int(custom_formatter(y, pos))}°N\"\n",
    "\n",
    "def create_2d_subplots(shp_path, lakes_path, r_values, bias_values, RMSE_values, ubRMSE_values, selected_model_names, plt_name = None):\n",
    "\n",
    "    # 创建图形对象\n",
    "    fig = plt.figure(figsize=(18, 24))\n",
    "\n",
    "    ratio_h = 4 # 竖直比例\n",
    "    ratio_v = 3 # 水平比例\n",
    "\n",
    "    absolute_dis_h = 0.35/ratio_h # 中央子图绝对大小（竖直）0.15\n",
    "    absolute_dis_v = 0.7/ratio_v # 中央子图绝对大小（水平）0.2\n",
    "\n",
    "    edge_h = 0.32 # 竖直边缘间隙\n",
    "    edge_v = 0.13 # 水平边缘间隙\n",
    "\n",
    "    mid_h = (1-edge_h*2-absolute_dis_h*4)/3 # 竖直间隙,可以通过竖直边缘间隙调节\n",
    "    mid_v = (1-edge_v*2-absolute_dis_v*3)/2 # 水平间隙……\n",
    "\n",
    "    # 计算第一层图起点:上层子图  左起点、下起点、宽度比例、高度比例\n",
    "    ax1 = fig.add_axes([edge_v, 1 - edge_h - absolute_dis_h, absolute_dis_v, absolute_dis_h])\n",
    "    ax2 = fig.add_axes([edge_v + absolute_dis_v + mid_v, 1 - edge_h - absolute_dis_h, absolute_dis_v, absolute_dis_h])\n",
    "    ax3 = fig.add_axes([edge_v + 2*(absolute_dis_v + mid_v), 1 - edge_h - absolute_dis_h, absolute_dis_v, absolute_dis_h])\n",
    "\n",
    "    ax4 = fig.add_axes([edge_v, 1 - edge_h - 2*absolute_dis_h - mid_h, absolute_dis_v, absolute_dis_h])\n",
    "    ax5 = fig.add_axes([edge_v + absolute_dis_v + mid_v, 1 - edge_h - 2*absolute_dis_h - mid_h, absolute_dis_v, absolute_dis_h])\n",
    "    ax6 = fig.add_axes([edge_v + 2*(absolute_dis_v + mid_v), 1 - edge_h - 2*absolute_dis_h - mid_h, absolute_dis_v, absolute_dis_h])\n",
    "\n",
    "    ax7 = fig.add_axes([edge_v, 1 - edge_h - 3*absolute_dis_h - 2*mid_h, absolute_dis_v, absolute_dis_h])\n",
    "    ax8 = fig.add_axes([edge_v + absolute_dis_v + mid_v, 1 - edge_h - 3*absolute_dis_h - 2*mid_h, absolute_dis_v, absolute_dis_h])\n",
    "    ax9 = fig.add_axes([edge_v + 2*(absolute_dis_v + mid_v), 1 - edge_h - 3*absolute_dis_h - 2*mid_h, absolute_dis_v, absolute_dis_h])\n",
    "\n",
    "    ax10 = fig.add_axes([edge_v, 1 - edge_h - 4*absolute_dis_h - 3*mid_h, absolute_dis_v, absolute_dis_h])\n",
    "    ax11 = fig.add_axes([edge_v + absolute_dis_v + mid_v, 1 - edge_h - 4*absolute_dis_h - 3*mid_h, absolute_dis_v, absolute_dis_h])\n",
    "    ax12 = fig.add_axes([edge_v + 2*(absolute_dis_v + mid_v) , 1 - edge_h - 4*absolute_dis_h - 3*mid_h, absolute_dis_v, absolute_dis_h])\n",
    "\n",
    "    axes = [ax1, ax2, ax3, ax4, ax5, ax6, ax7, ax8, ax9, ax10, ax11, ax12]\n",
    "    '''\n",
    "    # cmap\n",
    "    r_colors = ['#B7A0C4', '#36134A']\n",
    "    bias_colors = ['#9bafd9', '#a6d96a', '#103783']\n",
    "    ubrmse_colors = ['#75CCD8', '#abdda4', '#00505F'] \n",
    "    # 创建对应的色条\n",
    "    r_vmin,r_vmax = 0, 1\n",
    "    bias_vmin, bias_vmax = np.nanpercentile(bias_values.flatten(), 1), np.nanpercentile(bias_values.flatten(), 99)\n",
    "    ubRMSE_vmin, ubRMSE_vmax = np.nanpercentile(ubRMSE_values.flatten(), 1), np.nanpercentile(ubRMSE_values.flatten(), 99)\n",
    "\n",
    "    r_cmap = mcolors.LinearSegmentedColormap.from_list(\"r_cmap\", r_colors)\n",
    "    bias_cmap = mcolors.LinearSegmentedColormap.from_list(\"bias_cmap\", bias_colors)\n",
    "    ubrmse_cmap = mcolors.LinearSegmentedColormap.from_list(\"ubrmse_cmap\", ubrmse_colors)\n",
    "    '''\n",
    "    # 生成子子图的label\n",
    "    r_bins=[0, 0.2, 0.4, 0.6, 0.8, 1]  \n",
    "    bias_bins=[-0.08, -0.04, 0, 0.04, 0.08, 0.12]  \n",
    "    ubRMSE_bins=[0, 0.02, 0.04, 0.06, 0.08, 0.1]\n",
    "    # cmap\n",
    "    r_cmap = mcm.terrain\n",
    "    bias_cmap = mcm.BrBG\n",
    "    ubrmse_cmap = mcm.terrain_r\n",
    "    # vmin,vmax\n",
    "    r_vmin, r_vmax = r_bins[0], r_bins[-1]\n",
    "    bias_vmin, bias_vmax = bias_bins[0], bias_bins[-1]\n",
    "    ubRMSE_vmin, ubRMSE_vmax = ubRMSE_bins[0], ubRMSE_bins[-1]\n",
    "    r_norm = mcolors.Normalize(vmin=r_vmin, vmax=r_vmax)\n",
    "    midpoint = 0\n",
    "    bias_norm = mcolors.TwoSlopeNorm(vmin=bias_vmin, vcenter=midpoint, vmax=bias_vmax)\n",
    "    ubRMSE_norm = mcolors.Normalize(vmin=ubRMSE_vmin, vmax=ubRMSE_vmax)\n",
    "    r = mcm.ScalarMappable(norm=r_norm, cmap=r_cmap)\n",
    "    bias = mcm.ScalarMappable(norm=bias_norm, cmap=bias_cmap)\n",
    "    ubRMSE = mcm.ScalarMappable(norm=ubRMSE_norm, cmap=ubrmse_cmap)\n",
    "    # 生成子子图颜色\n",
    "    r_colors = get_cbar_colors(r, r_bins)\n",
    "    bias_colors = get_cbar_colors(bias, bias_bins)\n",
    "    ubRMSE_colors = get_cbar_colors(ubRMSE, ubRMSE_bins)\n",
    "    # 刻度查看和设置\n",
    "    '''\n",
    "    bias_vmin, bias_vmax = np.nanmin(bias_values.flatten()), np.nanmax(bias_values.flatten()) #-0.154,0.234\n",
    "    ubRMSE_vmin, ubRMSE_vmax = np.nanmin(ubRMSE_values.flatten()), np.nanmax(ubRMSE_values.flatten()) #0.0027 0.137\n",
    "    '''\n",
    "\n",
    "    for i in range(4):\n",
    "        bottom_spines = True  if i == 3 else False\n",
    "        draw_subplot(axes[3*i], r_values[..., i], selected_model_names[i], selected_model_names, shp_path, lakes_path, vmin=r_vmin, vmax=r_vmax, cmap=r_cmap, left_spines=(True if 3*i in [0,3,6,9] else False), bottom_spines=bottom_spines, plt_name = 'a', bins=r_bins, colors=r_colors, invert_order=True),\n",
    "        draw_subplot(axes[3*i+1], bias_values[..., i], selected_model_names[i], selected_model_names, shp_path, lakes_path, vmin=bias_vmin, vmax=bias_vmax, cmap=bias_cmap, left_spines=(True if 3*i+1 in [0,3,6,9] else False), bottom_spines=bottom_spines, plt_name = 'a', bins=bias_bins, colors=bias_colors, invert_order=False)\n",
    "        draw_subplot(axes[3*i+2], ubRMSE_values[..., i], selected_model_names[i], selected_model_names, shp_path, lakes_path, vmin=ubRMSE_vmin, vmax=ubRMSE_vmax, cmap=ubrmse_cmap, left_spines=(True if 3*i+2 in [0,3,6,9] else False), bottom_spines=bottom_spines, plt_name = 'a', bins=ubRMSE_bins, colors=ubRMSE_colors, invert_order=False)\n",
    "\n",
    "    cbar_lengths = [0.21, 0.175, 0.15]\n",
    "    for i, (mappable, label, bins) in enumerate(zip([r, bias, ubRMSE], ['Person R', 'bias', 'ubRMSE'], [r_bins, bias_bins, ubRMSE_bins])):\n",
    "        cbar_ax = fig.add_axes([0.2535*i+0.1315, 1 - edge_h - 4*absolute_dis_h - 3*mid_h-0.023, cbar_lengths[i], 0.008])\n",
    "        extend = 'neither' if i == 0 else ('both' if i == 1 else 'max')\n",
    "        cbar = plt.colorbar(mappable, cax=cbar_ax, orientation='horizontal', extend=extend)\n",
    "        # 设置刻度为bins数组的值\n",
    "        cbar.set_ticks(bins)\n",
    "        cbar.ax.xaxis.set_tick_params(labelsize=12)\n",
    "        cbar.ax.xaxis.set_major_locator(MaxNLocator(nbins=len(bins)))\n",
    "        cbar.ax.tick_params(axis='x', which='both', colors='black', length=2, width=1)\n",
    "        cbar.ax.xaxis.set_major_formatter(FuncFormatter(custom_formatter))\n",
    "        cbar.outline.set_linewidth(0.8)\n",
    "        for label in cbar.ax.get_xticklabels():\n",
    "            label.set_fontproperties(font_manager.FontProperties(weight='normal'))  # 设置为粗体\n",
    "            label.set_fontsize(12)\n",
    "    fig.text(0.35, 0.3045, 'R', fontsize=14, fontname='Times New Roman', verticalalignment='top')\n",
    "    fig.text(0.565, 0.3045, 'bias (m³/m³)', fontsize=14, fontname='Times New Roman', verticalalignment='top')\n",
    "    fig.text(0.795, 0.3045, 'ubRMSE (m³/m³)', fontsize=14, fontname='Times New Roman', verticalalignment='top')\n",
    "    \n",
    "    fig.savefig(r\"C:\\Users\\Administrator\\Desktop\\Draw\\sat_original_2d.jpeg\", dpi=1000, format='jpeg', bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "def draw_sub_subplot(ax, value, bins, colors, invert_order=True):\n",
    "    flat_value = value.flatten()\n",
    "    flat_value = flat_value[~np.isnan(flat_value)]\n",
    "    counts, edges = np.histogram(flat_value, bins=bins)\n",
    "    centers = (edges[:-1] + edges[1:]) / 2\n",
    "    widths = np.diff(edges)\n",
    "\n",
    "    for i, (count, center) in enumerate(zip(counts, centers)):\n",
    "        ax.bar(center, count, width=widths[i], color=colors[i % len(colors)], edgecolor='black', align='center')\n",
    "        percentage = f'{count / flat_value.size * 100:.1f}'\n",
    "        ax.text(center, count, percentage, ha='center', va='bottom', fontsize=8)\n",
    "   \n",
    "    if invert_order:\n",
    "        ax.invert_xaxis()\n",
    "\n",
    "    # 轴线控制\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['left'].set_visible(False)\n",
    "    ax.yaxis.set_visible(False)\n",
    "    ax.grid(False)\n",
    "\n",
    "    #刻度控制\n",
    "    ax.tick_params(axis='x', which='both', length=2, width=1, labelsize=9, pad=2, direction='out', bottom=True, top=False, labeltop=False, labelbottom=True)\n",
    "    ax.set_xticks(edges)\n",
    "    ax.set_xticklabels([\"{:.2f}\".format(edge) for edge in edges], fontsize=8)\n",
    "    ax.xaxis.set_major_formatter(FuncFormatter(custom_formatter))\n",
    "    ax.spines['bottom'].set_bounds(edges[0], edges[-1])\n",
    "    ax.spines['bottom'].set_color('black')\n",
    "    ax.spines['bottom'].set_linewidth(1)\n",
    "    ax.axis('on')\n",
    "    ax.set_facecolor('none')\n",
    "\n",
    "\n",
    "def draw_subplot(ax, value, model_name, model_names, shp_path, lakes_path, vmin=None, vmax=None, cmap=None, left_spines=None, bottom_spines=None, plt_name='a', bins=[0, 0.2, 0.4, 0.6, 0.8, 1], colors=None, invert_order=True):\n",
    "    ##  sub_draw\n",
    "    sub_ax = ax.inset_axes([0.02, 0.1, 0.3, 0.3])\n",
    "    draw_sub_subplot(sub_ax, value, bins=bins, colors=colors,invert_order=invert_order)\n",
    "\n",
    "    ## draw\n",
    "    gdf = gpd.read_file(shp_path)\n",
    "    lakes = gpd.read_file(lakes_path)\n",
    "    lakes = lakes.to_crs(epsg=4326)\n",
    "    lakes.plot(ax=ax, edgecolor='white', facecolor='white', linewidth=0)\n",
    "    gdf.plot(ax=ax, edgecolor='black', facecolor='none', linewidth=1.5)\n",
    "\n",
    "    lat_min, lat_max, lon_min, lon_max, lat_points, lon_points= 25.293533, 40.604774, 66.84705, 105.31184, 42, 104\n",
    "    lon_step = (lon_max - lon_min) / (lon_points - 1)\n",
    "    lat_step = (lat_max - lat_min) / (lat_points - 1)\n",
    "    new_lon_min = lon_min - lon_step / 2\n",
    "    new_lon_max = lon_max + lon_step / 2\n",
    "    new_lat_min = lat_min - lat_step / 2\n",
    "    new_lat_max = lat_max + lat_step / 2\n",
    "\n",
    "    image = ax.imshow(value, cmap=cmap, origin='lower', vmin=vmin, vmax=vmax, extent=[new_lon_min, new_lon_max, new_lat_min, new_lat_max])\n",
    "\n",
    "\n",
    "    # 设置X轴和Y轴的标签\n",
    "    ax.set_xlabel('Longitude(°)', fontsize=16, fontweight='normal', labelpad=0)\n",
    "    ax.set_ylabel('Latitude(°)', fontsize=16, fontweight='normal')\n",
    "    ax.tick_params(axis='x', which='both', colors='black', length=3, width=1.5, labelsize=14, direction='out', pad=2, bottom=True, top=True, labelbottom=True,labeltop=False)\n",
    "    ax.tick_params(axis='y', which='both', colors='black', length=3, width=1.5, labelsize=14, direction='out', pad=6, left=True, right=True, labelleft=True,labelright=False)\n",
    "    plt.setp(ax.get_xticklabels(), rotation=0,rotation_mode=\"anchor\")\n",
    "    plt.setp(ax.get_yticklabels(), rotation=90,ha=\"center\", rotation_mode=\"anchor\")\n",
    "\n",
    "    # 刻度标签轴绘制\n",
    "    labelsize = 12\n",
    "    pad_x = 4\n",
    "    pad_y = 4\n",
    "    ax.yaxis.label.set_visible(left_spines)\n",
    "    ax.xaxis.label.set_visible(bottom_spines)\n",
    "    ax.xaxis.set_major_formatter(FuncFormatter(add_degree_E))\n",
    "\n",
    "    # 刻度调节\n",
    "    plt.setp(ax.get_yticklabels(), rotation=90,ha=\"center\", rotation_mode=\"anchor\")\n",
    "    # 刻度值格式\n",
    "    ax.yaxis.set_major_formatter(FuncFormatter(add_degree_N))\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['bottom'].set_color('black')\n",
    "    ax.spines['bottom'].set_linewidth(1.5)\n",
    "    ax.spines['left'].set_color('black')\n",
    "    ax.spines['left'].set_linewidth(1.5)\n",
    "    ax.xaxis.set_major_locator(MaxNLocator(nbins=6))\n",
    "    ax.yaxis.set_major_locator(MaxNLocator(nbins=4))\n",
    "    ax.tick_params(axis='y', which='both', colors='black', length=5, width=1.5, labelsize=labelsize, direction='out', pad=pad_y, right=False, labelright=False)\n",
    "    ax.tick_params(axis='x', which='both', colors='black', length=5, width=1.5, labelsize=labelsize, direction='out', pad=pad_x, top=False, labeltop=False)\n",
    "    add_north(ax)\n",
    "    ax.grid(False)\n",
    "    # 文本\n",
    "    prefix = \"(\" + chr(ord('a') + model_names.index(model_name)) + \")\" \n",
    "    model_str = f'{prefix} {model_name}'\n",
    "    if left_spines: ax.text(0.24, 0.98, model_str, transform=ax.transAxes, fontsize=14, fontname='Times New Roman', verticalalignment='top')\n",
    "\n",
    "create_2d_subplots(shp_path, lakes_path, r_values, bias_values, RMSE_values, ubRMSE_values, selected_model_names, plt_name = 'a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.collections import PolyCollection\n",
    "import matplotlib.colors as mcolors\n",
    "from scipy.stats import gaussian_kde\n",
    "from matplotlib.colors import to_rgba\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "from matplotlib.font_manager import FontProperties\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import matplotlib.cm as mcm\n",
    "import matplotlib.patches as mpatches\n",
    "import geopandas as gpd\n",
    "import matplotlib.font_manager as font_manager\n",
    "\n",
    "\n",
    "sns.set(style=\"whitegrid\", font='Times New Roman')\n",
    "matplotlib.rcParams['font.family'] = 'Times New Roman'\n",
    "legend_font = FontProperties(family='Times New Roman', size=14)\n",
    "font_prop = FontProperties(family='Times New Roman', size=16)\n",
    "def get_cbar_colors(mappable, bins):\n",
    "    mid_points = [(bins[i] + bins[i+1])/2 for i in range(len(bins)-1)]\n",
    "    colors = [mappable.cmap(mappable.norm(mid_point)) for mid_point in mid_points]\n",
    "    return colors\n",
    "\n",
    "def add_north(ax, labelsize=12, loc_x=0.97, loc_y=0.96, width=0.03, height=0.15, pad=0.15):\n",
    "    \"\"\"\n",
    "    画一个比例尺带'N'文字注释\n",
    "    主要参数如下\n",
    "    :param ax: 要画的坐标区域 Axes实例 plt.gca()获取即可\n",
    "    :param labelsize: 显示'N'文字的大小\n",
    "    :param loc_x: 以文字下部为中心的占整个ax横向比例\n",
    "    :param loc_y: 以文字下部为中心的占整个ax纵向比例\n",
    "    :param width: 指南针占ax比例宽度\n",
    "    :param height: 指南针占ax比例高度\n",
    "    :param pad: 文字符号占ax比例间隙\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    minx, maxx = ax.get_xlim()\n",
    "    miny, maxy = ax.get_ylim()\n",
    "    ylen = maxy - miny\n",
    "    xlen = maxx - minx\n",
    "    left = [minx + xlen*(loc_x - width*.5), miny + ylen*(loc_y - pad)]\n",
    "    right = [minx + xlen*(loc_x + width*.5), miny + ylen*(loc_y - pad)]\n",
    "    top = [minx + xlen*loc_x, miny + ylen*(loc_y - pad + height)]\n",
    "    center = [minx + xlen*loc_x, left[1] + (top[1] - left[1])*.4]\n",
    "    triangle = mpatches.Polygon([left, top, right, center], color='k')\n",
    "    ax.text(s='N',\n",
    "            x=minx + xlen*loc_x,\n",
    "            y=miny + ylen*(loc_y - pad + height),\n",
    "            fontsize=labelsize,\n",
    "            horizontalalignment='center',\n",
    "            verticalalignment='bottom')\n",
    "    ax.add_patch(triangle)\n",
    "\n",
    "def add_scalebar(ax,lon0,lat0,length,size=0.45,lw=1.5):\n",
    "    '''\n",
    "    ax: 坐标轴\n",
    "    lon0: 经度\n",
    "    lat0: 纬度\n",
    "    length: 长度\n",
    "    size: 控制粗细和距离的\n",
    "    '''\n",
    "    # style 3\n",
    "    lw = lw\n",
    "    ax.hlines(y=lat0,  xmin = lon0, xmax = lon0+length/111, colors=\"black\", ls=\"-\", lw=lw, label='%d km' % (length))\n",
    "    ax.vlines(x = lon0, ymin = lat0-size, ymax = lat0+size, colors=\"black\", ls=\"-\", lw=lw)\n",
    "    ax.vlines(x = lon0+length/2/111, ymin = lat0-size, ymax = lat0+size, colors=\"black\", ls=\"-\", lw=lw)\n",
    "    ax.vlines(x = lon0+length/111, ymin = lat0-size, ymax = lat0+size, colors=\"black\", ls=\"-\", lw=lw)\n",
    "    ax.text(lon0+length/111,lat0+size+0.12,'%d' % (length),horizontalalignment = 'center',size=12)\n",
    "    ax.text(lon0+length/2/111,lat0+size+0.12,'%d' % (length/2),horizontalalignment = 'center',size=12)\n",
    "    ax.text(lon0,lat0+size+0.12,'0',horizontalalignment = 'center',size=12)\n",
    "    ax.text(lon0+length/111/2*2.5,lat0+size+0.12,'km',horizontalalignment = 'center',size=12)\n",
    "\n",
    "def custom_formatter(x, pos):\n",
    "    if x == 0:\n",
    "        return \"0\"\n",
    "    else:\n",
    "        # 保留足够的小数位数以确保精度，但不超过e记数法所需的位数\n",
    "        # 使用'.15g'确保数值在转换为科学计数法时保持精度，但不会因太多不必要的零而冗长\n",
    "        str_x = \"{:.15g}\".format(x)\n",
    "        # 分割指数和底数，如果是科学计数法的形式\n",
    "        if 'e' in str_x:\n",
    "            base, exponent = str_x.split('e')\n",
    "            # 清除底数中尾随的零和小数点\n",
    "            base = base.rstrip('0').rstrip('.')\n",
    "            # 格式化指数部分，去除前导+和前导0\n",
    "            exponent = exponent.replace('+', '').lstrip('0')\n",
    "            if exponent:  # 如果指数部分不为空\n",
    "                return f\"{base}e{exponent}\"\n",
    "            else:  # 如果指数为0\n",
    "                return base\n",
    "        else:\n",
    "            return str_x\n",
    "        \n",
    "def add_degree_E(x, pos):\n",
    "    return f\"{int(x)}°E\"\n",
    "\n",
    "def add_degree_N(y, pos):\n",
    "    return f\"{int(custom_formatter(y, pos))}°N\"\n",
    "\n",
    "def create_2d_subplots(shp_path, lakes_path, r_values, bias_values, RMSE_values, ubRMSE_values, selected_model_names, plt_name = None):\n",
    "\n",
    "    # 创建图形对象\n",
    "    fig = plt.figure(figsize=(18, 24))\n",
    "\n",
    "    ratio_h = 4 # 竖直比例\n",
    "    ratio_v = 3 # 水平比例\n",
    "\n",
    "    absolute_dis_h = 0.35/ratio_h # 中央子图绝对大小（竖直）0.15\n",
    "    absolute_dis_v = 0.7/ratio_v # 中央子图绝对大小（水平）0.2\n",
    "\n",
    "    edge_h = 0.32 # 竖直边缘间隙\n",
    "    edge_v = 0.13 # 水平边缘间隙\n",
    "\n",
    "    mid_h = (1-edge_h*2-absolute_dis_h*4)/3 # 竖直间隙,可以通过竖直边缘间隙调节\n",
    "    mid_v = (1-edge_v*2-absolute_dis_v*3)/2 # 水平间隙……\n",
    "\n",
    "    # 计算第一层图起点:上层子图  左起点、下起点、宽度比例、高度比例\n",
    "    ax1 = fig.add_axes([edge_v, 1 - edge_h - absolute_dis_h, absolute_dis_v, absolute_dis_h])\n",
    "    ax2 = fig.add_axes([edge_v + absolute_dis_v + mid_v, 1 - edge_h - absolute_dis_h, absolute_dis_v, absolute_dis_h])\n",
    "    ax3 = fig.add_axes([edge_v + 2*(absolute_dis_v + mid_v), 1 - edge_h - absolute_dis_h, absolute_dis_v, absolute_dis_h])\n",
    "\n",
    "    ax4 = fig.add_axes([edge_v, 1 - edge_h - 2*absolute_dis_h - mid_h, absolute_dis_v, absolute_dis_h])\n",
    "    ax5 = fig.add_axes([edge_v + absolute_dis_v + mid_v, 1 - edge_h - 2*absolute_dis_h - mid_h, absolute_dis_v, absolute_dis_h])\n",
    "    ax6 = fig.add_axes([edge_v + 2*(absolute_dis_v + mid_v), 1 - edge_h - 2*absolute_dis_h - mid_h, absolute_dis_v, absolute_dis_h])\n",
    "\n",
    "    ax7 = fig.add_axes([edge_v, 1 - edge_h - 3*absolute_dis_h - 2*mid_h, absolute_dis_v, absolute_dis_h])\n",
    "    ax8 = fig.add_axes([edge_v + absolute_dis_v + mid_v, 1 - edge_h - 3*absolute_dis_h - 2*mid_h, absolute_dis_v, absolute_dis_h])\n",
    "    ax9 = fig.add_axes([edge_v + 2*(absolute_dis_v + mid_v), 1 - edge_h - 3*absolute_dis_h - 2*mid_h, absolute_dis_v, absolute_dis_h])\n",
    "\n",
    "    ax10 = fig.add_axes([edge_v, 1 - edge_h - 4*absolute_dis_h - 3*mid_h, absolute_dis_v, absolute_dis_h])\n",
    "    ax11 = fig.add_axes([edge_v + absolute_dis_v + mid_v, 1 - edge_h - 4*absolute_dis_h - 3*mid_h, absolute_dis_v, absolute_dis_h])\n",
    "    ax12 = fig.add_axes([edge_v + 2*(absolute_dis_v + mid_v) , 1 - edge_h - 4*absolute_dis_h - 3*mid_h, absolute_dis_v, absolute_dis_h])\n",
    "\n",
    "    axes = [ax1, ax2, ax3, ax4, ax5, ax6, ax7, ax8, ax9, ax10, ax11, ax12]\n",
    "    '''\n",
    "    # cmap\n",
    "    r_colors = ['#B7A0C4', '#36134A']\n",
    "    bias_colors = ['#9bafd9', '#a6d96a', '#103783']\n",
    "    ubrmse_colors = ['#75CCD8', '#abdda4', '#00505F'] \n",
    "    # 创建对应的色条\n",
    "    r_vmin,r_vmax = 0, 1\n",
    "    bias_vmin, bias_vmax = np.nanpercentile(bias_values.flatten(), 1), np.nanpercentile(bias_values.flatten(), 99)\n",
    "    ubRMSE_vmin, ubRMSE_vmax = np.nanpercentile(ubRMSE_values.flatten(), 1), np.nanpercentile(ubRMSE_values.flatten(), 99)\n",
    "\n",
    "    r_cmap = mcolors.LinearSegmentedColormap.from_list(\"r_cmap\", r_colors)\n",
    "    bias_cmap = mcolors.LinearSegmentedColormap.from_list(\"bias_cmap\", bias_colors)\n",
    "    ubrmse_cmap = mcolors.LinearSegmentedColormap.from_list(\"ubrmse_cmap\", ubrmse_colors)\n",
    "    '''\n",
    "    # 生成子子图的label\n",
    "    r_bins=[0, 0.2, 0.4, 0.6, 0.8, 1]  \n",
    "    bias_bins=[-0.08, -0.04, 0, 0.04, 0.08, 0.12]  \n",
    "    ubRMSE_bins=[0, 0.02, 0.04, 0.06, 0.08, 0.1]\n",
    "    sub_param_r = [0.06, 0.035, 0.20, 0.4]\n",
    "    sub_param_bias = [0.085, 0.035, 0.20, 0.4]\n",
    "    sub_param_ubRMSE = [0.085, 0.035, 0.20, 0.4]\n",
    "    # cmap\n",
    "    r_cmap = mcm.terrain\n",
    "    bias_cmap = mcm.BrBG\n",
    "    ubrmse_cmap = mcm.terrain_r\n",
    "    # vmin,vmax\n",
    "    r_vmin, r_vmax = r_bins[0], r_bins[-1]\n",
    "    bias_vmin, bias_vmax = bias_bins[0], bias_bins[-1]\n",
    "    ubRMSE_vmin, ubRMSE_vmax = ubRMSE_bins[0], ubRMSE_bins[-1]\n",
    "    r_norm = mcolors.Normalize(vmin=r_vmin, vmax=r_vmax)\n",
    "    midpoint = 0\n",
    "    bias_norm = mcolors.TwoSlopeNorm(vmin=bias_vmin, vcenter=midpoint, vmax=bias_vmax)\n",
    "    ubRMSE_norm = mcolors.Normalize(vmin=ubRMSE_vmin, vmax=ubRMSE_vmax)\n",
    "    r = mcm.ScalarMappable(norm=r_norm, cmap=r_cmap)\n",
    "    bias = mcm.ScalarMappable(norm=bias_norm, cmap=bias_cmap)\n",
    "    ubRMSE = mcm.ScalarMappable(norm=ubRMSE_norm, cmap=ubrmse_cmap)\n",
    "    # 生成子子图颜色\n",
    "    r_colors = get_cbar_colors(r, r_bins)\n",
    "    bias_colors = get_cbar_colors(bias, bias_bins)\n",
    "    ubRMSE_colors = get_cbar_colors(ubRMSE, ubRMSE_bins)\n",
    "    # 刻度查看和设置\n",
    "    '''\n",
    "    bias_vmin, bias_vmax = np.nanmin(bias_values.flatten()), np.nanmax(bias_values.flatten()) #-0.154,0.234\n",
    "    ubRMSE_vmin, ubRMSE_vmax = np.nanmin(ubRMSE_values.flatten()), np.nanmax(ubRMSE_values.flatten()) #0.0027 0.137\n",
    "    '''\n",
    "\n",
    "    for i in range(4):\n",
    "        bottom_spines = True  if i == 3 else False\n",
    "        draw_subplot(axes[3*i], r_values[..., i], selected_model_names[i], selected_model_names, shp_path, lakes_path, vmin=r_vmin, vmax=r_vmax, cmap=r_cmap, left_spines=(True if 3*i in [0,3,6,9] else False), bottom_spines=bottom_spines, plt_name = 'a', bins=r_bins, colors=r_colors, invert_order=True, sub_param = sub_param_r),\n",
    "        draw_subplot(axes[3*i+1], bias_values[..., i], selected_model_names[i], selected_model_names, shp_path, lakes_path, vmin=bias_vmin, vmax=bias_vmax, cmap=bias_cmap, left_spines=(True if 3*i+1 in [0,3,6,9] else False), bottom_spines=bottom_spines, plt_name = 'a', bins=bias_bins, colors=bias_colors, invert_order=False, sub_param = sub_param_bias)\n",
    "        draw_subplot(axes[3*i+2], ubRMSE_values[..., i], selected_model_names[i], selected_model_names, shp_path, lakes_path, vmin=ubRMSE_vmin, vmax=ubRMSE_vmax, cmap=ubrmse_cmap, left_spines=(True if 3*i+2 in [0,3,6,9] else False), bottom_spines=bottom_spines, plt_name = 'a', bins=ubRMSE_bins, colors=ubRMSE_colors, invert_order=False, sub_param = sub_param_ubRMSE)\n",
    "\n",
    "    cbar_lengths = [0.21, 0.175, 0.15]\n",
    "    for i, (mappable, label, bins) in enumerate(zip([r, bias, ubRMSE], ['Person R', 'bias', 'ubRMSE'], [r_bins, bias_bins, ubRMSE_bins])):\n",
    "        cbar_ax = fig.add_axes([0.2535*i+0.1315, 1 - edge_h - 4*absolute_dis_h - 3*mid_h-0.023, cbar_lengths[i], 0.008])\n",
    "        extend = 'neither' if i == 0 else ('both' if i == 1 else 'max')\n",
    "        cbar = plt.colorbar(mappable, cax=cbar_ax, orientation='horizontal', extend=extend)\n",
    "        # 设置刻度为bins数组的值\n",
    "        cbar.set_ticks(bins)\n",
    "        cbar.ax.xaxis.set_tick_params(labelsize=12)\n",
    "        cbar.ax.xaxis.set_major_locator(MaxNLocator(nbins=len(bins)))\n",
    "        cbar.ax.tick_params(axis='x', which='both', colors='black', length=2, width=1)\n",
    "        cbar.ax.xaxis.set_major_formatter(FuncFormatter(custom_formatter))\n",
    "        for label in cbar.ax.get_xticklabels():\n",
    "            label.set_fontproperties(font_manager.FontProperties(weight='normal'))  # 设置为粗体\n",
    "            label.set_fontsize(12)\n",
    "    fig.text(0.35, 0.3045, 'R', fontsize=14, fontname='Times New Roman', verticalalignment='top')\n",
    "    fig.text(0.565, 0.3045, 'bias (m³/m³)', fontsize=14, fontname='Times New Roman', verticalalignment='top')\n",
    "    fig.text(0.795, 0.3045, 'ubRMSE (m³/m³)', fontsize=14, fontname='Times New Roman', verticalalignment='top')\n",
    "    \n",
    "    fig.savefig(r\"C:\\Users\\Administrator\\Desktop\\Draw\\sat_original_2d.jpeg\", dpi=1000, format='jpeg', bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "def draw_sub_subplot(ax, value, bins, colors, invert_order=True):\n",
    "    flat_value = value.flatten()\n",
    "    flat_value = flat_value[~np.isnan(flat_value)]\n",
    "    counts, edges = np.histogram(flat_value, bins=bins)\n",
    "    centers = (edges[:-1] + edges[1:]) / 2\n",
    "    heights = np.diff(edges)\n",
    "\n",
    "    for i, (count, center) in enumerate(zip(counts, centers)):\n",
    "        ax.barh(center, count, height=heights[i], color=colors[i % len(colors)], edgecolor='black', align='center')\n",
    "        percentage = f'{count / flat_value.size * 100:.1f}%'\n",
    "        ax.text(count+40, center, percentage, ha='left', va='center', fontsize=10)\n",
    "    \n",
    "    if invert_order:\n",
    "        ax.invert_yaxis()\n",
    "\n",
    "    # 轴线控制\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['bottom'].set_visible(False)\n",
    "    ax.xaxis.set_visible(False)\n",
    "    ax.grid(False)\n",
    "\n",
    "    # 刻度控制\n",
    "    ax.tick_params(axis='y', which='both', length=2, width=1, labelsize=10, pad=2, direction='out', left=True, right=False, labelleft=True, labelright=False)\n",
    "    ax.set_yticks(edges)\n",
    "    ax.set_yticklabels([\"{:.2f}\".format(edge) for edge in edges], fontsize=10)\n",
    "\n",
    "    ax.yaxis.set_major_formatter(FuncFormatter(custom_formatter))\n",
    "    ax.spines['left'].set_bounds(edges[0], edges[-1])\n",
    "    ax.spines['left'].set_color('black')\n",
    "    ax.spines['left'].set_linewidth(1)\n",
    "    ax.axis('on')\n",
    "    ax.set_facecolor('none')\n",
    "\n",
    "def draw_subplot(ax, value, model_name, model_names, shp_path, lakes_path, vmin=None, vmax=None, cmap=None, \n",
    "                 left_spines=None, bottom_spines=None, plt_name='a', bins=[0, 0.2, 0.4, 0.6, 0.8, 1], colors=None, invert_order=True,\n",
    "                 sub_param = None):\n",
    "\n",
    "    ##  sub_draw\n",
    "    sub_ax = ax.inset_axes(sub_param)\n",
    "    draw_sub_subplot(sub_ax, value, bins=bins, colors=colors,invert_order=invert_order)\n",
    "\n",
    "    ## draw\n",
    "    gdf = gpd.read_file(shp_path)\n",
    "    lakes = gpd.read_file(lakes_path)\n",
    "    lakes = lakes.to_crs(epsg=4326)\n",
    "    lakes.plot(ax=ax, edgecolor='white', facecolor='white', linewidth=0)\n",
    "    gdf.plot(ax=ax, edgecolor='black', facecolor='none', linewidth=1.5)\n",
    "\n",
    "    lat_min, lat_max, lon_min, lon_max, lat_points, lon_points= 25.293533, 40.604774, 66.84705, 105.31184, 42, 104\n",
    "    lon_step = (lon_max - lon_min) / (lon_points - 1)\n",
    "    lat_step = (lat_max - lat_min) / (lat_points - 1)\n",
    "    new_lon_min = lon_min - lon_step / 2\n",
    "    new_lon_max = lon_max + lon_step / 2\n",
    "    new_lat_min = lat_min - lat_step / 2\n",
    "    new_lat_max = lat_max + lat_step / 2\n",
    "\n",
    "    image = ax.imshow(value, cmap=cmap, origin='lower', vmin=vmin, vmax=vmax, extent=[new_lon_min, new_lon_max, new_lat_min, new_lat_max])\n",
    "\n",
    "    # 设置X轴和Y轴的标签\n",
    "    ax.set_xlabel('Longitude(°)', fontsize=16, fontweight='normal', labelpad=0)\n",
    "    ax.set_ylabel('Latitude(°)', fontsize=16, fontweight='normal')\n",
    "\n",
    "    # 刻度标签轴绘制\n",
    "    labelsize = 12\n",
    "    pad_x = 4\n",
    "    pad_y = 4\n",
    "    ax.yaxis.label.set_visible(left_spines)\n",
    "    ax.xaxis.label.set_visible(bottom_spines)\n",
    "    ax.xaxis.set_major_formatter(FuncFormatter(add_degree_E))\n",
    "\n",
    "    # 刻度调节\n",
    "    plt.setp(ax.get_xticklabels(), rotation=0,rotation_mode=\"anchor\")\n",
    "    plt.setp(ax.get_yticklabels(), rotation=90,ha=\"center\", rotation_mode=\"anchor\")\n",
    "    # 刻度值格式\n",
    "    ax.yaxis.set_major_formatter(FuncFormatter(add_degree_N))\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['bottom'].set_color('black')\n",
    "    ax.spines['bottom'].set_linewidth(1.5)\n",
    "    ax.spines['left'].set_color('black')\n",
    "    ax.spines['left'].set_linewidth(1.5)\n",
    "    ax.xaxis.set_major_locator(MaxNLocator(nbins=6))\n",
    "    ax.yaxis.set_major_locator(MaxNLocator(nbins=4))\n",
    "    ax.tick_params(axis='y', which='both', colors='black', length=5, width=1.5, labelsize=labelsize, direction='out', pad=pad_y, right=False, labelright=False)\n",
    "    ax.tick_params(axis='x', which='both', colors='black', length=5, width=1.5, labelsize=labelsize, direction='out', pad=pad_x, top=False, labeltop=False)\n",
    "    add_north(ax)\n",
    "    ax.grid(False)\n",
    "\n",
    "    # 文本\n",
    "    prefix = \"(\" + chr(ord('a') + model_names.index(model_name)) + \")\" \n",
    "    model_str = f'{prefix} {model_name}'\n",
    "    if left_spines: ax.text(0.02, 1.05, model_str, transform=ax.transAxes, fontsize=16, fontname='Times New Roman', verticalalignment='top')\n",
    "\n",
    "create_2d_subplots(shp_path, lakes_path, r_values, bias_values, RMSE_values, ubRMSE_values, selected_model_names, plt_name = 'a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (数据处理) 重建结果数据提取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "import xarray as xr\n",
    "import zarr\n",
    "def generate_clipping_mask(lon_grid, lat_grid, shp_path):\n",
    "    \"\"\"\n",
    "    使用.shp文件生成数据掩码：在边界外的数据掩码为0，在边界内的数据掩码为1。\n",
    "    \"\"\"\n",
    "    import geopandas as gpd\n",
    "    from shapely.geometry import Point\n",
    "    import numpy as np\n",
    "\n",
    "    gdf = gpd.read_file(shp_path)\n",
    "    poly = gdf.unary_union\n",
    "    mask = np.zeros_like(lon_grid, dtype=bool)\n",
    "\n",
    "    for i in range(lon_grid.shape[0]):\n",
    "        for j in range(lon_grid.shape[1]):\n",
    "            point = Point(lon_grid[i, j], lat_grid[i, j])\n",
    "            if point.within(poly):\n",
    "                mask[i, j] = True\n",
    "\n",
    "    return mask\n",
    "\n",
    "shp_path = r\"D:\\\\Data_Store\\\\TPBoundary_new(2021)\\\\TPBoundary_new(2021).shp\"\n",
    "lakes_path = r\"D:\\Data_Store\\TPBoundary_new(2021)\\Lake_TP_2010.shp\"\n",
    "\n",
    "reconstruction_paths = {\n",
    "    \"Original\": r\"D:\\Data_Store\\Dataset\\Reconstruction\\37SM\\soil_moisture.npy\",\n",
    "    \"LSTM\": r\"D:\\Data_Store\\Dataset\\Reconstruction\\LSTM\\LSTM1.npy\",\n",
    "    'PSC-FENet-ST': r\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\ST_PSC-FENet.npy\",\n",
    "    \"RF\": r\"D:\\Data_Store\\Dataset\\Reconstruction\\RF\\RF_reconstructed_data.npy\",\n",
    "    'Temporal': r\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\Temporal.npy\",\n",
    "    'PSC-FENet-SP': r\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\SP_PSC-FENet.npy\",\n",
    "    'UNet-ST': r\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\ST_UNet.npy\",\n",
    "    'ResNet-ST': r\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\ST_ResNet.npy\",\n",
    "    'VIT-ST': r\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\ST_ViT.npy\",\n",
    "    'UNet-SP': r\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\SP_UNet.npy\",\n",
    "    'ResNet-SP': r\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\SP_ResNet.npy\",\n",
    "    'VIT-SP': r\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\SP_ViT.npy\",\n",
    "}\n",
    "\n",
    "settings = {\n",
    "    0.37: (25.293533, 40.604774, 66.84705, 105.31184, 42, 104),\n",
    "    0.1: (25.550117, 40.450184, 66.95031, 104.95048, 150, 381)\n",
    "}\n",
    "\n",
    "'''\n",
    "selected_model_names = ['KPNet-ST', 'ResNet-ST', 'UNet-ST', 'VIT-ST',  'LSTM', 'RF', 'Temporal']\n",
    "selected_reconstruction_paths = {name: paths for name, paths in file_paths.items() if name in selected_model_names}\n",
    "'''\n",
    "dem_data = xr.open_zarr(r\"D:\\Data_Store\\Dataset\\Original_Data\\0.1_Data.zarr\").as_dem_3s.isel(time=0).compute()\n",
    "masks = {}\n",
    "for mode, (lat_min, lat_max, lon_min, lon_max, lat_points, lon_points) in settings.items():\n",
    "    lon_step = (lon_max - lon_min) / (lon_points - 1)\n",
    "    lat_step = (lat_max - lat_min) / (lat_points - 1)\n",
    "    new_lon_min = lon_min - lon_step / 2\n",
    "    new_lon_max = lon_max + lon_step / 2\n",
    "    new_lat_min = lat_min - lat_step / 2\n",
    "    new_lat_max = lat_max + lat_step / 2\n",
    "    lon_range, lat_range = np.linspace(new_lon_min, new_lon_max, lon_points), np.linspace(new_lat_min, new_lat_max, lat_points)\n",
    "    lon_grid, lat_grid = np.meshgrid(lon_range, lat_range)\n",
    "    masks[mode] = generate_clipping_mask(lon_grid, lat_grid, shp_path)\n",
    "\n",
    "reconstruction_clip_data = {}\n",
    "reconstruction_data = {}\n",
    "for i, (key, data_path) in enumerate(reconstruction_paths.items()):\n",
    "    mode = 0.37 if i == 0 else 0.1\n",
    "    mask = masks[mode]\n",
    "    data = np.load(data_path)[7817]\n",
    "    clipped_data = np.where(mask, data, np.nan)\n",
    "    reconstruction_clip_data[key] = clipped_data\n",
    "    reconstruction_data[key] = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dem_data = xr.open_zarr(r\"D:\\Data_Store\\Dataset\\Original_Data\\0.1_Data.zarr\").as_dem_3s.isel(time=0).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import zarr\n",
    "\n",
    "dem_data = xr.open_zarr(r\"D:\\Data_Store\\Dataset\\Original_Data\\0.1_Data.zarr\").as_dem_3s.isel(time=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (绘图) V1.0 重建结果图 4*3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import seaborn as sns\n",
    "from matplotlib.font_manager import FontProperties\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.cm as mcm\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "sns.set(style=\"whitegrid\", font='Times New Roman')\n",
    "matplotlib.rcParams['font.family'] = 'Times New Roman'\n",
    "legend_font = FontProperties(family='Times New Roman', size=14)\n",
    "font_prop = FontProperties(family='Times New Roman', size=16)\n",
    "\n",
    "def add_north(ax, labelsize=12, loc_x=0.97, loc_y=0.96, width=0.03, height=0.15, pad=0.15):\n",
    "    \"\"\"\n",
    "    画一个比例尺带'N'文字注释\n",
    "    主要参数如下\n",
    "    :param ax: 要画的坐标区域 Axes实例 plt.gca()获取即可\n",
    "    :param labelsize: 显示'N'文字的大小\n",
    "    :param loc_x: 以文字下部为中心的占整个ax横向比例\n",
    "    :param loc_y: 以文字下部为中心的占整个ax纵向比例\n",
    "    :param width: 指南针占ax比例宽度\n",
    "    :param height: 指南针占ax比例高度\n",
    "    :param pad: 文字符号占ax比例间隙\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    minx, maxx = ax.get_xlim()\n",
    "    miny, maxy = ax.get_ylim()\n",
    "    ylen = maxy - miny\n",
    "    xlen = maxx - minx\n",
    "    left = [minx + xlen*(loc_x - width*.5), miny + ylen*(loc_y - pad)]\n",
    "    right = [minx + xlen*(loc_x + width*.5), miny + ylen*(loc_y - pad)]\n",
    "    top = [minx + xlen*loc_x, miny + ylen*(loc_y - pad + height)]\n",
    "    center = [minx + xlen*loc_x, left[1] + (top[1] - left[1])*.4]\n",
    "    triangle = mpatches.Polygon([left, top, right, center], color='k')\n",
    "    ax.text(s='N',\n",
    "            x=minx + xlen*loc_x,\n",
    "            y=miny + ylen*(loc_y - pad + height),\n",
    "            fontsize=labelsize,\n",
    "            horizontalalignment='center',\n",
    "            verticalalignment='bottom')\n",
    "    ax.add_patch(triangle)\n",
    "\n",
    "def add_scalebar(ax,lon0,lat0,length,size=0.45,lw=1.5):\n",
    "    '''\n",
    "    ax: 坐标轴\n",
    "    lon0: 经度\n",
    "    lat0: 纬度\n",
    "    length: 长度\n",
    "    size: 控制粗细和距离的\n",
    "    '''\n",
    "    # style 3\n",
    "    lw = lw\n",
    "    ax.hlines(y=lat0,  xmin = lon0, xmax = lon0+length/111, colors=\"black\", ls=\"-\", lw=lw, label='%d km' % (length))\n",
    "    ax.vlines(x = lon0, ymin = lat0-size, ymax = lat0+size, colors=\"black\", ls=\"-\", lw=lw)\n",
    "    ax.vlines(x = lon0+length/2/111, ymin = lat0-size, ymax = lat0+size, colors=\"black\", ls=\"-\", lw=lw)\n",
    "    ax.vlines(x = lon0+length/111, ymin = lat0-size, ymax = lat0+size, colors=\"black\", ls=\"-\", lw=lw)\n",
    "    ax.text(lon0+length/111,lat0+size+0.12,'%d' % (length),horizontalalignment = 'center',size=12)\n",
    "    ax.text(lon0+length/2/111,lat0+size+0.12,'%d' % (length/2),horizontalalignment = 'center',size=12)\n",
    "    ax.text(lon0,lat0+size+0.12,'0',horizontalalignment = 'center',size=12)\n",
    "    ax.text(lon0+length/111/2*2.5,lat0+size+0.12,'km',horizontalalignment = 'center',size=12)\n",
    "\n",
    "def custom_formatter(x, pos):\n",
    "    if x == 0:\n",
    "        return \"0\"\n",
    "    else:\n",
    "        str_x = \"{:.15g}\".format(x)\n",
    "        if 'e' in str_x:\n",
    "            base, exponent = str_x.split('e')\n",
    "            base = base.rstrip('0').rstrip('.')\n",
    "            exponent = exponent.replace('+', '').lstrip('0')\n",
    "            if exponent:\n",
    "                return f\"{base}e{exponent}\"\n",
    "            else:\n",
    "                return base\n",
    "        else:\n",
    "            return str_x\n",
    "        \n",
    "def add_degree_E(x, pos):\n",
    "    return f\"{int(x)}°E\"\n",
    "\n",
    "def add_degree_N(y, pos):\n",
    "    return f\"{int(custom_formatter(y, pos))}°N\"\n",
    "\n",
    "def create_subplots(reconstruction_datas,shp_path,lakes_path,model_names):\n",
    "    fig = plt.figure(figsize=(24, 24))\n",
    "\n",
    "    ratio_h = 7 # 竖直比例 3\n",
    "    ratio_v = 4 # 水平比例\n",
    "\n",
    "    absolute_dis_h = 0.7/ratio_h # 中央子图绝对大小（竖直）0.7\n",
    "    absolute_dis_v = 0.7/ratio_v # 中央子图绝对大小（水平）\n",
    "\n",
    "    edge_h = 0.36 # 竖直边缘间隙\n",
    "    edge_v = 0.135 # 水平边缘间隙\n",
    "\n",
    "    mid_h = (1-edge_h*2-absolute_dis_h*3)/2 # 竖直间隙,可以通过竖直边缘间隙调节\n",
    "    mid_v = (1-edge_v*2-absolute_dis_v*4)/3 # 水平间隙……\n",
    "\n",
    "    right_offset_first = 0 # 第一层向右偏移量\n",
    "\n",
    "    # 计算第一层图起点:上层子图  左起点、下起点、宽度比例、高度比例\n",
    "    ax1 = fig.add_axes([edge_v + right_offset_first, 1 - edge_h - absolute_dis_h, absolute_dis_v, absolute_dis_h])\n",
    "    ax2 = fig.add_axes([edge_v + right_offset_first + absolute_dis_v + mid_v, 1 - edge_h - absolute_dis_h, absolute_dis_v, absolute_dis_h])\n",
    "    ax3 = fig.add_axes([edge_v + right_offset_first + 2*(absolute_dis_v + mid_v), 1 - edge_h - absolute_dis_h, absolute_dis_v, absolute_dis_h])\n",
    "    ax4 = fig.add_axes([edge_v + right_offset_first + 3*(absolute_dis_v + mid_v), 1 - edge_h - absolute_dis_h, absolute_dis_v, absolute_dis_h])\n",
    "\n",
    "    ax5 = fig.add_axes([edge_v + right_offset_first, 1 - edge_h - 2*absolute_dis_h - mid_h, absolute_dis_v, absolute_dis_h])\n",
    "    ax6 = fig.add_axes([edge_v + right_offset_first + absolute_dis_v + mid_v, 1 - edge_h - 2*absolute_dis_h - mid_h, absolute_dis_v, absolute_dis_h])\n",
    "    ax7 = fig.add_axes([edge_v + right_offset_first + 2*(absolute_dis_v + mid_v), 1 - edge_h - 2*absolute_dis_h - mid_h, absolute_dis_v, absolute_dis_h])\n",
    "    ax8 = fig.add_axes([edge_v + right_offset_first + 3*(absolute_dis_v + mid_v), 1 - edge_h - 2*absolute_dis_h - mid_h, absolute_dis_v, absolute_dis_h])\n",
    "\n",
    "    ax9 = fig.add_axes([edge_v + right_offset_first, 1 - edge_h - 3*absolute_dis_h - 2*mid_h, absolute_dis_v, absolute_dis_h])\n",
    "    ax10 = fig.add_axes([edge_v + right_offset_first + absolute_dis_v + mid_v, 1 - edge_h - 3*absolute_dis_h - 2*mid_h, absolute_dis_v, absolute_dis_h])\n",
    "    ax11 = fig.add_axes([edge_v + right_offset_first + 2*(absolute_dis_v + mid_v), 1 - edge_h - 3*absolute_dis_h - 2*mid_h, absolute_dis_v, absolute_dis_h])\n",
    "    ax12 = fig.add_axes([edge_v + right_offset_first + 3*(absolute_dis_v + mid_v), 1 - edge_h - 3*absolute_dis_h - 2*mid_h, absolute_dis_v, absolute_dis_h])\n",
    "\n",
    "    axes = [ax1, ax2, ax3, ax4, ax5, ax6, ax7, ax8, ax9, ax10, ax11, ax12]\n",
    "    vmin, vmax = 0, 0.5\n",
    "    norm = mcolors.Normalize(vmin=vmin, vmax=vmax)\n",
    "    cmap = mcm.BrBG\n",
    "    sm = mcm.ScalarMappable(norm=norm, cmap=cmap)\n",
    "\n",
    "    for i, (key, data_path) in enumerate(reconstruction_datas.items()):\n",
    "        mode = 0.37 if i == 0 else 0.1\n",
    "        left_spines = True  if i in [0,4,8] else False\n",
    "        right_spines = True  if i in [3,7,11] else False\n",
    "        top_spines = True  if i in [0,1,2,3] else False\n",
    "        bottom_spines = True  if i in [8,9,10,11] else False\n",
    "        draw_subplot(axes[i], reconstruction_datas[key],mode,shp_path,lakes_path,model_names,model_name=key,vmin=vmin, vmax=vmax,cmap=cmap,top_spines=top_spines,right_spines=right_spines, bottom_spines=bottom_spines, left_spines=left_spines)\n",
    "\n",
    "    cbar_ax = fig.add_axes([0.25, 1 - edge_h - 3*absolute_dis_h - 2*mid_h-0.023, 0.5, 0.012])\n",
    "    cbar = plt.colorbar(sm, cax=cbar_ax, orientation='horizontal')#, extend='both'\n",
    "    cbar.set_ticks([vmin, (vmin+vmax)/2, vmax])\n",
    "    cbar.ax.xaxis.set_tick_params(labelsize=14)\n",
    "    cbar.ax.xaxis.set_major_locator(MaxNLocator(nbins=5))\n",
    "    cbar.ax.tick_params(axis='x', which='both', colors='black', length=3, width=1.5)\n",
    "    fig.text(0.757, 0.334, 'm³/m³', fontsize=16, fontname='Times New Roman', verticalalignment='top')\n",
    "    fig.savefig(r\"C:\\Users\\Administrator\\Desktop\\Draw\\eval_darw.jpeg\", dpi=1000, format='jpeg', bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "def draw_subplot(ax, clipped_data, mode, shp_path,lakes_path,model_names,model_name=None,vmin=None, vmax=None,cmap=None,\n",
    "                 top_spines=True, right_spines=True, bottom_spines=True, left_spines=True):\n",
    "    gdf = gpd.read_file(shp_path)\n",
    "    lakes = gpd.read_file(lakes_path)\n",
    "    lakes = lakes.to_crs(epsg=4326)\n",
    "    lakes.plot(ax=ax, edgecolor='white', facecolor='white', linewidth=0)\n",
    "    gdf.plot(ax=ax, edgecolor='black', facecolor='none', linewidth=1.5)\n",
    "\n",
    "    lat_min, lat_max, lon_min, lon_max, lat_points, lon_points= 25.550117,40.450184,66.95031,104.95048,150,381\n",
    "    lat_step = (lat_max - lat_min) / (lat_points - 1)\n",
    "    lon_step = (lon_max - lon_min) / (lon_points - 1)\n",
    "\n",
    "    lon_min_adj = lon_min - lon_step / 2\n",
    "    lon_max_adj = lon_max + lon_step / 2\n",
    "    lat_min_adj = lat_min - lat_step / 2\n",
    "    lat_max_adj = lat_max + lat_step / 2\n",
    "    image = ax.imshow(clipped_data, cmap=cmap, origin='lower', vmin=vmin, vmax=vmax, extent=[lon_min_adj, lon_max_adj, lat_min_adj, lat_max_adj])\n",
    "\n",
    "    # 设置X轴和Y轴的标签\n",
    "    ax.set_xlabel('Longitude(°)',size=16)\n",
    "    ax.set_ylabel('Latitude(°)',size=16)\n",
    "    ax.tick_params(axis='x', which='both', colors='black', length=3, width=1.5, labelsize=14, direction='out', pad=2, bottom=True, top=True, labelbottom=True,labeltop=False)\n",
    "    ax.tick_params(axis='y', which='both', colors='black', length=3, width=1.5, labelsize=14, direction='out', pad=6, left=True, right=True, labelleft=True,labelright=False)\n",
    "    plt.setp(ax.get_xticklabels(), rotation=0,rotation_mode=\"anchor\")\n",
    "    plt.setp(ax.get_yticklabels(), rotation=90,ha=\"center\", rotation_mode=\"anchor\")\n",
    "\n",
    "    # 刻度标签轴绘制\n",
    "    labelsize = 12\n",
    "    pad_x = 4\n",
    "    pad_y = 4\n",
    "    ax.yaxis.label.set_visible(left_spines)\n",
    "    ax.xaxis.label.set_visible(bottom_spines)\n",
    "    ax.xaxis.set_major_formatter(FuncFormatter(add_degree_E))\n",
    "\n",
    "    # 刻度调节\n",
    "    plt.setp(ax.get_yticklabels(), rotation=90,ha=\"center\", rotation_mode=\"anchor\")\n",
    "    # 刻度值格式\n",
    "    ax.yaxis.set_major_formatter(FuncFormatter(add_degree_N))\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['bottom'].set_color('black')\n",
    "    ax.spines['bottom'].set_linewidth(1.5)\n",
    "    ax.spines['left'].set_color('black')\n",
    "    ax.spines['left'].set_linewidth(1.5)\n",
    "    ax.xaxis.set_major_locator(MaxNLocator(nbins=6))\n",
    "    ax.yaxis.set_major_locator(MaxNLocator(nbins=4))\n",
    "    ax.tick_params(axis='y', which='both', colors='black', length=5, width=1.5, labelsize=labelsize, direction='inout', pad=pad_y, right=False, labelright=False)\n",
    "    ax.tick_params(axis='x', which='both', colors='black', length=5, width=1.5, labelsize=labelsize, direction='inout', pad=pad_x, top=False, labeltop=False)\n",
    "    add_north(ax)\n",
    "    add_scalebar(ax,67.9,26,1200,size=0.5,lw=1.5)\n",
    "    ax.grid(False)\n",
    "    # 文本\n",
    "    prefix = \"(\" + chr(ord('a') + model_names.index(model_name)) + \")\" \n",
    "    model_str = f'{prefix} {model_name}'\n",
    "    ax.text(0.01, 0.32, model_str, transform=ax.transAxes, fontsize=16, fontname='Times New Roman', verticalalignment='top')\n",
    "model_names = list(reconstruction_paths.keys())\n",
    "create_subplots(reconstruction_datas,shp_path,lakes_path,model_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (绘图) V2.0 重建结果图 3*4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import seaborn as sns\n",
    "from matplotlib.font_manager import FontProperties\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.cm as mcm\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "sns.set(style=\"whitegrid\", font='Times New Roman')\n",
    "matplotlib.rcParams['font.family'] = 'Times New Roman'\n",
    "legend_font = FontProperties(family='Times New Roman', size=14)\n",
    "font_prop = FontProperties(family='Times New Roman', size=16)\n",
    "\n",
    "def add_north(ax, labelsize=12, loc_x=0.97, loc_y=0.96, width=0.03, height=0.15, pad=0.15):\n",
    "    \"\"\"\n",
    "    画一个比例尺带'N'文字注释\n",
    "    主要参数如下\n",
    "    :param ax: 要画的坐标区域 Axes实例 plt.gca()获取即可\n",
    "    :param labelsize: 显示'N'文字的大小\n",
    "    :param loc_x: 以文字下部为中心的占整个ax横向比例\n",
    "    :param loc_y: 以文字下部为中心的占整个ax纵向比例\n",
    "    :param width: 指南针占ax比例宽度\n",
    "    :param height: 指南针占ax比例高度\n",
    "    :param pad: 文字符号占ax比例间隙\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    minx, maxx = ax.get_xlim()\n",
    "    miny, maxy = ax.get_ylim()\n",
    "    ylen = maxy - miny\n",
    "    xlen = maxx - minx\n",
    "    left = [minx + xlen*(loc_x - width*.5), miny + ylen*(loc_y - pad)]\n",
    "    right = [minx + xlen*(loc_x + width*.5), miny + ylen*(loc_y - pad)]\n",
    "    top = [minx + xlen*loc_x, miny + ylen*(loc_y - pad + height)]\n",
    "    center = [minx + xlen*loc_x, left[1] + (top[1] - left[1])*.4]\n",
    "    triangle = mpatches.Polygon([left, top, right, center], color='k')\n",
    "    ax.text(s='N',\n",
    "            x=minx + xlen*loc_x,\n",
    "            y=miny + ylen*(loc_y - pad + height),\n",
    "            fontsize=labelsize,\n",
    "            horizontalalignment='center',\n",
    "            verticalalignment='bottom')\n",
    "    ax.add_patch(triangle)\n",
    "\n",
    "def add_scalebar(ax,lon0,lat0,length,size=0.45,lw=1.5):\n",
    "    '''\n",
    "    ax: 坐标轴\n",
    "    lon0: 经度\n",
    "    lat0: 纬度\n",
    "    length: 长度\n",
    "    size: 控制粗细和距离的\n",
    "    '''\n",
    "    # style 3\n",
    "    lw = lw\n",
    "    ax.hlines(y=lat0,  xmin = lon0, xmax = lon0+length/111, colors=\"black\", ls=\"-\", lw=lw, label='%d km' % (length))\n",
    "    ax.vlines(x = lon0, ymin = lat0-size, ymax = lat0+size, colors=\"black\", ls=\"-\", lw=lw)\n",
    "    ax.vlines(x = lon0+length/2/111, ymin = lat0-size, ymax = lat0+size, colors=\"black\", ls=\"-\", lw=lw)\n",
    "    ax.vlines(x = lon0+length/111, ymin = lat0-size, ymax = lat0+size, colors=\"black\", ls=\"-\", lw=lw)\n",
    "    ax.text(lon0+length/111,lat0+size+0.12,'%d' % (length),horizontalalignment = 'center',size=12)\n",
    "    ax.text(lon0+length/2/111,lat0+size+0.12,'%d' % (length/2),horizontalalignment = 'center',size=12)\n",
    "    ax.text(lon0,lat0+size+0.12,'0',horizontalalignment = 'center',size=12)\n",
    "    ax.text(lon0+length/111/2*2.5,lat0+size+0.12,'km',horizontalalignment = 'center',size=12)\n",
    "\n",
    "def custom_formatter(x, pos):\n",
    "    if x == 0:\n",
    "        return \"0\"\n",
    "    else:\n",
    "        str_x = \"{:.15g}\".format(x)\n",
    "        if 'e' in str_x:\n",
    "            base, exponent = str_x.split('e')\n",
    "            base = base.rstrip('0').rstrip('.')\n",
    "            exponent = exponent.replace('+', '').lstrip('0')\n",
    "            if exponent:\n",
    "                return f\"{base}e{exponent}\"\n",
    "            else:\n",
    "                return base\n",
    "        else:\n",
    "            return str_x\n",
    "        \n",
    "def add_degree_E(x, pos):\n",
    "    return f\"{int(x)}°E\"\n",
    "\n",
    "def add_degree_N(y, pos):\n",
    "    return f\"{int(custom_formatter(y, pos))}°N\"\n",
    "\n",
    "def create_subplots(reconstruction_datas,shp_path,lakes_path,model_names):\n",
    "    fig = plt.figure(figsize=(24, 24))\n",
    "\n",
    "    ratio_h = 7 # 竖直比例 3\n",
    "    ratio_v = 4 # 水平比例\n",
    "\n",
    "    absolute_dis_h = 0.7/ratio_h # 中央子图绝对大小（竖直）0.7\n",
    "    absolute_dis_v = 0.7/ratio_v # 中央子图绝对大小（水平）\n",
    "\n",
    "    edge_h = 0.32 # 竖直边缘间隙\n",
    "    edge_v = 0.225 # 水平边缘间隙\n",
    "\n",
    "    mid_h = (1-edge_h*2-absolute_dis_h*4)/3 # 竖直间隙,可以通过竖直边缘间隙调节\n",
    "    mid_v = (1-edge_v*2-absolute_dis_v*3)/2 # 水平间隙……\n",
    "\n",
    "    # 计算第一层图起点:上层子图  左起点、下起点、宽度比例、高度比例\n",
    "    ax1 = fig.add_axes([edge_v, 1 - edge_h - absolute_dis_h, absolute_dis_v, absolute_dis_h])\n",
    "    ax2 = fig.add_axes([edge_v + absolute_dis_v + mid_v, 1 - edge_h - absolute_dis_h, absolute_dis_v, absolute_dis_h])\n",
    "    ax3 = fig.add_axes([edge_v + 2*(absolute_dis_v + mid_v), 1 - edge_h - absolute_dis_h, absolute_dis_v, absolute_dis_h])\n",
    "\n",
    "    ax4 = fig.add_axes([edge_v, 1 - edge_h - 2*absolute_dis_h - mid_h, absolute_dis_v, absolute_dis_h])\n",
    "    ax5 = fig.add_axes([edge_v + absolute_dis_v + mid_v, 1 - edge_h - 2*absolute_dis_h - mid_h, absolute_dis_v, absolute_dis_h])\n",
    "    ax6 = fig.add_axes([edge_v + 2*(absolute_dis_v + mid_v), 1 - edge_h - 2*absolute_dis_h - mid_h, absolute_dis_v, absolute_dis_h])\n",
    "\n",
    "    ax7 = fig.add_axes([edge_v, 1 - edge_h - 3*absolute_dis_h - 2*mid_h, absolute_dis_v, absolute_dis_h])\n",
    "    ax8 = fig.add_axes([edge_v + absolute_dis_v + mid_v, 1 - edge_h - 3*absolute_dis_h - 2*mid_h, absolute_dis_v, absolute_dis_h])\n",
    "    ax9 = fig.add_axes([edge_v + 2*(absolute_dis_v + mid_v), 1 - edge_h - 3*absolute_dis_h - 2*mid_h, absolute_dis_v, absolute_dis_h])\n",
    "\n",
    "    ax10 = fig.add_axes([edge_v, 1 - edge_h - 4*absolute_dis_h - 3*mid_h, absolute_dis_v, absolute_dis_h])\n",
    "    ax11 = fig.add_axes([edge_v + absolute_dis_v + mid_v, 1 - edge_h - 4*absolute_dis_h - 3*mid_h, absolute_dis_v, absolute_dis_h])\n",
    "    ax12 = fig.add_axes([edge_v + 2*(absolute_dis_v + mid_v), 1 - edge_h - 4*absolute_dis_h - 3*mid_h, absolute_dis_v, absolute_dis_h])\n",
    "\n",
    "    axes = [ax1, ax2, ax3, ax4, ax5, ax6, ax7, ax8, ax9, ax10, ax11, ax12]\n",
    "    vmin, vmax = 0, 0.5\n",
    "    norm = mcolors.Normalize(vmin=vmin, vmax=vmax)\n",
    "    cmap = mcm.BrBG\n",
    "    sm = mcm.ScalarMappable(norm=norm, cmap=cmap)\n",
    "\n",
    "    for i, (key, data_path) in enumerate(reconstruction_datas.items()):\n",
    "        mode = 0.37 if i == 0 else 0.1\n",
    "        left_spines = True  if i in [0,3,6,9] else False\n",
    "        right_spines = True  if i in [2,5,8,11] else False\n",
    "        top_spines = True  if i in [0,1,2] else False\n",
    "        bottom_spines = True  if i in [9,10,11] else False\n",
    "        draw_subplot(axes[i], reconstruction_datas[key],mode,shp_path,lakes_path,model_names,model_name=key,vmin=vmin, vmax=vmax,cmap=cmap,top_spines=top_spines,right_spines=right_spines, bottom_spines=bottom_spines, left_spines=left_spines)\n",
    "\n",
    "    cbar_ax = fig.add_axes([0.25, 1 - edge_h - 4*absolute_dis_h - 3*mid_h-0.02, 0.5, 0.012])\n",
    "    cbar = plt.colorbar(sm, cax=cbar_ax, orientation='horizontal')#, extend='both'\n",
    "    cbar.set_ticks([vmin, (vmin+vmax)/2, vmax])\n",
    "    cbar.ax.xaxis.set_tick_params(labelsize=14)\n",
    "    cbar.ax.xaxis.set_major_locator(MaxNLocator(nbins=5))\n",
    "    cbar.ax.tick_params(axis='x', which='both', colors='black', length=3, width=1.5)\n",
    "    fig.text(0.752, 0.31, 'm³/m³', fontsize=16, fontname='Times New Roman', verticalalignment='top')\n",
    "\n",
    "    fig.savefig(r\"C:\\Users\\Administrator\\Desktop\\Draw\\eval_darw.jpeg\", dpi=1000, format='jpeg', bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "def draw_subplot(ax, clipped_data, mode, shp_path,lakes_path,model_names,model_name=None,vmin=None, vmax=None,cmap=None,\n",
    "                 top_spines=True, right_spines=True, bottom_spines=True, left_spines=True):\n",
    "    gdf = gpd.read_file(shp_path)\n",
    "    lakes = gpd.read_file(lakes_path)\n",
    "    lakes = lakes.to_crs(epsg=4326)\n",
    "    lakes.plot(ax=ax, edgecolor='white', facecolor='white', linewidth=0)\n",
    "    gdf.plot(ax=ax, edgecolor='black', facecolor='none', linewidth=1.5)\n",
    "\n",
    "    lat_min, lat_max, lon_min, lon_max, lat_points, lon_points= 25.550117,40.450184,66.95031,104.95048,150,381\n",
    "    lat_step = (lat_max - lat_min) / (lat_points - 1)\n",
    "    lon_step = (lon_max - lon_min) / (lon_points - 1)\n",
    "\n",
    "    lon_min_adj = lon_min - lon_step / 2\n",
    "    lon_max_adj = lon_max + lon_step / 2\n",
    "    lat_min_adj = lat_min - lat_step / 2\n",
    "    lat_max_adj = lat_max + lat_step / 2\n",
    "    image = ax.imshow(clipped_data, cmap=cmap, origin='lower', vmin=vmin, vmax=vmax, extent=[lon_min_adj, lon_max_adj, lat_min_adj, lat_max_adj])\n",
    "\n",
    "    # 设置X轴和Y轴的标签\n",
    "    ax.set_xlabel('Longitude(°)',size=16)\n",
    "    ax.set_ylabel('Latitude(°)',size=16)\n",
    "    ax.tick_params(axis='x', which='both', colors='black', length=3, width=1.5, labelsize=14, direction='out', pad=2, bottom=True, top=True, labelbottom=True,labeltop=False)\n",
    "    ax.tick_params(axis='y', which='both', colors='black', length=3, width=1.5, labelsize=14, direction='out', pad=6, left=True, right=True, labelleft=True,labelright=False)\n",
    "    plt.setp(ax.get_xticklabels(), rotation=0,rotation_mode=\"anchor\")\n",
    "    plt.setp(ax.get_yticklabels(), rotation=90,ha=\"center\", rotation_mode=\"anchor\")\n",
    "\n",
    "    # 刻度标签轴绘制\n",
    "    labelsize = 12\n",
    "    pad_x = 4\n",
    "    pad_y = 6\n",
    "    ax.yaxis.label.set_visible(left_spines)\n",
    "    ax.xaxis.label.set_visible(bottom_spines)\n",
    "    ax.xaxis.set_major_formatter(FuncFormatter(add_degree_E))\n",
    "\n",
    "    # 刻度调节\n",
    "    plt.setp(ax.get_yticklabels(), rotation=90,ha=\"center\", rotation_mode=\"anchor\")\n",
    "    # 刻度值格式\n",
    "    ax.yaxis.set_major_formatter(FuncFormatter(add_degree_N))\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['bottom'].set_color('black')\n",
    "    ax.spines['bottom'].set_linewidth(1.5)\n",
    "    ax.spines['left'].set_color('black')\n",
    "    ax.spines['left'].set_linewidth(1.5)\n",
    "    ax.xaxis.set_major_locator(MaxNLocator(nbins=6))\n",
    "    ax.yaxis.set_major_locator(MaxNLocator(nbins=4))\n",
    "    ax.tick_params(axis='y', which='both', colors='black', length=3, width=1.5, labelsize=labelsize, direction='in', pad=pad_y, right=False, labelright=False)\n",
    "    ax.tick_params(axis='x', which='both', colors='black', length=3, width=1.5, labelsize=labelsize, direction='in', pad=pad_x, top=False, labeltop=False)\n",
    "    add_north(ax)\n",
    "    add_scalebar(ax,67.9,26.3,1200,size=0.5,lw=1.5)\n",
    "    ax.grid(False)\n",
    "    # 文本\n",
    "    prefix = \"(\" + chr(ord('a') + model_names.index(model_name)) + \")\" \n",
    "    model_str = f'{prefix} {model_name}'\n",
    "    ax.text(0.01, 0.32, model_str, transform=ax.transAxes, fontsize=16, fontname='Times New Roman', verticalalignment='top')\n",
    "model_names = list(reconstruction_paths.keys())\n",
    "create_subplots(reconstruction_datas,shp_path,lakes_path,model_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (绘图) V3.0 重建结果图 3*4 \n",
    "inset_axes: 右下角开窗，图像缩小右移"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import seaborn as sns\n",
    "from matplotlib.font_manager import FontProperties\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.cm as mcm\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "from matplotlib.patches import ConnectionPatch\n",
    "from mpl_toolkits.axes_grid1.inset_locator import mark_inset\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "sns.set(style=\"whitegrid\", font='Times New Roman')\n",
    "matplotlib.rcParams['font.family'] = 'Times New Roman'\n",
    "legend_font = FontProperties(family='Times New Roman', size=14)\n",
    "font_prop = FontProperties(family='Times New Roman', size=16)\n",
    "\n",
    "def add_north(ax, labelsize=12, loc_x=0.97, loc_y=1, width=0.03, height=0.15, pad=0.15):\n",
    "    \"\"\"\n",
    "    画一个比例尺带'N'文字注释\n",
    "    主要参数如下\n",
    "    :param ax: 要画的坐标区域 Axes实例 plt.gca()获取即可\n",
    "    :param labelsize: 显示'N'文字的大小\n",
    "    :param loc_x: 以文字下部为中心的占整个ax横向比例\n",
    "    :param loc_y: 以文字下部为中心的占整个ax纵向比例\n",
    "    :param width: 指南针占ax比例宽度\n",
    "    :param height: 指南针占ax比例高度\n",
    "    :param pad: 文字符号占ax比例间隙\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    minx, maxx = ax.get_xlim()\n",
    "    miny, maxy = ax.get_ylim()\n",
    "    ylen = maxy - miny\n",
    "    xlen = maxx - minx\n",
    "    left = [minx + xlen*(loc_x - width*.5), miny + ylen*(loc_y - pad)]\n",
    "    right = [minx + xlen*(loc_x + width*.5), miny + ylen*(loc_y - pad)]\n",
    "    top = [minx + xlen*loc_x, miny + ylen*(loc_y - pad + height)]\n",
    "    center = [minx + xlen*loc_x, left[1] + (top[1] - left[1])*.4]\n",
    "    triangle = mpatches.Polygon([left, top, right, center], color='k')\n",
    "    ax.text(s='N',\n",
    "            x=minx + xlen*loc_x,\n",
    "            y=miny + ylen*(loc_y - pad + height),\n",
    "            fontsize=labelsize,\n",
    "            horizontalalignment='center',\n",
    "            verticalalignment='bottom')\n",
    "    ax.add_patch(triangle)\n",
    "\n",
    "def add_scalebar(ax,lon0,lat0,length,size=0.45,lw=1.5):\n",
    "    '''\n",
    "    ax: 坐标轴\n",
    "    lon0: 经度\n",
    "    lat0: 纬度\n",
    "    length: 长度\n",
    "    size: 控制粗细和距离的\n",
    "    '''\n",
    "    # style 3\n",
    "    lw = lw\n",
    "    ax.hlines(y=lat0,  xmin = lon0, xmax = lon0+length/111, colors=\"black\", ls=\"-\", lw=lw, label='%d km' % (length))\n",
    "    ax.vlines(x = lon0, ymin = lat0-size, ymax = lat0+size, colors=\"black\", ls=\"-\", lw=lw)\n",
    "    ax.vlines(x = lon0+length/2/111, ymin = lat0-size, ymax = lat0+size, colors=\"black\", ls=\"-\", lw=lw)\n",
    "    ax.vlines(x = lon0+length/111, ymin = lat0-size, ymax = lat0+size, colors=\"black\", ls=\"-\", lw=lw)\n",
    "    ax.text(lon0+length/111,lat0+size+0.14,'%d' % (length),horizontalalignment = 'center',size=12)\n",
    "    ax.text(lon0+length/2/111,lat0+size+0.14,'%d' % (length/2),horizontalalignment = 'center',size=12)\n",
    "    ax.text(lon0,lat0+size+0.14,'0',horizontalalignment = 'center',size=12)\n",
    "    ax.text(lon0+length/111/2*2.35,lat0+size+0.14,'km',horizontalalignment = 'center',size=12)\n",
    "\n",
    "def custom_formatter(x, pos):\n",
    "    if x == 0:\n",
    "        return \"0\"\n",
    "    else:\n",
    "        str_x = \"{:.15g}\".format(x)\n",
    "        if 'e' in str_x:\n",
    "            base, exponent = str_x.split('e')\n",
    "            base = base.rstrip('0').rstrip('.')\n",
    "            exponent = exponent.replace('+', '').lstrip('0')\n",
    "            if exponent:\n",
    "                return f\"{base}e{exponent}\"\n",
    "            else:\n",
    "                return base\n",
    "        else:\n",
    "            return str_x\n",
    "        \n",
    "def add_degree_E(x, pos):\n",
    "    return f\"{int(x)}°E\"\n",
    "\n",
    "def add_degree_N(y, pos):\n",
    "    return f\"{int(float(custom_formatter(y, pos)))}°N\"\n",
    "\n",
    "def create_subplots(reconstruction_data, reconstruction_clip_data, dem_data, shp_path, lakes_path, model_names):\n",
    "    fig = plt.figure(figsize=(24, 24))\n",
    "\n",
    "    ratio_h = 7 # 竖直比例 3\n",
    "    ratio_v = 4 # 水平比例\n",
    "\n",
    "    absolute_dis_h = 0.7/ratio_h # 中央子图绝对大小（竖直）0.7\n",
    "    absolute_dis_v = 0.7/ratio_v # 中央子图绝对大小（水平）\n",
    "\n",
    "    edge_h = 0.32 # 竖直边缘间隙\n",
    "    edge_v = 0.225 # 水平边缘间隙\n",
    "\n",
    "    mid_h = (1-edge_h*2-absolute_dis_h*4)/3 # 竖直间隙,可以通过竖直边缘间隙调节\n",
    "    mid_v = (1-edge_v*2-absolute_dis_v*3)/2 # 水平间隙……\n",
    "\n",
    "    # 计算第一层图起点:上层子图  左起点、下起点、宽度比例、高度比例\n",
    "    ax1 = fig.add_axes([edge_v, 1 - edge_h - absolute_dis_h, absolute_dis_v, absolute_dis_h])\n",
    "    ax2 = fig.add_axes([edge_v + absolute_dis_v + mid_v, 1 - edge_h - absolute_dis_h, absolute_dis_v, absolute_dis_h])\n",
    "    ax3 = fig.add_axes([edge_v + 2*(absolute_dis_v + mid_v), 1 - edge_h - absolute_dis_h, absolute_dis_v, absolute_dis_h])\n",
    "\n",
    "    ax4 = fig.add_axes([edge_v, 1 - edge_h - 2*absolute_dis_h - mid_h, absolute_dis_v, absolute_dis_h])\n",
    "    ax5 = fig.add_axes([edge_v + absolute_dis_v + mid_v, 1 - edge_h - 2*absolute_dis_h - mid_h, absolute_dis_v, absolute_dis_h])\n",
    "    ax6 = fig.add_axes([edge_v + 2*(absolute_dis_v + mid_v), 1 - edge_h - 2*absolute_dis_h - mid_h, absolute_dis_v, absolute_dis_h])\n",
    "\n",
    "    ax7 = fig.add_axes([edge_v, 1 - edge_h - 3*absolute_dis_h - 2*mid_h, absolute_dis_v, absolute_dis_h])\n",
    "    ax8 = fig.add_axes([edge_v + absolute_dis_v + mid_v, 1 - edge_h - 3*absolute_dis_h - 2*mid_h, absolute_dis_v, absolute_dis_h])\n",
    "    ax9 = fig.add_axes([edge_v + 2*(absolute_dis_v + mid_v), 1 - edge_h - 3*absolute_dis_h - 2*mid_h, absolute_dis_v, absolute_dis_h])\n",
    "\n",
    "    ax10 = fig.add_axes([edge_v, 1 - edge_h - 4*absolute_dis_h - 3*mid_h, absolute_dis_v, absolute_dis_h])\n",
    "    ax11 = fig.add_axes([edge_v + absolute_dis_v + mid_v, 1 - edge_h - 4*absolute_dis_h - 3*mid_h, absolute_dis_v, absolute_dis_h])\n",
    "    ax12 = fig.add_axes([edge_v + 2*(absolute_dis_v + mid_v), 1 - edge_h - 4*absolute_dis_h - 3*mid_h, absolute_dis_v, absolute_dis_h])\n",
    "\n",
    "    axes = [ax1, ax2, ax3, ax4, ax5, ax6, ax7, ax8, ax9, ax10, ax11, ax12]\n",
    "    vmin, vmax = 0, 0.5\n",
    "    norm = mcolors.Normalize(vmin=vmin, vmax=vmax)\n",
    "    cmap = mcm.BrBG\n",
    "    sm = mcm.ScalarMappable(norm=norm, cmap=cmap)\n",
    "\n",
    "    for i, (key, data_path) in enumerate(reconstruction_data.items()):\n",
    "        mode = 0.37 if i == 0 else 0.1\n",
    "        left_spines = True  if i in [0,3,6,9] else False\n",
    "        right_spines = True  if i in [2,5,8,11] else False\n",
    "        top_spines = True  if i in [0,1,2] else False\n",
    "        bottom_spines = True  if i in [9,10,11] else False\n",
    "        full_mode = 'dem' if i==0 else None\n",
    "        draw_subplot(axes[i], reconstruction_data[key], dem_data, reconstruction_clip_data[key], mode, shp_path, lakes_path, model_names, model_name=key,\n",
    "                    vmin=vmin, vmax=vmax, cmap=cmap, top_spines=top_spines, right_spines=right_spines, bottom_spines=bottom_spines, left_spines=left_spines, full_mode=full_mode)\n",
    "\n",
    "    cbar_ax = fig.add_axes([0.25, 1 - edge_h - 4*absolute_dis_h - 3*mid_h-0.02, 0.5, 0.012])\n",
    "    cbar = plt.colorbar(sm, cax=cbar_ax, orientation='horizontal')\n",
    "    cbar.set_ticks([vmin, (vmin+vmax)/2, vmax])\n",
    "    cbar.ax.xaxis.set_tick_params(labelsize=14)\n",
    "    cbar.ax.xaxis.set_major_locator(MaxNLocator(nbins=5))\n",
    "    cbar.ax.tick_params(axis='x', which='both', colors='black', length=3, width=1.5)\n",
    "    cbar.outline.set_linewidth(0.8)\n",
    "    fig.text(0.752, 0.31, 'm³/m³', fontsize=16, fontname='Times New Roman', verticalalignment='top')\n",
    "\n",
    "    fig.savefig(r\"C:\\Users\\Administrator\\Desktop\\Draw\\orginal_darw.jpeg\", dpi=1000, format='jpeg', bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "def draw_subplot(ax, full_data, full_dem, clipped_data, mode, shp_path,lakes_path,model_names,model_name=None,vmin=None, vmax=None,cmap=None,\n",
    "                 top_spines=True, right_spines=True, bottom_spines=True, left_spines=True, full_mode=None):\n",
    "    \n",
    "    gdf = gpd.read_file(shp_path)\n",
    "    lakes = gpd.read_file(lakes_path)\n",
    "    lakes = lakes.to_crs(epsg=4326)\n",
    "    lakes.plot(ax=ax, edgecolor='white', facecolor='white', linewidth=0)\n",
    "    gdf.plot(ax=ax, edgecolor='black', facecolor='none', linewidth=1.5)\n",
    "\n",
    "    lat_min, lat_max, lon_min, lon_max, lat_points, lon_points= 25.550117,40.450184,66.95031,104.95048,150,381\n",
    "    lat_step = (lat_max - lat_min) / (lat_points - 1)\n",
    "    lon_step = (lon_max - lon_min) / (lon_points - 1)\n",
    "\n",
    "    lon_min_adj = lon_min - lon_step / 2\n",
    "    lon_max_adj = lon_max + lon_step / 2\n",
    "    lat_min_adj = lat_min - lat_step / 2\n",
    "    lat_max_adj = lat_max + lat_step / 2\n",
    "\n",
    "    # 开窗\n",
    "    sub_ax = ax.inset_axes((-0.10, 0.043, 0.7, 0.52))\n",
    "    \n",
    "    if full_mode is None:\n",
    "        draw_sub_subplot(sub_ax, gdf, lakes, full_data, full_dem, vmin, vmax, cmap,lon_min_adj,lon_max_adj,lat_min_adj,lat_max_adj,draw_mode=full_mode)\n",
    "    else:\n",
    "        sub_ax2 = ax.inset_axes((-0.065, 0.572, 0.45, 0.32))\n",
    "        draw_sub_subplot(sub_ax, gdf, lakes, full_data, full_dem, vmin, vmax, cmap,lon_min_adj,lon_max_adj,lat_min_adj,lat_max_adj,draw_mode='dem')\n",
    "        draw_sub_subplot(sub_ax2, gdf, lakes, full_data, full_dem, vmin, vmax, cmap,lon_min_adj,lon_max_adj,lat_min_adj,lat_max_adj,draw_mode='original')\n",
    "\n",
    "    image = ax.imshow(clipped_data, cmap=cmap, origin='lower', vmin=vmin, vmax=vmax, extent=[lon_min_adj, lon_max_adj, lat_min_adj, lat_max_adj])\n",
    "    ax.set_xlim([lon_min_adj-11, lon_max_adj])\n",
    "    ax.set_ylim([lat_min_adj-4, lat_max_adj])\n",
    "    # 设置X轴和Y轴的标签\n",
    "    ax.set_xlabel('Longitude(°)',size=16)\n",
    "    ax.set_ylabel('Latitude(°)',size=16)\n",
    "    ax.tick_params(axis='x', which='both', colors='black', length=3, width=1.5, labelsize=14, direction='out', pad=2, bottom=True, top=True, labelbottom=True,labeltop=False)\n",
    "    ax.tick_params(axis='y', which='both', colors='black', length=3, width=1.5, labelsize=14, direction='out', pad=6, left=True, right=True, labelleft=True,labelright=False)\n",
    "    plt.setp(ax.get_xticklabels(), rotation=0,rotation_mode=\"anchor\")\n",
    "    plt.setp(ax.get_yticklabels(), rotation=90,ha=\"center\", rotation_mode=\"anchor\")\n",
    "\n",
    "    # 刻度标签轴绘制\n",
    "    labelsize = 12\n",
    "    pad_x = 4\n",
    "    pad_y = 6\n",
    "    ax.yaxis.label.set_visible(left_spines)\n",
    "    ax.xaxis.label.set_visible(bottom_spines)\n",
    "    ax.xaxis.set_major_formatter(FuncFormatter(add_degree_E))\n",
    "\n",
    "    # 刻度调节\n",
    "    plt.setp(ax.get_yticklabels(), rotation=90,ha=\"center\", rotation_mode=\"anchor\")\n",
    "    # 刻度值格式\n",
    "    ax.yaxis.set_major_formatter(FuncFormatter(add_degree_N))\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['bottom'].set_color('black')\n",
    "    ax.spines['bottom'].set_linewidth(1.5)\n",
    "    ax.spines['left'].set_color('black')\n",
    "    ax.spines['left'].set_linewidth(1.5)\n",
    "    ax.xaxis.set_major_locator(MaxNLocator(nbins=6))\n",
    "    ax.yaxis.set_major_locator(MaxNLocator(nbins=4))\n",
    "    ax.tick_params(axis='y', which='both', colors='black', length=3, width=1.5, labelsize=labelsize, direction='in', pad=pad_y, right=False, labelright=False)\n",
    "    ax.tick_params(axis='x', which='both', colors='black', length=3, width=1.5, labelsize=labelsize, direction='in', pad=pad_x, top=False, labeltop=False)\n",
    "    add_north(ax)\n",
    "    add_scalebar(ax,84,22.6,2000,size=0.5,lw=1.5)\n",
    "    ax.grid(False)\n",
    "    # 文本\n",
    "    prefix = \"(\" + chr(ord('a') + model_names.index(model_name)) + \")\" \n",
    "    model_str = f'{prefix} {model_name}'\n",
    "    ax.text(0.02, 1.02, model_str, transform=ax.transAxes, fontsize=14, fontname='Times New Roman', verticalalignment='top')\n",
    "    if full_mode is not None: ax.text(0.53, 0.245, 'DEM', transform=ax.transAxes, fontsize=14, fontname='Times New Roman', verticalalignment='top')\n",
    "    if full_mode is not None: \n",
    "        mark_inset(ax, sub_ax, loc1=1, loc2=4, fc=\"none\", ec='black', lw=0.8, ls='dashed')\n",
    "        mark_inset(ax, sub_ax2, loc1=1, loc2=4, fc=\"none\", ec='black', lw=0.5)\n",
    "    else: mark_inset(ax, sub_ax, loc1=2, loc2=4, fc=\"none\", ec='black', lw=0.8)\n",
    "\n",
    "def draw_sub_subplot(ax, gdf=None, lakes=None, full_data=None, full_dem=None, vmin=None, vmax=None, cmap=None, lon_min_adj=None, lon_max_adj=None, lat_min_adj=None, lat_max_adj=None,draw_mode=None):\n",
    "    \n",
    "    specific_lon_min, specific_lat_min = 92, 35.5\n",
    "    specific_lon_max, specific_lat_max = 99, 38.5\n",
    "    lakes.plot(ax=ax, edgecolor='white', facecolor='white', linewidth=0)\n",
    "    gdf.plot(ax=ax, edgecolor='black', facecolor='none', linewidth=1.5)\n",
    "    if draw_mode == None:\n",
    "        ax.imshow(full_data, extent=[lon_min_adj, lon_max_adj, lat_min_adj, lat_max_adj], origin='lower', cmap=cmap, vmin=vmin, vmax=vmax)\n",
    "    elif draw_mode == 'dem':\n",
    "        vmin_new = full_dem.min()\n",
    "        vmax_new = full_dem.max()\n",
    "        im = ax.imshow(full_dem, extent=[lon_min_adj, lon_max_adj, lat_min_adj, lat_max_adj], origin='lower', cmap=cmap, vmin=vmin_new, vmax=vmax_new)\n",
    "        cbar_ax = ax.inset_axes([1.02, 0.0, 0.06, 0.6])\n",
    "        cbar = plt.colorbar(im, cax=cbar_ax, orientation='vertical')\n",
    "        cbar.outline.set_linewidth(0.5)\n",
    "        cbar_ax.yaxis.set_ticks_position('right')\n",
    "        cbar_ax.yaxis.set_tick_params(which='major', direction='inout', length=3, width=1, pad=1, labelleft=False, labelright=True)\n",
    "    else:\n",
    "        ax.imshow(full_data, extent=[lon_min_adj, lon_max_adj, lat_min_adj, lat_max_adj], origin='lower', cmap=cmap, vmin=vmin, vmax=vmax)\n",
    "    ax.set_xlim([specific_lon_min, specific_lon_max])\n",
    "    ax.set_ylim([specific_lat_min, specific_lat_max])\n",
    "\n",
    "    ax.tick_params(axis='x', which='both', length=0, width=0, labelsize=0, labelbottom=False, labeltop=False)\n",
    "    ax.tick_params(axis='y', which='both', length=0, width=0, labelsize=0, labelleft=False, labelright=False)\n",
    "    ax.xaxis.label.set_visible(False)\n",
    "    ax.yaxis.label.set_visible(False)\n",
    "    ax.spines['bottom'].set_color('black')\n",
    "    ax.spines['bottom'].set_linewidth(1)\n",
    "    ax.spines['left'].set_color('black')\n",
    "    ax.spines['left'].set_linewidth(1)\n",
    "    ax.spines['top'].set_color('black')\n",
    "    ax.spines['top'].set_linewidth(1)\n",
    "    ax.spines['right'].set_color('black')\n",
    "    ax.spines['right'].set_linewidth(1)\n",
    "    ax.grid(False)\n",
    "\n",
    "model_names = list(reconstruction_paths.keys())\n",
    "create_subplots(reconstruction_data, reconstruction_clip_data, dem_data, shp_path, lakes_path, model_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 假设 dem_data 是一个二维数组，形状应该与 lat_points 和 lon_points 一致\n",
    "# 如果 dem_data 不是numpy数组，需要先将其转换为numpy数组\n",
    "# 例如: dem_data = np.array(dem_data)\n",
    "lat_min, lat_max, lon_min, lon_max, lat_points, lon_points= 25.550117,40.450184,66.95031,104.95048,150,381\n",
    "lat_step = (lat_max - lat_min) / (lat_points - 1)\n",
    "lon_step = (lon_max - lon_min) / (lon_points - 1)\n",
    "\n",
    "lon_min_adj = lon_min - lon_step / 2\n",
    "lon_max_adj = lon_max + lon_step / 2\n",
    "lat_min_adj = lat_min - lat_step / 2\n",
    "lat_max_adj = lat_max + lat_step / 2\n",
    "# 创建经纬度的网格\n",
    "latitudes = np.linspace(lat_min, lat_max, lat_points)\n",
    "longitudes = np.linspace(lon_min, lon_max, lon_points)\n",
    "lon_grid, lat_grid = np.meshgrid(longitudes, latitudes)\n",
    "\n",
    "# 绘制图像\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "cmap = 'viridis'  # 选择一个合适的colormap，'terrain'适合地形数据\n",
    "extent = [lon_min_adj, lon_max_adj, lat_min_adj, lat_max_adj]\n",
    "\n",
    "# 使用imshow绘制dem_data\n",
    "# 注意：这里假设dem_data的行对应latitudes，列对应longitudes\n",
    "im = ax.imshow(dem_data, extent=extent, origin='lower', cmap=cmap)\n",
    "\n",
    "# 添加colorbar\n",
    "cbar = fig.colorbar(im, ax=ax, orientation='vertical')\n",
    "cbar.set_label('Elevation (m)')\n",
    "cbar.ax.tick_params(axis='y', which='both', direction='inout', colors='black', length=5, width=1.5)\n",
    "cbar.outline.set_linewidth(0)\n",
    "# 设置图像标题和坐标轴标签\n",
    "ax.set_title('DEM Data Visualization')\n",
    "ax.set_xlabel('Longitude')\n",
    "ax.set_ylabel('Latitude')\n",
    "\n",
    "# 设置坐标轴的范围（可选，imshow已通过extent参数设置）\n",
    "ax.set_xlim(lon_min_adj, lon_max_adj)\n",
    "ax.set_ylim(lat_min_adj, lat_max_adj)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 全局原位网络站点评估图 - 云雨图 + 柱状折线图 + 时间动态图\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (数据处理) 站点指标统计 GDOWN和常规指标\n",
    "用于表格填充，指标统计"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# xlxs文件路径\n",
    "file_paths = {\n",
    "    'PSC-FENet-ST':\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\STPSC-FENet_station_metrics.xlsx\",\n",
    "    'PSC-FENet-SP':\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\SPPSC-FENet_station_metrics.xlsx\",\n",
    "    'PSC-FENet-ST32':\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\STPSC-FENet36_station_metrics.xlsx\",\n",
    "    'PSC-FENet-SP32':\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\SPPSC-FENet36_station_metrics.xlsx\",\n",
    "    'RF': \"D:\\Data_Store\\Dataset\\Reconstruction\\RF\\RF_station_metrics.xlsx\",\n",
    "    'LSTM': \"D:\\Data_Store\\Dataset\\Reconstruction\\LSTM\\LSTM_station_metrics.xlsx\",\n",
    "    'ResNet-SP': \"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\SPRESNET_station_metrics.xlsx\",\n",
    "    'UNet-SP': \"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\SPUNET_station_metrics.xlsx\",\n",
    "    'ResNet-ST': \"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\STRESNET_station_metrics.xlsx\",\n",
    "    'UNet-ST': \"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\STUNET_station_metrics.xlsx\",\n",
    "    'VIT-SP': \"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\SPVIT_station_metrics.xlsx\",\n",
    "    'VIT-ST': \"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\STVIT_station_metrics.xlsx\",\n",
    "    'Temporal': \"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\TEMPORAL_station_metrics.xlsx\"\n",
    "}\n",
    "file_paths_G = {\n",
    "    'PSC-FENet-ST':\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\STPSC-FENet_station_G_metrics.xlsx\",\n",
    "    'PSC-FENet-SP':\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\SPPSC-FENet_station_G_metrics.xlsx\",\n",
    "    'PSC-FENet-ST32':\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\STPSC-FENet36_station_G_metrics.xlsx\",\n",
    "    'PSC-FENet-SP32':\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\SPPSC-FENet36_station_G_metrics.xlsx\",\n",
    "    'RF': \"D:\\Data_Store\\Dataset\\Reconstruction\\RF\\RF_station_G_metrics.xlsx\",\n",
    "    'LSTM': \"D:\\Data_Store\\Dataset\\Reconstruction\\LSTM\\LSTM_station_G_metrics.xlsx\",\n",
    "    'ResNet-SP': \"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\SPRESNET_station_G_metrics.xlsx\",\n",
    "    'UNet-SP': \"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\SPUNET_station_G_metrics.xlsx\",\n",
    "    'ResNet-ST': \"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\STRESNET_station_G_metrics.xlsx\",\n",
    "    'UNet-ST': \"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\STUNET_station_G_metrics.xlsx\",\n",
    "    'VIT-SP': \"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\SPVIT_station_G_metrics.xlsx\",\n",
    "    'VIT-ST': \"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\STVIT_station_G_metrics.xlsx\",\n",
    "    'Temporal': \"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\TEMPORAL_station_G_metrics.xlsx\"\n",
    "}\n",
    "\n",
    "def merge_data(file_paths, file_paths_G):\n",
    "    all_data = {}\n",
    "\n",
    "    # 对每个模型执行合并操作\n",
    "    for model in file_paths:\n",
    "        all_data[model] = {}\n",
    "\n",
    "        # 获取对应的文件路径\n",
    "        file_path_metrics = file_paths[model]\n",
    "        file_path_g_metrics = file_paths_G[model]\n",
    "        \n",
    "        # 对每个子表进行操作\n",
    "        sheets = ['Shiquanhe', 'Ali', 'Maqu', 'Naqu', 'CTP']\n",
    "        for sheet in sheets:\n",
    "            # 读取原始指标和G指标的数据\n",
    "            df_metrics = pd.read_excel(file_path_metrics, sheet_name=sheet)\n",
    "            df_g_metrics = pd.read_excel(file_path_g_metrics, sheet_name=sheet)\n",
    "            \n",
    "            # 根据站点名合并两个DataFrame\n",
    "            merged_df = pd.merge(df_metrics, df_g_metrics, on='站点名', suffixes=('', '_G'))\n",
    "            \n",
    "            # 将合并后的DataFrame存入字典\n",
    "            all_data[model][sheet] = merged_df\n",
    "\n",
    "    return all_data\n",
    "\n",
    "def summarize_metrics(all_data):\n",
    "    summary_by_model = {}\n",
    "\n",
    "    for model in all_data:\n",
    "        # 创建一个空的DataFrame用于当前模型\n",
    "        model_summaries = []\n",
    "        \n",
    "        for sheet in all_data[model]:\n",
    "            df = all_data[model][sheet]\n",
    "            split_data = df['站点名'].str.rsplit('_', n=2, expand=True)\n",
    "            df['point_name'] = split_data[0]\n",
    "            df['lon'] = split_data[1].astype(float)\n",
    "            df['lat'] = split_data[2].astype(float)\n",
    "            df_sorted = df.sort_values(by=['lat', 'lon'], ascending=[False, True])\n",
    "            best_records = df_sorted.groupby(['lat', 'lon'], as_index=False).apply(lambda x: x.nlargest(1, 'PearsonR')).reset_index(drop=True)\n",
    "            \n",
    "            # 选择所需的指标列\n",
    "            metrics_df = best_records[['RMSE', 'ubRMSE', 'PearsonR', 'Bias', 'GEFFI', 'GPREC', 'GACCU', 'GDOWN']]\n",
    "            \n",
    "            # 计算均值（排除缺失值）\n",
    "            mean_values = {col: metrics_df[col].mean(skipna=True) for col in ['RMSE', 'ubRMSE', 'PearsonR', 'Bias']}\n",
    "            \n",
    "            # 计算数值为正的比例，排除NaN\n",
    "            positive_ratios = {col: (metrics_df[col] > 0).mean(skipna=True) for col in ['GEFFI', 'GPREC', 'GACCU', 'GDOWN']}\n",
    "            \n",
    "            # 创建一个包含所有统计信息的字典\n",
    "            stats_dict = {\n",
    "                'Network': sheet,\n",
    "                'RMSE': mean_values['RMSE'],\n",
    "                'ubRMSE': mean_values['ubRMSE'],\n",
    "                'PearsonR': mean_values['PearsonR'],\n",
    "                'Bias': mean_values['Bias'],\n",
    "                'GEFFI': positive_ratios['GEFFI'],\n",
    "                'GPREC': positive_ratios['GPREC'],\n",
    "                'GACCU': positive_ratios['GACCU'],\n",
    "                'GDOWN': positive_ratios['GDOWN'],\n",
    "            }\n",
    "            \n",
    "            # 将字典转换为DataFrame并添加到列表中\n",
    "            model_summaries.append(pd.DataFrame([stats_dict]))\n",
    "        \n",
    "        # 使用concat代替append来合并所有的DataFrame\n",
    "        summary_data = pd.concat(model_summaries, ignore_index=True)\n",
    "        \n",
    "        # 将'Network'列作为索引\n",
    "        summary_data.set_index('Network', inplace=True)\n",
    "        \n",
    "        # 将当前模型的DataFrame存储在总字典中\n",
    "        summary_by_model[model] = summary_data\n",
    "\n",
    "    return summary_by_model\n",
    "\n",
    "all_data = merge_data(file_paths, file_paths_G)\n",
    "summary_data = summarize_metrics(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reorder_and_extract(summary_data, new_order, new_columns_order):\n",
    "    # 为了按照新的行列顺序重新排列数据，先创建一个空的DataFrame\n",
    "    reordered_summary = pd.DataFrame(columns=new_columns_order, index=new_order)\n",
    "\n",
    "    # 遍历所有模型的数据\n",
    "    for model, data in summary_data.items():\n",
    "        # 对于每个模型，提取每个指定网络的数据，并且重新排列\n",
    "        for network in new_order:\n",
    "            if network in data.index:  # 确保网络名存在于数据中\n",
    "                network_data = data.loc[network, new_columns_order]\n",
    "                reordered_summary.loc[network, :] = network_data.values  # 确保分配的是一维数组\n",
    "            else:\n",
    "                # 如果网络名在数据中不存在，则在该位置填充NaN\n",
    "                reordered_summary.loc[network, :] = np.nan\n",
    "\n",
    "        # 如果需要，此处可以添加输出语句来检查或保存每个模型的数据\n",
    "        print(f\"{model} reordered data:\")\n",
    "        print(reordered_summary)\n",
    "        print(\"\\n\")\n",
    "pd.options.display.float_format = '{:,.4f}'.format\n",
    "new_order = ['Maqu', 'Naqu', 'Ali', 'Shiquanhe', 'CTP']\n",
    "new_columns_order = ['PearsonR', 'Bias', 'RMSE', 'ubRMSE', 'GDOWN']\n",
    "reordered_summary = reorder_and_extract(summary_data, new_order, new_columns_order)\n",
    "print(summary_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "def create_summary_excel(summary_data, models_st, models_sp):\n",
    "    networks = ['Maqu', 'Naqu', 'Ali', 'Shiquanhe', 'CTP']\n",
    "    metrics = ['PearsonR', 'Bias', 'RMSE', 'ubRMSE', 'GDOWN']\n",
    "    columns = ['Products'] + models_st + ['ST-Mean'] + models_sp + ['SP-Mean']\n",
    "    \n",
    "    # 初始化DataFrame\n",
    "    df_summary = pd.DataFrame(index=pd.MultiIndex.from_product([networks, metrics]), columns=columns)\n",
    "    \n",
    "    # 填充数据\n",
    "    for model in models_st + models_sp:\n",
    "        if model not in summary_data:\n",
    "            print(f\"Warning: Model '{model}' not found in summary data.\")\n",
    "            continue\n",
    "        for network in networks:\n",
    "            if network not in summary_data[model].index:\n",
    "                print(f\"Warning: Network '{network}' not found in model '{model}'.\")\n",
    "                continue\n",
    "            for metric in metrics:\n",
    "                if metric not in summary_data[model].columns:\n",
    "                    print(f\"Warning: Metric '{metric}' not found in model '{model}', network '{network}'.\")\n",
    "                    continue\n",
    "                metric_key = 'GDOWN' if metric == '+Gdown' else metric\n",
    "                value = summary_data[model].loc[network, metric_key]\n",
    "                df_summary.loc[(network, metric), model] = value\n",
    "    \n",
    "    # 计算平均值\n",
    "    for metric in metrics:\n",
    "        st_mean = df_summary.loc[(slice(None), metric), models_st].mean(axis=1, skipna=True)\n",
    "        sp_mean = df_summary.loc[(slice(None), metric), models_sp].mean(axis=1, skipna=True)\n",
    "        df_summary.loc[(slice(None), metric), 'ST-Mean'] = st_mean\n",
    "        df_summary.loc[(slice(None), metric), 'SP-Mean'] = sp_mean\n",
    "    \n",
    "    # 保存到Excel\n",
    "    writer = pd.ExcelWriter('C:/Users/Administrator/Desktop/1.xlsx', engine='openpyxl')\n",
    "    df_summary.to_excel(writer, sheet_name=\"Summary\")\n",
    "    writer.save()\n",
    "    writer.close()\n",
    "\n",
    "    return df_summary\n",
    "\n",
    "# 模型名称\n",
    "models_st = ['PSC-FENet-ST','PSC-FENet-ST32', 'ResNet-ST', 'UNet-ST', 'VIT-ST']\n",
    "models_sp = ['PSC-FENet-SP','PSC-FENet-SP32', 'ResNet-SP', 'UNet-SP', 'VIT-SP']\n",
    "\n",
    "# 假设summary_data已经准备好\n",
    "# summary_data = summarize_metrics(all_data)\n",
    "\n",
    "# 创建并保存汇总数据\n",
    "summary_excel = create_summary_excel(summary_data, models_st, models_sp)\n",
    "print(summary_excel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def create_summary_excel(summary_data, models_st, models_sp):\n",
    "    networks = ['Maqu', 'Naqu', 'Ali', 'Shiquanhe', 'CTP']\n",
    "    metrics = ['PearsonR', 'Bias', 'RMSE', 'ubRMSE', 'GDOWN']\n",
    "    columns = ['Products'] + models_st + ['ST-Mean'] + models_sp + ['SP-Mean']\n",
    "    \n",
    "    # 初始化DataFrame\n",
    "    df_summary = pd.DataFrame(index=pd.MultiIndex.from_product([networks, metrics]), columns=columns)\n",
    "    \n",
    "    # 填充数据\n",
    "    for model in models_st + models_sp:\n",
    "        if model not in summary_data:\n",
    "            print(f\"Warning: Model '{model}' not found in summary data.\")\n",
    "            continue\n",
    "        for network in networks:\n",
    "            if network not in summary_data[model].index:\n",
    "                print(f\"Warning: Network '{network}' not found in model '{model}'.\")\n",
    "                continue\n",
    "            for metric in metrics:\n",
    "                if metric not in summary_data[model].columns:\n",
    "                    print(f\"Warning: Metric '{metric}' not found in model '{model}', network '{network}'.\")\n",
    "                    continue\n",
    "                value = summary_data[model].loc[network, metric]\n",
    "                df_summary.loc[(network, metric), model] = value\n",
    "    \n",
    "    # 计算平均值\n",
    "    for metric in metrics:\n",
    "        st_mean = df_summary.loc[(slice(None), metric), models_st].mean(axis=1, skipna=True)\n",
    "        sp_mean = df_summary.loc[(slice(None), metric), models_sp].mean(axis=1, skipna=True)\n",
    "        df_summary.loc[(slice(None), metric), 'ST-Mean'] = st_mean\n",
    "        df_summary.loc[(slice(None), metric), 'SP-Mean'] = sp_mean\n",
    "    \n",
    "    # 保存到Excel\n",
    "    writer = pd.ExcelWriter('C:/Users/Administrator/Desktop/2.xlsx', engine='openpyxl')\n",
    "    df_summary.to_excel(writer, sheet_name=\"Summary\")\n",
    "    writer.save()\n",
    "    writer.close()\n",
    "\n",
    "    return df_summary\n",
    "\n",
    "# 模型名称\n",
    "models_st = ['PSC-FENet-ST', 'LSTM', 'RF', 'Temporal']  # Assuming these are all ST models for example\n",
    "models_sp = []  # No SP models listed\n",
    "\n",
    "# 假设summary_data已经准备好\n",
    "# summary_data = summarize_metrics(all_data)\n",
    "\n",
    "# 创建并保存汇总数据\n",
    "summary_excel = create_summary_excel(summary_data, models_st, models_sp)\n",
    "print(summary_excel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def merge_data(file_paths, file_paths_G):\n",
    "    all_data = {}\n",
    "    sheets = ['Shiquanhe', 'Ali', 'Maqu', 'Naqu', 'CTP']\n",
    "\n",
    "    for model in file_paths:\n",
    "        model_data = {}\n",
    "        df_metrics = pd.read_excel(file_paths[model], sheet_name=None)  # Read all sheets\n",
    "        df_g_metrics = pd.read_excel(file_paths_G[model], sheet_name=None)  # Read all sheets\n",
    "\n",
    "        for sheet in sheets:\n",
    "            # Ensure both metrics and G metrics have the sheet\n",
    "            if sheet in df_metrics and sheet in df_g_metrics:\n",
    "                # Merge on '站点名' assuming it's a common column\n",
    "                merged_df = pd.merge(df_metrics[sheet], df_g_metrics[sheet], on='站点名', suffixes=('', '_G'))\n",
    "                model_data[sheet] = merged_df\n",
    "\n",
    "        all_data[model] = model_data\n",
    "\n",
    "    return all_data\n",
    "\n",
    "def calculate_means(all_data):\n",
    "    results = {}\n",
    "    metrics = ['RMSE', 'ubRMSE', 'PearsonR', 'Bias']  # Ensure these are correct and exist in your DataFrame\n",
    "\n",
    "    for model, data in all_data.items():\n",
    "        model_metrics = []\n",
    "        for sheet in data.values():  # Iterate through each sheet's DataFrame\n",
    "            mean_values = sheet[metrics].mean(skipna=True)\n",
    "            model_metrics.append(mean_values)\n",
    "        \n",
    "        # Average over all sheets if multiple sheets data were collected\n",
    "        model_means = pd.concat(model_metrics, axis=1).mean(axis=1)\n",
    "        results[model] = model_means\n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df.to_excel('C:/Users/Administrator/Desktop/3.xlsx', sheet_name='Summary')\n",
    "\n",
    "    return results_df\n",
    "\n",
    "# Assuming file_paths and file_paths_G are defined\n",
    "all_data = merge_data(file_paths, file_paths_G)\n",
    "results_df = calculate_means(all_data)\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (数据处理) ST/SP产品站点指标统计(2*2箱线图)\n",
    "用于STSP模型对比\n",
    "注：使用的文件是xlsx文件，xlsx文件在最开始的部分生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# xlxs文件路径\n",
    "file_paths = {\n",
    "    'Original': r\"D:\\Data_Store\\Dataset\\Reconstruction\\37SM\\Original_station_metrics.xlsx\",\n",
    "    'PSC-FENet-ST':\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\STPSC-FENet_station_metrics.xlsx\",\n",
    "    'PSC-FENet-SP':\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\SPPSC-FENet_station_metrics.xlsx\",\n",
    "    'PSC-FENet-ST32':\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\STPSC-FENet36_station_metrics.xlsx\",\n",
    "    'PSC-FENet-SP32':\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\SPPSC-FENet36_station_metrics.xlsx\",\n",
    "    'RF': \"D:\\Data_Store\\Dataset\\Reconstruction\\RF\\RF_station_metrics.xlsx\",\n",
    "    'LSTM': \"D:\\Data_Store\\Dataset\\Reconstruction\\LSTM\\LSTM_station_metrics.xlsx\",\n",
    "    'ResNet-SP': \"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\SPRESNET_station_metrics.xlsx\",\n",
    "    'UNet-SP': \"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\SPUNET_station_metrics.xlsx\",\n",
    "    'ResNet-ST': \"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\STRESNET_station_metrics.xlsx\",\n",
    "    'UNet-ST': \"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\STUNET_station_metrics.xlsx\",\n",
    "    'VIT-SP': \"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\SPVIT_station_metrics.xlsx\",\n",
    "    'VIT-ST': \"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\STVIT_station_metrics.xlsx\",\n",
    "    'Temporal': \"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\TEMPORAL_station_metrics.xlsx\"\n",
    "}\n",
    "selected_model_names = ['PSC-FENet-ST', 'PSC-FENet-SP', 'ResNet-ST', 'ResNet-SP', 'UNet-SP', 'UNet-ST', 'VIT-ST', 'VIT-SP']\n",
    "selected_file_paths = {name: paths for name, paths in file_paths.items() if name in selected_model_names}\n",
    "\n",
    "all_metrics_data = pd.DataFrame()\n",
    "labels = list(selected_file_paths.keys())\n",
    "# 加载所有子表，第二行读取\n",
    "for method, path in selected_file_paths.items():\n",
    "    xls = pd.ExcelFile(path)\n",
    "    for sheet_name in xls.sheet_names:\n",
    "        df = pd.read_excel(path, sheet_name=sheet_name, header=0)\n",
    "        df['Method'] = method\n",
    "        all_metrics_data = pd.concat([all_metrics_data, df], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (绘图)  V1.0 2*2箱线图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "from matplotlib.font_manager import FontProperties\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.collections as clt\n",
    "import ptitprince as pt\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "sns.set(style=\"whitegrid\", font='Times New Roman')\n",
    "matplotlib.rcParams['font.family'] = 'Times New Roman'\n",
    "legend_font = FontProperties(family='Times New Roman', size=14)\n",
    "font_prop = FontProperties(family='Times New Roman', size=16)\n",
    "\n",
    "def custom_formatter(x, pos):\n",
    "    if x == 0:\n",
    "        return \"0\"\n",
    "    else:\n",
    "        str_x = \"{:.15g}\".format(x)\n",
    "        if 'e' in str_x:\n",
    "            base, exponent = str_x.split('e')\n",
    "            base = base.rstrip('0').rstrip('.')\n",
    "            exponent = exponent.replace('+', '').lstrip('0')\n",
    "            if exponent:\n",
    "                return f\"{base}e{exponent}\"\n",
    "            else:\n",
    "                return base\n",
    "        else:\n",
    "            return str_x\n",
    "def clean_axis(ax):\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['bottom'].set_visible(False)\n",
    "    ax.spines['left'].set_visible(False)\n",
    "    ax.tick_params(axis='x', which='both', length=0, labelbottom=False)\n",
    "    ax.tick_params(axis='y', which='both', length=0, labelleft=False)\n",
    "    ax.grid(False)\n",
    "\n",
    "def filter_outliers(data, dx, dy):\n",
    "    filtered_data = pd.DataFrame(columns=data.columns)\n",
    "    groups = data.groupby(dy)\n",
    "    for name, group in groups:\n",
    "        Q1 = group[dx].quantile(0.25)\n",
    "        Q3 = group[dx].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        filtered_group = group[(group[dx] >= lower_bound) & (group[dx] <= upper_bound)]\n",
    "        filtered_data = pd.concat([filtered_data, filtered_group])\n",
    "    return filtered_data\n",
    "\n",
    "def create_2d_subplots(all_metrics_data, plt_name = None):\n",
    "    # 创建图形对象\n",
    "    fig = plt.figure(figsize=(18, 24),dpi=500)\n",
    "\n",
    "    ratio_h = 4 # 竖直比例\n",
    "    ratio_v = 3 # 水平比例\n",
    "\n",
    "    absolute_dis_h = 0.4/ratio_h # 中央子图绝对大小（竖直）\n",
    "    absolute_dis_v = 0.9/ratio_v # 中央子图绝对大小（水平）\n",
    "\n",
    "    edge_h = 0.393 # 竖直边缘间隙\n",
    "    edge_v = 0.185 # 水平边缘间隙\n",
    "\n",
    "    mid_h = (1-edge_h*2-absolute_dis_h*2)/1 # 竖直间隙,可以通过竖直边缘间隙调节\n",
    "    mid_v = (1-edge_v*2-absolute_dis_v*2)/1 # 水平间隙……\n",
    "\n",
    "    # 计算第一层图起点:上层子图  左起点、下起点、宽度比例、高度比例\n",
    "    ax1 = fig.add_axes([edge_v, 1 - edge_h - absolute_dis_h, absolute_dis_v, absolute_dis_h])\n",
    "    ax2 = fig.add_axes([edge_v + absolute_dis_v + mid_v, 1 - edge_h - absolute_dis_h, absolute_dis_v, absolute_dis_h])\n",
    "    ax3 = fig.add_axes([edge_v, 1 - edge_h - 2*absolute_dis_h - mid_h, absolute_dis_v, absolute_dis_h])\n",
    "    ax4 = fig.add_axes([edge_v + absolute_dis_v + mid_v, 1 - edge_h - 2*absolute_dis_h - mid_h, absolute_dis_v, absolute_dis_h])\n",
    "\n",
    "    axes = [ax1, ax2, ax3, ax4]\n",
    "    sub_param_left = [0.0, 0., 0.49, 1]\n",
    "    sub_param_right = [0.51, 0., 0.49, 1]\n",
    "\n",
    "    metric_columns = ['PearsonR', 'Bias', 'RMSE', 'ubRMSE']\n",
    "    for i, metric in enumerate(metric_columns):\n",
    "        left_spines = True if i in [0,2] else False\n",
    "        bottom_spines = True if i in [2,3] else False\n",
    "        draw_subplot(axes[i], all_metrics_data[['Method', metric]], metric, sub_param_left, sub_param_right, left_spines=left_spines, bottom_spines=bottom_spines,i=i)\n",
    "\n",
    "    fig.savefig(r\"C:\\Users\\Administrator\\Desktop\\Draw\\sat_original_statistics.jpeg\", dpi=1000, format='jpeg', bbox_inches='tight')\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "def draw_subplot(ax, data, metric, sub_param_left, sub_param_right, left_spines, bottom_spines,i=None):\n",
    "    selected_model_left = ['PSC-FENet-ST', 'ResNet-ST', 'UNet-ST', 'VIT-ST']\n",
    "    selected_model_right = ['PSC-FENet-SP', 'ResNet-SP', 'UNet-SP', 'VIT-SP']\n",
    "    data_left = data[data['Method'].isin(selected_model_left)]\n",
    "    data_right = data[data['Method'].isin(selected_model_right)]\n",
    "    min_val = data[metric].min()\n",
    "    max_val = data[metric].max()\n",
    "    range_val = max_val - min_val\n",
    "    expand_val = range_val * 0.05\n",
    "\n",
    "    min_val_expanded = min_val - expand_val\n",
    "    max_val_expanded = max_val + expand_val\n",
    "    xlim = [-0.5,1] if metric in ['PearsonR'] else (min_val_expanded, max_val_expanded)\n",
    "\n",
    "    sub_ax_left = ax.inset_axes(sub_param_left)\n",
    "    sub_ax_right = ax.inset_axes(sub_param_right)\n",
    "\n",
    "    draw_sub_subplot(sub_ax_left, data_left, metric, xlim, left_tick_spines = True, left_spines=left_spines, bottom_spines=bottom_spines,i=i)\n",
    "    draw_sub_subplot(sub_ax_right, data_right, metric, xlim, left_tick_spines  = False, left_spines=left_spines, bottom_spines=bottom_spines,i=i)\n",
    "\n",
    "    middle_x = 0.5\n",
    "    ax.axvline(x=middle_x, color='black', linestyle='--', linewidth=1.5, alpha=0.5)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['bottom'].set_visible(False)\n",
    "    ax.spines['left'].set_visible(False)\n",
    "    clean_axis(ax)\n",
    "\n",
    "# 未划分左右\n",
    "def draw_sub_subplot(ax, data, metric, xlim, left_tick_spines, left_spines, bottom_spines,i):\n",
    "    dx = metric\n",
    "    dy = 'Method'\n",
    "    ort = \"h\"\n",
    "\n",
    "    # Point Color\n",
    "    color_scheme1 = ['#AA7BC9']\n",
    "    color_scheme2 = ['#4B6CFB']\n",
    "    color_scheme3 = ['#048A93']\n",
    "    color_scheme4 = ['#26A269']\n",
    "\n",
    "    # Violin Color\n",
    "    Violin_Color = ['#C293D8','#7790FB','#20969E','#3EAB78']\n",
    "\n",
    "    all_colors = color_scheme1 + color_scheme2 + color_scheme3 + color_scheme4\n",
    "\n",
    "    half_violinplot_pal = sns.color_palette(Violin_Color)\n",
    "    stripplot_pal = sns.color_palette(all_colors)\n",
    "    ax=pt.half_violinplot(ax=ax, x=dx, y=dy, data=data, palette=half_violinplot_pal, bw=0.4, cut=0., \n",
    "                        scale=\"area\", width=1, inner=None, orient=ort, linewidth=1, alpha=0.7)\n",
    "    \n",
    "    ax = sns.boxplot(ax=ax, x=dx, y=dy, data=data, color=\"black\", width=0.3, zorder=10,\n",
    "                    showcaps=True, \n",
    "                    boxprops={'facecolor':'none', 'edgecolor':'black', 'linewidth':0.8, 'zorder':10}, \n",
    "                    medianprops={'color':'black', 'linewidth':1.5}, \n",
    "                    whiskerprops={'color':'grey', 'linestyle':'--', 'linewidth':1.2}, \n",
    "                    capprops={'color':'black', 'linewidth':1}, \n",
    "                    flierprops={'marker':'o', 'markerfacecolor':'none', 'markeredgecolor':'black', 'markersize':3, 'linewidth':0.6},\n",
    "                    saturation=1, orient=ort)\n",
    "    \n",
    "    filtered_data = pd.DataFrame(columns=data.columns)\n",
    "\n",
    "    groups = data.groupby(dy)\n",
    "    for name, group in groups:\n",
    "        Q1 = group[dx].quantile(0.25)\n",
    "        Q3 = group[dx].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "        filtered_group = group[(group[dx] >= lower_bound) & (group[dx] <= upper_bound)]\n",
    "        filtered_data = pd.concat([filtered_data, filtered_group])\n",
    "    sns.stripplot(ax=ax, x=dx, y=dy, data=filtered_data, jitter=0.12, alpha=0.5,\n",
    "                palette=stripplot_pal, edgecolor='none', size=3, orient=ort, zorder=0)\n",
    "    \n",
    "    group_means = data.groupby(dy)[dx].mean().reset_index()\n",
    "    \n",
    "    for i, (method, mean) in enumerate(group_means.itertuples(index=False)):\n",
    "            ax.scatter(x=mean, y=i, s=20, color='#E86565', zorder=10)\n",
    "\n",
    "    # 设置X轴和Y轴的标签\n",
    "    ax.spines['bottom'].set_color('black')\n",
    "    ax.spines['bottom'].set_linewidth(1.5)\n",
    "    ax.spines['left'].set_color('black')\n",
    "    ax.spines['left'].set_linewidth(1.5)\n",
    "\n",
    "    # 刻度标签轴绘制\n",
    "    locs = ax.get_yticks()\n",
    "    new_locs = locs -0.2\n",
    "    labels = [item.get_text() for item in ax.get_yticklabels()]\n",
    "    new_labels = [label.replace('-ST', '').replace('-SP', '') for label in labels]\n",
    "    ax.set_yticks(new_locs)\n",
    "    ax.set_yticklabels(new_labels)\n",
    "\n",
    "    ax.yaxis.label.set_visible(left_tick_spines)\n",
    "    ax.spines['left'].set_visible(left_tick_spines)\n",
    "    ax.xaxis.label.set_visible(False)\n",
    "\n",
    "    ax.xaxis.set_major_formatter(FuncFormatter(custom_formatter))\n",
    "\n",
    "    i_nbins = [4,4,4,4]\n",
    "    ax.xaxis.set_major_locator(MaxNLocator(nbins=i_nbins[i]))\n",
    "\n",
    "    ax.set_ylim(3.4,-0.8)\n",
    "    ax.set_xlim(xlim)\n",
    "    # 刻度调节\n",
    "    plt.setp(ax.get_xticklabels(), rotation=0,rotation_mode=\"anchor\")\n",
    "    plt.setp(ax.get_yticklabels(), rotation=90,ha=\"center\", rotation_mode=\"anchor\")\n",
    "\n",
    "    # 刻度值格式\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['bottom'].set_visible(True)\n",
    "    labelsize = 10\n",
    "    pad_x = 4\n",
    "    pad_y = 4\n",
    "    ax.tick_params(axis='y', which='both', colors='black', length=3, width=1, labelsize=labelsize, direction='out', pad=pad_y, left=left_tick_spines, right=False, labelright=False, labelleft=left_tick_spines)\n",
    "    ax.tick_params(axis='x', which='both', colors='black', length=3, width=1, labelsize=labelsize, direction='out', pad=pad_x, top=False, bottom=True, labelbottom=True, labeltop=False)\n",
    "\n",
    "    # text\n",
    "    model_str='(ST)' if left_tick_spines else '(SP)'\n",
    "    model_str='ST' if left_tick_spines else 'SP'\n",
    "    left_start = 0.05\n",
    "    down_start = 0.95\n",
    "    ax.text(left_start, down_start, model_str, transform=ax.transAxes, fontsize=12, fontname='Times New Roman', verticalalignment='top')\n",
    "\n",
    "    # 轴Label\n",
    "    if left_spines and not bottom_spines:\n",
    "        ax.set_ylabel('Method-R', fontsize=16, fontweight='normal', labelpad=4)\n",
    "    elif not left_spines and not bottom_spines:\n",
    "        ax.set_ylabel('Method-bias(m³/m³)', fontsize=16, fontweight='normal', labelpad=4)\n",
    "    elif left_spines and bottom_spines:\n",
    "        ax.set_ylabel('Method-RMSE(m³/m³)', fontsize=16, fontweight='normal', labelpad=4)\n",
    "    elif not left_spines and bottom_spines:\n",
    "        ax.set_ylabel('Method-ubRMSE(m³/m³)', fontsize=16, fontweight='normal', labelpad=4)\n",
    "        \n",
    "    ax.set_xlabel('')\n",
    "    ax.grid(False)\n",
    "\n",
    "create_2d_subplots(all_metrics_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (数据处理) ST/SP产品站点指标统计(4*1箱线图)\n",
    "用于STSP模型对比\n",
    "注：使用的文件是xlsx文件，xlsx文件在最开始的部分生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# xlxs文件路径\n",
    "file_paths = {\n",
    "    'Original': r\"D:\\Data_Store\\Dataset\\Reconstruction\\37SM\\Original_station_metrics.xlsx\",\n",
    "    'PSC-FENet-ST':\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\STPSC-FENet_station_metrics.xlsx\",\n",
    "    'PSC-FENet-SP':\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\SPPSC-FENet_station_metrics.xlsx\",\n",
    "    'RF': \"D:\\Data_Store\\Dataset\\Reconstruction\\RF\\RF_station_metrics.xlsx\",\n",
    "    'LSTM': \"D:\\Data_Store\\Dataset\\Reconstruction\\LSTM\\LSTM_station_metrics.xlsx\",\n",
    "    'ResNet-SP': \"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\SPRESNET_station_metrics.xlsx\",\n",
    "    'UNet-SP': \"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\SPUNET_station_metrics.xlsx\",\n",
    "    'ResNet-ST': \"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\STRESNET_station_metrics.xlsx\",\n",
    "    'UNet-ST': \"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\STUNET_station_metrics.xlsx\",\n",
    "    'VIT-SP': \"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\SPVIT_station_metrics.xlsx\",\n",
    "    'VIT-ST': \"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\STVIT_station_metrics.xlsx\",\n",
    "    'Temporal': \"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\TEMPORAL_station_metrics.xlsx\"\n",
    "}\n",
    "\n",
    "selected_model_names = ['PSC-FENet-ST', 'PSC-FENet-SP', 'ResNet-ST', 'ResNet-SP', 'UNet-ST', 'UNet-SP', 'VIT-ST', 'VIT-SP']\n",
    "\n",
    "selected_file_paths = {name: paths for name, paths in file_paths.items() if name in selected_model_names}\n",
    "\n",
    "all_metrics_data = pd.DataFrame()\n",
    "labels = list(selected_file_paths.keys())\n",
    "# 加载所有子表，第二行读取\n",
    "for method, path in selected_file_paths.items():\n",
    "    xls = pd.ExcelFile(path)\n",
    "    for sheet_name in xls.sheet_names:\n",
    "        df = pd.read_excel(path, sheet_name=sheet_name, header=0)\n",
    "        df['Method'] = method\n",
    "        all_metrics_data = pd.concat([all_metrics_data, df], ignore_index=True)\n",
    "all_metrics_data['Method']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (绘图) V2.0 4*1 箱线图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "from matplotlib.font_manager import FontProperties\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.collections as clt\n",
    "import ptitprince as pt\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "sns.set(style=\"whitegrid\", font='Times New Roman')\n",
    "matplotlib.rcParams['font.family'] = 'Times New Roman'\n",
    "legend_font = FontProperties(family='Times New Roman', size=14)\n",
    "font_prop = FontProperties(family='Times New Roman', size=16)\n",
    "\n",
    "def custom_formatter(x, pos):\n",
    "    if x == 0:\n",
    "        return \"0\"\n",
    "    else:\n",
    "        str_x = \"{:.15g}\".format(x)\n",
    "        if 'e' in str_x:\n",
    "            base, exponent = str_x.split('e')\n",
    "            base = base.rstrip('0').rstrip('.')\n",
    "            exponent = exponent.replace('+', '').lstrip('0')\n",
    "            if exponent:\n",
    "                return f\"{base}e{exponent}\"\n",
    "            else:\n",
    "                return base\n",
    "        else:\n",
    "            return str_x\n",
    "def clean_axis(ax):\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['bottom'].set_visible(False)\n",
    "    ax.spines['left'].set_visible(False)\n",
    "    ax.tick_params(axis='x', which='both', length=0, labelbottom=False)\n",
    "    ax.tick_params(axis='y', which='both', length=0, labelleft=False)\n",
    "    ax.grid(False)\n",
    "\n",
    "def create_2d_subplots(all_metrics_data, plt_name = None):\n",
    "    sns.set(style=\"whitegrid\", font='Times New Roman')\n",
    "    matplotlib.rcParams['font.family'] = 'Times New Roman'\n",
    "    matplotlib.rcParams['axes.labelweight'] = 'bold'\n",
    "\n",
    "    # 创建图形对象\n",
    "    fig = plt.figure(figsize=(18, 24))\n",
    "\n",
    "    ratio_h = 4 # 竖直比例\n",
    "    ratio_v = 2.5 # 水平比例\n",
    "\n",
    "    absolute_dis_h = 0.25/ratio_h # 中央子图绝对大小（竖直）0.15\n",
    "    absolute_dis_v = 1/ratio_v # 中央子图绝对大小（水平）0.2\n",
    "\n",
    "    edge_h = 0.37 # 竖直边缘间隙\n",
    "    edge_v = 0.1 # 水平边缘间隙\n",
    "\n",
    "    mid_h = (1-edge_h*2-absolute_dis_h*4)/3   # 竖直间隙\n",
    "    mid_v = 0 # 水平间隙……\n",
    "\n",
    "\n",
    "    # 计算第一层图起点:上层子图  左起点、下起点、宽度比例、高度比例\n",
    "    ax1 = fig.add_axes([edge_v, 1 - edge_h - absolute_dis_h, absolute_dis_v, absolute_dis_h])\n",
    "    ax2 = fig.add_axes([edge_v, 1 - edge_h - 2*absolute_dis_h - mid_h, absolute_dis_v, absolute_dis_h])\n",
    "    ax3 = fig.add_axes([edge_v, 1 - edge_h - 3*absolute_dis_h - 2*mid_h, absolute_dis_v, absolute_dis_h])\n",
    "    ax4 = fig.add_axes([edge_v, 1 - edge_h - 4*absolute_dis_h - 3*mid_h, absolute_dis_v, absolute_dis_h])\n",
    "\n",
    "    axes = [ax1, ax2, ax3, ax4]\n",
    "\n",
    "    metric_columns = ['PearsonR', 'Bias', 'RMSE', 'ubRMSE']\n",
    "    for i, metric in enumerate(metric_columns):\n",
    "        bottom_labels = True if i in [3] else False\n",
    "        draw_subplot(axes[i], all_metrics_data[['Method', metric]], metric, bottom_labels=bottom_labels,i=i)\n",
    "\n",
    "    fig.savefig(r\"C:\\Users\\Administrator\\Desktop\\Draw\\sat_original_statistics.jpeg\", dpi=1000, format='jpeg', bbox_inches='tight')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def draw_subplot(ax, data, metric, bottom_labels,i=None):\n",
    "\n",
    "    min_val = data[metric].min()\n",
    "    max_val = data[metric].max()\n",
    "    range_val = max_val - min_val\n",
    "    expand_val = range_val * 0.05\n",
    "\n",
    "    min_val_expanded = min_val - expand_val\n",
    "    max_val_expanded = max_val + expand_val\n",
    "\n",
    "    dy = metric\n",
    "    dx = 'Method'\n",
    "    ort = \"v\"\n",
    "\n",
    "    # Violin Color\n",
    "    Violin_Color = ['#D1B8DC','#C293D8','#C5CEF5','#7790FB','#6EABAF','#20969E','#81BA9F','#3EAB78']\n",
    "    all_colors = ['#AA7BC9','#AA7BC9','#4B6CFB','#4B6CFB','#048A93','#048A93','#26A269','#26A269']\n",
    "\n",
    "    half_violinplot_pal = sns.color_palette(Violin_Color)\n",
    "    stripplot_pal = sns.color_palette(all_colors)\n",
    "\n",
    "    ax=pt.half_violinplot(ax=ax, x=dx, y=dy, data=data, palette=half_violinplot_pal, bw=0.3, cut=0., \n",
    "                        scale=\"area\", width=0.8, inner=None, orient=ort, linewidth=1, alpha=0.7,order=selected_model_names)\n",
    "    \n",
    "    ax = sns.boxplot(ax=ax, x=dx, y=dy, data=data, color=\"black\", width=0.25, zorder=10,\n",
    "                    showcaps=True, \n",
    "                    boxprops={'facecolor':'none', 'edgecolor':'black', 'linewidth':0.8, 'zorder':10}, \n",
    "                    medianprops={'color':'black', 'linewidth':1.5}, \n",
    "                    whiskerprops={'color':'grey', 'linestyle':'--', 'linewidth':1.2}, \n",
    "                    capprops={'color':'black', 'linewidth':1}, \n",
    "                    flierprops={'marker':'o', 'markerfacecolor':'none', 'markeredgecolor':'black', 'markersize':3, 'linewidth':0.6},\n",
    "                    saturation=1, orient=ort,order=selected_model_names)\n",
    "    \n",
    "    filtered_data = pd.DataFrame(columns=data.columns)\n",
    "\n",
    "    groups = data.groupby(dx)\n",
    "    for name, group in groups:\n",
    "        Q1 = group[dy].quantile(0.25)\n",
    "        Q3 = group[dy].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "        filtered_group = group[(group[dy] >= lower_bound) & (group[dy] <= upper_bound)]\n",
    "        filtered_data = pd.concat([filtered_data, filtered_group])\n",
    "\n",
    "    sns.stripplot(ax=ax, x=dx, y=dy, data=filtered_data, jitter=0.1, alpha=0.5,\n",
    "                palette=stripplot_pal, edgecolor='none', size=3, orient=ort, zorder=0,order=selected_model_names)\n",
    "    \n",
    "    group_means = data.groupby(dx)[dy].mean().reset_index()\n",
    "    \n",
    "    for _, (method, mean) in enumerate(group_means.itertuples(index=False)):\n",
    "        if method in selected_model_names:\n",
    "            ax.scatter(x=selected_model_names.index(method), y=mean, s=20, color='#E86565', zorder=10)\n",
    "    \n",
    "    overall_mean = data[dy].mean()\n",
    "    ax.axhline(y=overall_mean, color='r', linestyle='--', label=f'Overall Mean: {overall_mean:.2f}')\n",
    "\n",
    "\n",
    "    # 设置绘制范围\n",
    "    ylim = [-0.5,1] if metric in ['PearsonR'] else (min_val_expanded, max_val_expanded)\n",
    "    \n",
    "    ax.set_ylim(ylim)\n",
    "    # 设置X轴和Y轴的标签\n",
    "    ax.spines['bottom'].set_color('black')\n",
    "    ax.spines['bottom'].set_linewidth(1.5)\n",
    "    ax.spines['left'].set_color('black')\n",
    "    ax.spines['left'].set_linewidth(1.5)\n",
    "    #ax.spines['top'].set_visible(False)\n",
    "    #ax.spines['right'].set_visible(False)\n",
    "    ax.spines['top'].set_color('black')\n",
    "    ax.spines['top'].set_linewidth(1.5)\n",
    "    ax.spines['right'].set_color('black')\n",
    "    ax.spines['right'].set_linewidth(1.5)\n",
    "\n",
    "    # 轴标签可视性\n",
    "    if not bottom_labels:ax.tick_params(labelbottom=False)\n",
    "    else: ax.set_xlabel(dx)\n",
    "    ax.xaxis.label.set_visible(bottom_labels)\n",
    "\n",
    "    # 轴刻度属性设置\n",
    "    labelsize_x = 9\n",
    "    labelsize_y = 10\n",
    "    pad_x = 4\n",
    "    pad_y = 4\n",
    "    ax.tick_params(axis='y', which='both', colors='black', length=3, width=1, labelsize=labelsize_y, direction='in', pad=pad_y, left=True, right=False, labelright=False, labelleft=True)\n",
    "    ax.tick_params(axis='x', which='both', colors='black', length=3, width=1.5, labelsize=labelsize_x, direction='in', pad=pad_x, top=True, bottom=True, labelbottom=bottom_labels, labeltop=False)\n",
    "    \n",
    "    # 轴Label\n",
    "    if i==0:ax.set_ylabel('Method-R', fontsize=12, fontweight='normal', labelpad=4)\n",
    "    if i==1:ax.set_ylabel('Method-bias(m³/m³)', fontsize=12, fontweight='normal', labelpad=4)\n",
    "    if i==2:ax.set_ylabel('Method-RMSE(m³/m³)', fontsize=12, fontweight='normal', labelpad=4)\n",
    "    if i==3:ax.set_ylabel('Method-ubRMSE(m³/m³)', fontsize=12, fontweight='normal', labelpad=4)\n",
    "\n",
    "    # x轴刻度标签偏移\n",
    "    locs = ax.get_xticks()\n",
    "    new_locs = locs - 0.142  # 向左移动0.2单位\n",
    "    labels = [item.get_text() for item in ax.get_xticklabels()]\n",
    "    ax.set_xticks(new_locs)\n",
    "    ax.set_xticklabels(selected_model_names)\n",
    "\n",
    "    # x轴标签隐藏\n",
    "    ax.set_xlabel('')\n",
    "    ax.grid(False)\n",
    "\n",
    "    # y轴刻度调整\n",
    "    ax.yaxis.set_major_formatter(FuncFormatter(custom_formatter))\n",
    "    i_nbins = [4,4,4,4]\n",
    "    ax.yaxis.set_major_locator(MaxNLocator(nbins=i_nbins[i]))\n",
    "\n",
    "create_2d_subplots(all_metrics_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (数据处理) ST/SP产品站点指标统计(4*1 4*8箱线图)\n",
    "用于STSP模型对比\n",
    "注：使用的文件是xlsx文件，xlsx文件在最开始的部分生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# xlxs文件路径\n",
    "file_paths = {\n",
    "    'Original': r\"D:\\Data_Store\\Dataset\\Reconstruction\\37SM\\Original_station_metrics.xlsx\",\n",
    "    'PSC(ST)':\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\STPSC-FENet_station_metrics.xlsx\",\n",
    "    'PSC(SP)':\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\SPPSC-FENet_station_metrics.xlsx\",\n",
    "    'PSC(ST32)':\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\STPSC-FENet36_station_metrics.xlsx\",\n",
    "    'PSC(SP32)':\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\SPPSC-FENet36_station_metrics.xlsx\",\n",
    "    'RF': \"D:\\Data_Store\\Dataset\\Reconstruction\\RF\\RF_station_metrics.xlsx\",\n",
    "    'LSTM': \"D:\\Data_Store\\Dataset\\Reconstruction\\LSTM\\LSTM_station_metrics.xlsx\",\n",
    "    'ResNet-SP': \"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\SPRESNET_station_metrics.xlsx\",\n",
    "    'UNet-SP': \"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\SPUNET_station_metrics.xlsx\",\n",
    "    'ResNet-ST': \"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\STRESNET_station_metrics.xlsx\",\n",
    "    'UNet-ST': \"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\STUNET_station_metrics.xlsx\",\n",
    "    'VIT-SP': \"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\SPVIT_station_metrics.xlsx\",\n",
    "    'VIT-ST': \"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\STVIT_station_metrics.xlsx\",\n",
    "    'Temporal': \"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\TEMPORAL_station_metrics.xlsx\"\n",
    "}\n",
    "\n",
    "selected_model_names = ['PSC(ST)', 'PSC(SP)', 'PSC(ST32)', 'PSC(SP32)', 'ResNet-ST', 'ResNet-SP', 'UNet-ST', 'UNet-SP', 'VIT-ST', 'VIT-SP']\n",
    "\n",
    "selected_file_paths = {name: paths for name, paths in file_paths.items() if name in selected_model_names}\n",
    "\n",
    "all_metrics_data = pd.DataFrame()\n",
    "labels = list(selected_file_paths.keys())\n",
    "# 加载所有子表，第二行读取\n",
    "for method, path in selected_file_paths.items():\n",
    "    xls = pd.ExcelFile(path)\n",
    "    for sheet_name in xls.sheet_names:\n",
    "        df = pd.read_excel(path, sheet_name=sheet_name, header=0)\n",
    "        df['Method'] = method\n",
    "        all_metrics_data = pd.concat([all_metrics_data, df], ignore_index=True)\n",
    "all_metrics_data['Method']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (绘图) V2.0 4*1 4*8 箱线图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "from matplotlib.font_manager import FontProperties\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.collections as clt\n",
    "import ptitprince as pt\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "sns.set(style=\"whitegrid\", font='Times New Roman')\n",
    "matplotlib.rcParams['font.family'] = 'Times New Roman'\n",
    "legend_font = FontProperties(family='Times New Roman', size=14)\n",
    "font_prop = FontProperties(family='Times New Roman', size=16)\n",
    "\n",
    "def custom_formatter(x, pos):\n",
    "    if x == 0:\n",
    "        return \"0\"\n",
    "    else:\n",
    "        str_x = \"{:.15g}\".format(x)\n",
    "        if 'e' in str_x:\n",
    "            base, exponent = str_x.split('e')\n",
    "            base = base.rstrip('0').rstrip('.')\n",
    "            exponent = exponent.replace('+', '').lstrip('0')\n",
    "            if exponent:\n",
    "                return f\"{base}e{exponent}\"\n",
    "            else:\n",
    "                return base\n",
    "        else:\n",
    "            return str_x\n",
    "def clean_axis(ax):\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['bottom'].set_visible(False)\n",
    "    ax.spines['left'].set_visible(False)\n",
    "    ax.tick_params(axis='x', which='both', length=0, labelbottom=False)\n",
    "    ax.tick_params(axis='y', which='both', length=0, labelleft=False)\n",
    "    ax.grid(False)\n",
    "\n",
    "def create_2d_subplots(all_metrics_data, plt_name = None):\n",
    "    sns.set(style=\"whitegrid\", font='Times New Roman')\n",
    "    matplotlib.rcParams['font.family'] = 'Times New Roman'\n",
    "    matplotlib.rcParams['axes.labelweight'] = 'bold'\n",
    "\n",
    "    # 创建图形对象\n",
    "    fig = plt.figure(figsize=(18, 24))\n",
    "\n",
    "    ratio_h = 4 # 竖直比例\n",
    "    ratio_v = 2.5 # 水平比例\n",
    "\n",
    "    absolute_dis_h = 0.25/ratio_h # 中央子图绝对大小（竖直）0.15\n",
    "    absolute_dis_v = 1/ratio_v # 中央子图绝对大小（水平）0.2\n",
    "\n",
    "    edge_h = 0.37 # 竖直边缘间隙\n",
    "    edge_v = 0.1 # 水平边缘间隙\n",
    "\n",
    "    mid_h = (1-edge_h*2-absolute_dis_h*4)/3   # 竖直间隙\n",
    "    mid_v = 0 # 水平间隙……\n",
    "\n",
    "\n",
    "    # 计算第一层图起点:上层子图  左起点、下起点、宽度比例、高度比例\n",
    "    ax1 = fig.add_axes([edge_v, 1 - edge_h - absolute_dis_h, absolute_dis_v, absolute_dis_h])\n",
    "    ax2 = fig.add_axes([edge_v, 1 - edge_h - 2*absolute_dis_h - mid_h, absolute_dis_v, absolute_dis_h])\n",
    "    ax3 = fig.add_axes([edge_v, 1 - edge_h - 3*absolute_dis_h - 2*mid_h, absolute_dis_v, absolute_dis_h])\n",
    "    ax4 = fig.add_axes([edge_v, 1 - edge_h - 4*absolute_dis_h - 3*mid_h, absolute_dis_v, absolute_dis_h])\n",
    "\n",
    "    axes = [ax1, ax2, ax3, ax4]\n",
    "\n",
    "    metric_columns = ['PearsonR', 'Bias', 'RMSE', 'ubRMSE']\n",
    "    for i, metric in enumerate(metric_columns):\n",
    "        bottom_labels = True if i in [3] else False\n",
    "        draw_subplot(axes[i], all_metrics_data[['Method', metric]], metric, bottom_labels=bottom_labels,i=i)\n",
    "\n",
    "    fig.savefig(r\"C:\\Users\\Administrator\\Desktop\\Draw\\sat_original_statistics.jpeg\", dpi=1000, format='jpeg', bbox_inches='tight')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def draw_subplot(ax, data, metric, bottom_labels,i=None):\n",
    "\n",
    "    min_val = data[metric].min()\n",
    "    max_val = data[metric].max()\n",
    "    range_val = max_val - min_val\n",
    "    expand_val = range_val * 0.05\n",
    "\n",
    "    min_val_expanded = min_val - expand_val\n",
    "    max_val_expanded = max_val + expand_val\n",
    "\n",
    "    dy = metric\n",
    "    dx = 'Method'\n",
    "    ort = \"v\"\n",
    "\n",
    "    # Violin Color\n",
    "    Violin_Color = ['#D1B8DC','#C293D8','#C5CEF5','#7790FB','#6EABAF','#20969E','#81BA9F','#3EAB78','#77BC00','#558700']\n",
    "    all_colors = ['#AA7BC9','#AA7BC9','#4B6CFB','#4B6CFB','#048A93','#048A93','#26A269','#26A269','#73B700','#73B700']\n",
    "\n",
    "    half_violinplot_pal = sns.color_palette(Violin_Color)\n",
    "    stripplot_pal = sns.color_palette(all_colors)\n",
    "\n",
    "    ax=pt.half_violinplot(ax=ax, x=dx, y=dy, data=data, palette=half_violinplot_pal, bw=0.3, cut=0., \n",
    "                        scale=\"area\", width=1.05, inner=None, orient=ort, linewidth=1, alpha=0.7,order=selected_model_names)\n",
    "    \n",
    "    ax = sns.boxplot(ax=ax, x=dx, y=dy, data=data, color=\"black\", width=0.25, zorder=10,\n",
    "                    showcaps=True, \n",
    "                    boxprops={'facecolor':'none', 'edgecolor':'black', 'linewidth':0.8, 'zorder':10}, \n",
    "                    medianprops={'color':'black', 'linewidth':1.5}, \n",
    "                    whiskerprops={'color':'grey', 'linestyle':'--', 'linewidth':1.2}, \n",
    "                    capprops={'color':'black', 'linewidth':1}, \n",
    "                    flierprops={'marker':'o', 'markerfacecolor':'none', 'markeredgecolor':'black', 'markersize':3, 'linewidth':0.6},\n",
    "                    saturation=1, orient=ort,order=selected_model_names)\n",
    "    \n",
    "    filtered_data = pd.DataFrame(columns=data.columns)\n",
    "\n",
    "    groups = data.groupby(dx)\n",
    "    for name, group in groups:\n",
    "        Q1 = group[dy].quantile(0.25)\n",
    "        Q3 = group[dy].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "        filtered_group = group[(group[dy] >= lower_bound) & (group[dy] <= upper_bound)]\n",
    "        filtered_data = pd.concat([filtered_data, filtered_group])\n",
    "\n",
    "    sns.stripplot(ax=ax, x=dx, y=dy, data=filtered_data, jitter=0.1, alpha=0.5,\n",
    "                palette=stripplot_pal, edgecolor='none', size=3, orient=ort, zorder=0,order=selected_model_names)\n",
    "    \n",
    "    group_means = data.groupby(dx)[dy].mean().reset_index()\n",
    "    \n",
    "    for _, (method, mean) in enumerate(group_means.itertuples(index=False)):\n",
    "        if method in selected_model_names:\n",
    "            ax.scatter(x=selected_model_names.index(method), y=mean, s=20, color='#E86565', zorder=10)\n",
    "    \n",
    "    overall_mean = data[dy].mean()\n",
    "    ax.axhline(y=overall_mean, color='r', linestyle='--', label=f'Overall Mean: {overall_mean:.2f}')\n",
    "    print(overall_mean)\n",
    "\n",
    "    # 设置绘制范围\n",
    "    ylim = [-0.5,1] if metric in ['PearsonR'] else (min_val_expanded, max_val_expanded)\n",
    "    \n",
    "    ax.set_ylim(ylim)\n",
    "    # 设置X轴和Y轴的标签\n",
    "    ax.spines['bottom'].set_color('black')\n",
    "    ax.spines['bottom'].set_linewidth(1.5)\n",
    "    ax.spines['left'].set_color('black')\n",
    "    ax.spines['left'].set_linewidth(1.5)\n",
    "    #ax.spines['top'].set_visible(False)\n",
    "    #ax.spines['right'].set_visible(False)\n",
    "    ax.spines['top'].set_color('black')\n",
    "    ax.spines['top'].set_linewidth(1.5)\n",
    "    ax.spines['right'].set_color('black')\n",
    "    ax.spines['right'].set_linewidth(1.5)\n",
    "\n",
    "    # 轴标签可视性\n",
    "\n",
    "    if not bottom_labels:ax.tick_params(labelbottom=False)\n",
    "    else: ax.set_xlabel(dx)\n",
    "    ax.xaxis.label.set_visible(bottom_labels)\n",
    "\n",
    "    # 轴刻度属性设置\n",
    "    labelsize_x = 10\n",
    "    labelsize_y = 10\n",
    "    pad_x = 4\n",
    "    pad_y = 4\n",
    "    ax.tick_params(axis='y', which='both', colors='black', length=3, width=1, labelsize=labelsize_y, direction='in', pad=pad_y, left=True, right=False, labelright=False, labelleft=True)\n",
    "    ax.tick_params(axis='x', which='both', colors='black', length=3, width=1.5, labelsize=labelsize_x, direction='in', pad=pad_x, top=True, bottom=True, labelbottom=bottom_labels, labeltop=False)\n",
    "    '''\n",
    "    if bottom_labels:\n",
    "        labels = ax.get_xticklabels()\n",
    "        for i in range(2):\n",
    "            labels[i].set_fontsize(7)\n",
    "    ''' \n",
    "\n",
    "    # 轴Label\n",
    "    if i==0:ax.set_ylabel('R', fontsize=12, fontweight='normal', labelpad=4)\n",
    "    if i==1:ax.set_ylabel('bias(m³/m³)', fontsize=12, fontweight='normal', labelpad=4)\n",
    "    if i==2:ax.set_ylabel('RMSE(m³/m³)', fontsize=12, fontweight='normal', labelpad=4)\n",
    "    if i==3:ax.set_ylabel('ubRMSE(m³/m³)', fontsize=12, fontweight='normal', labelpad=4)\n",
    "\n",
    "    # x轴刻度标签偏移\n",
    "    locs = ax.get_xticks()\n",
    "    new_locs = locs - 0.142  # 向左移动0.2单位\n",
    "    labels = [item.get_text() for item in ax.get_xticklabels()]\n",
    "    ax.set_xticks(new_locs)\n",
    "    ax.set_xticklabels(selected_model_names)\n",
    "\n",
    "    # x轴标签隐藏\n",
    "    ax.set_xlabel('')\n",
    "    ax.grid(False)\n",
    "\n",
    "    # y轴刻度调整\n",
    "    ax.yaxis.set_major_formatter(FuncFormatter(custom_formatter))\n",
    "    i_nbins = [4,4,4,4]\n",
    "    ax.yaxis.set_major_locator(MaxNLocator(nbins=i_nbins[i]))\n",
    "\n",
    "create_2d_subplots(all_metrics_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (数据处理) 不同模型、全时刻指标统计(柱状折线图)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "time_points = ['0', '3', '6', '9', '12', '15', '18', '21']\n",
    "network_names = ['Shiquanhe', 'Ali', 'Maqu', 'Naqu', 'CTP']\n",
    "file_paths = {\n",
    "    'PSC-FENet-ST': r\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\STPSCFENet_station_time_metrics.xlsx\",\n",
    "    'PSC-FENet-SP': r\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\SPPSCFENet_station_time_metrics.xlsx\",\n",
    "    'PSC-FENet-ST36': r\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\STPSCFENet36_station_time_metrics.xlsx\",\n",
    "    'PSC-FENet-SP36': r\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\SPPSCFENet36_station_time_metrics.xlsx\",\n",
    "    'ResNet-ST': r\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\STRESNET_station_time_metrics.xlsx\",\n",
    "    'ResNet-SP': r\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\SPRESNET_station_time_metrics.xlsx\",\n",
    "    'UNet-ST': r\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\STUNET_station_time_metrics.xlsx\",\n",
    "    'UNet-SP': r\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\SPUNET_station_time_metrics.xlsx\",\n",
    "    'VIT-ST': r\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\STVIT_station_time_metrics.xlsx\",\n",
    "    'VIT-SP': r\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\SPVIT_station_time_metrics.xlsx\",\n",
    "    'RF': r\"D:\\Data_Store\\Dataset\\Reconstruction\\RF\\RF_station_time_metrics.xlsx\",\n",
    "    'LSTM': r\"D:\\Data_Store\\Dataset\\Reconstruction\\LSTM\\LSTM_station_time_metrics.xlsx\",\n",
    "    'Temporal': r\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\TEMPORAL_station_time_metrics.xlsx\"\n",
    "}\n",
    "\n",
    "\n",
    "def create_combined_data(time_points, network_names, file_paths):\n",
    "    all_data = {model: {time: {network: pd.DataFrame() for network in network_names} for time in time_points} for model in file_paths.keys()}\n",
    "    \n",
    "    for model, file_path in file_paths.items():\n",
    "        xls = pd.ExcelFile(file_path)\n",
    "        for time_point in time_points:\n",
    "            networks_data = {network: pd.DataFrame() for network in network_names}\n",
    "            for sheet_name in xls.sheet_names:\n",
    "                if f\"_{time_point}h\" in sheet_name:\n",
    "                    df = xls.parse(sheet_name)\n",
    "                    split_data = df['站点名'].str.rsplit('_', n=2, expand=True)\n",
    "                    df['point_name'] = split_data[0]\n",
    "                    df['lon'] = split_data[1].astype(float)\n",
    "                    df['lat'] = split_data[2].astype(float)\n",
    "                    df_renamed = df.rename(columns={'PearsonR': 'R'})\n",
    "                    df_sorted = df_renamed.sort_values(by=['lat', 'lon'], ascending=[False, True])\n",
    "                    best_records = df_sorted.groupby(['lat', 'lon'], as_index=False).apply(lambda x: x.loc[x['R'].idxmax()]).reset_index(drop=True)\n",
    "                    best_records = best_records[['point_name', 'lon', 'lat', 'RMSE', 'ubRMSE', 'R', 'Bias']]\n",
    "                    for network in network_names:\n",
    "                        if network in sheet_name:\n",
    "                            networks_data[network] = pd.concat([networks_data[network], best_records], ignore_index=True)\n",
    "            for network in network_names:\n",
    "                all_data[model][time_point][network] = pd.concat([all_data[model][time_point][network], networks_data[network]], ignore_index=True)\n",
    "\n",
    "    return all_data\n",
    "\n",
    "def calculate_average_metrics(all_data):\n",
    "    averages_data = {}\n",
    "    for model in all_data:\n",
    "        averages_data[model] = {}\n",
    "        for time_point in all_data[model]:\n",
    "            combined_df = pd.DataFrame()\n",
    "            for network in all_data[model][time_point]:\n",
    "                df = all_data[model][time_point][network]\n",
    "                if not df.empty:\n",
    "                    # 平均值\n",
    "                    df_metrics = df[['RMSE', 'ubRMSE', 'R', 'Bias']]\n",
    "                    averages = df_metrics.mean().to_frame().T \n",
    "                    averages['Network'] = network \n",
    "                    combined_df = pd.concat([combined_df, averages], ignore_index=True)\n",
    "                    combined_df = combined_df[['Network', 'RMSE', 'ubRMSE', 'R', 'Bias']]\n",
    "            averages_data[model][time_point] = combined_df\n",
    "    return averages_data\n",
    "\n",
    "def calculate_all_networks_average_metrics(all_data):  \n",
    "    all_networks_averages = {}\n",
    "\n",
    "    for model in all_data:\n",
    "        all_networks_averages[model] = {}\n",
    "        combined_df = pd.DataFrame()\n",
    "        data_for_all_time_points = {network: pd.DataFrame() for network in all_data[model][next(iter(all_data[model]))]}\n",
    "        for time_point in all_data[model]:\n",
    "            for network in all_data[model][time_point]:\n",
    "                df = all_data[model][time_point][network]\n",
    "                if not df.empty:\n",
    "                    data_for_all_time_points[network] = pd.concat([data_for_all_time_points[network], df], ignore_index=True)\n",
    "        for network, df in data_for_all_time_points.items():\n",
    "            if not df.empty:\n",
    "                df_metrics = df[['RMSE', 'ubRMSE', 'R', 'Bias']]\n",
    "                averages = df_metrics.mean().to_frame().T\n",
    "                averages['Network'] = network\n",
    "                combined_df = pd.concat([combined_df, averages], ignore_index=True)\n",
    "        combined_df = combined_df[['Network', 'RMSE', 'ubRMSE', 'R', 'Bias']]\n",
    "        all_networks_averages[model] = combined_df\n",
    "\n",
    "    return all_networks_averages\n",
    "\n",
    "\n",
    "all_data = create_combined_data(time_points, network_names, file_paths)\n",
    "average_metrics = calculate_average_metrics(all_data)\n",
    "all_networks_average_metrics = calculate_all_networks_average_metrics(all_data)\n",
    "\n",
    "'''\n",
    "df_example = all_data['UNet-SP']['3']['Ali'] 访问模式\n",
    "model = 'KPNet-ST'\n",
    "time_point = '18'\n",
    "print(average_metrics[model][time_point]) 访问模式\n",
    "model = 'KPNet-ST'\n",
    "all_networks_average_metrics[model]\n",
    "'''\n",
    "# 设置显示浮点数为四位小数\n",
    "pd.options.display.float_format = '{:,.4f}'.format\n",
    "\n",
    "# 指定网络的新顺序\n",
    "new_order = ['Maqu', 'Naqu', 'Ali', 'Shiquanhe', 'CTP']\n",
    "new_columns_order = ['R', 'Bias', 'RMSE', 'ubRMSE']\n",
    "\n",
    "# 遍历字典并打印每个调整后的DataFrame\n",
    "for model, df in all_networks_average_metrics.items():\n",
    "    # 按新顺序重新索引网络，并选择新的列顺序\n",
    "    df_ordered = df.set_index('Network').reindex(new_order).reset_index()[['Network'] + new_columns_order]\n",
    "\n",
    "    print(f\"{model} Performance Metrics:\")\n",
    "    print(df_ordered)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_networks_average_metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (数据处理) 选择模型、全时刻指标统计(柱状折线图)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def create_combined_data(time_points, network_names, file_paths):\n",
    "    all_data = {model: {time: {network: pd.DataFrame() for network in network_names} for time in time_points} for model in file_paths.keys()}\n",
    "    \n",
    "    for model, file_path in file_paths.items():\n",
    "        xls = pd.ExcelFile(file_path)\n",
    "        for time_point in time_points:\n",
    "            networks_data = {network: pd.DataFrame() for network in network_names}\n",
    "            for sheet_name in xls.sheet_names:\n",
    "                if f\"_{time_point}h\" in sheet_name:\n",
    "                    df = xls.parse(sheet_name)\n",
    "                    split_data = df['站点名'].str.rsplit('_', n=2, expand=True)\n",
    "                    df['point_name'] = split_data[0]\n",
    "                    df['lon'] = split_data[1].astype(float)\n",
    "                    df['lat'] = split_data[2].astype(float)\n",
    "                    df_renamed = df.rename(columns={'PearsonR': 'R'})\n",
    "                    df_sorted = df_renamed.sort_values(by=['lat', 'lon'], ascending=[False, True])\n",
    "                    best_records = df_sorted.groupby(['lat', 'lon'], as_index=False).apply(lambda x: x.loc[x['R'].idxmax()]).reset_index(drop=True)\n",
    "                    best_records = best_records[['point_name', 'lon', 'lat', 'RMSE', 'ubRMSE', 'R', 'Bias']]\n",
    "                    for network in network_names:\n",
    "                        if network in sheet_name:\n",
    "                            networks_data[network] = pd.concat([networks_data[network], best_records], ignore_index=True)\n",
    "            for network in network_names:\n",
    "                all_data[model][time_point][network] = pd.concat([all_data[model][time_point][network], networks_data[network]], ignore_index=True)\n",
    "\n",
    "    return all_data\n",
    "\n",
    "def calculate_average_metrics(all_data):\n",
    "    averages_data = {}\n",
    "    for model in all_data:\n",
    "        averages_data[model] = {}\n",
    "        for time_point in all_data[model]:\n",
    "            combined_df = pd.DataFrame()\n",
    "            for network in all_data[model][time_point]:\n",
    "                df = all_data[model][time_point][network]\n",
    "                if not df.empty:\n",
    "                    # 平均值\n",
    "                    df_metrics = df[['RMSE', 'ubRMSE', 'R', 'Bias']]\n",
    "                    averages = df_metrics.mean().to_frame().T \n",
    "                    averages['Network'] = network \n",
    "                    combined_df = pd.concat([combined_df, averages], ignore_index=True)\n",
    "                    combined_df = combined_df[['Network', 'RMSE', 'ubRMSE', 'R', 'Bias']]\n",
    "            averages_data[model][time_point] = combined_df\n",
    "    return averages_data\n",
    "\n",
    "def calculate_all_networks_average_metrics(all_data):  \n",
    "    all_networks_averages = {}\n",
    "\n",
    "    for model in all_data:\n",
    "        all_networks_averages[model] = {}\n",
    "        combined_df = pd.DataFrame()\n",
    "        data_for_all_time_points = {network: pd.DataFrame() for network in all_data[model][next(iter(all_data[model]))]}\n",
    "        for time_point in all_data[model]:\n",
    "            for network in all_data[model][time_point]:\n",
    "                df = all_data[model][time_point][network]\n",
    "                if not df.empty:\n",
    "                    data_for_all_time_points[network] = pd.concat([data_for_all_time_points[network], df], ignore_index=True)\n",
    "        for network, df in data_for_all_time_points.items():\n",
    "            if not df.empty:\n",
    "                df_metrics = df[['RMSE', 'ubRMSE', 'R', 'Bias']]\n",
    "                averages = df_metrics.mean().to_frame().T\n",
    "                averages['Network'] = network\n",
    "                combined_df = pd.concat([combined_df, averages], ignore_index=True)\n",
    "        combined_df = combined_df[['Network', 'RMSE', 'ubRMSE', 'R', 'Bias']]\n",
    "        all_networks_averages[model] = combined_df\n",
    "\n",
    "    return all_networks_averages\n",
    "\n",
    "\n",
    "\n",
    "time_points = ['0', '3', '6', '9', '12', '15', '18', '21']\n",
    "network_names = ['Shiquanhe', 'Ali', 'Maqu', 'Naqu', 'CTP']\n",
    "file_paths = {\n",
    "    'PSC-FENet-ST': r\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\STPSCFENet_station_time_metrics.xlsx\",\n",
    "    'PSC-FENet-SP': r\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\SPPSCFENet_station_time_metrics.xlsx\",\n",
    "    'ResNet-ST': r\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\STRESNET_station_time_metrics.xlsx\",\n",
    "    'ResNet-SP': r\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\SPRESNET_station_time_metrics.xlsx\",\n",
    "    'UNet-ST': r\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\STUNET_station_time_metrics.xlsx\",\n",
    "    'UNet-SP': r\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\SPUNET_station_time_metrics.xlsx\",\n",
    "    'VIT-ST': r\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\STVIT_station_time_metrics.xlsx\",\n",
    "    'VIT-SP': r\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\SPVIT_station_time_metrics.xlsx\",\n",
    "    'RF': r\"D:\\Data_Store\\Dataset\\Reconstruction\\RF\\RF_station_time_metrics.xlsx\",\n",
    "    'LSTM': r\"D:\\Data_Store\\Dataset\\Reconstruction\\LSTM\\LSTM_station_time_metrics.xlsx\",\n",
    "    'Temporal': r\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\TEMPORAL_station_time_metrics.xlsx\"\n",
    "}\n",
    "\n",
    "#selected_model_names = ['PSC-FENet-ST', 'PSC-FENet-SP',  'LSTM', 'RF', 'Temporal']\n",
    "selected_model_names = ['PSC-FENet-ST', 'PSC-FENet-SP', 'ResNet-ST',  'ResNet-SP', 'UNet-ST', 'UNet-SP', 'VIT-ST', 'VIT-SP', 'LSTM', 'RF', 'Temporal']\n",
    "selected_reconstruction_paths = {name: paths for name, paths in file_paths.items() if name in selected_model_names}\n",
    "all_data = create_combined_data(time_points, network_names, selected_reconstruction_paths)\n",
    "average_metrics = calculate_average_metrics(all_data)\n",
    "all_networks_average_metrics = calculate_all_networks_average_metrics(all_data)\n",
    "\n",
    "'''\n",
    "df_example = all_data['UNet-SP']['3']['Ali'] 访问模式\n",
    "model = 'KPNet-ST'\n",
    "time_point = '18'\n",
    "print(average_metrics[model][time_point]) 访问模式\n",
    "model = 'KPNet-ST'\n",
    "all_networks_average_metrics[model]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "time_points = ['0', '3', '6', '9', '12', '15', '18', '21']\n",
    "model_names = ['PSC-FENet-ST', 'PSC-FENet-SP', 'ResNet-ST', 'ResNet-SP', 'UNet-ST', 'UNet-SP', 'VIT-ST', 'VIT-SP', 'LSTM', 'RF', 'Temporal']\n",
    "\n",
    "# 准备一个字典来存储每个度量的DataFrame\n",
    "metrics = ['RMSE', 'ubRMSE', 'R', 'Bias']\n",
    "metric_dfs = {metric: pd.DataFrame(index=time_points, columns=model_names) for metric in metrics}\n",
    "\n",
    "# 聚合数据并计算平均值\n",
    "for model in model_names:\n",
    "    for time in time_points:\n",
    "        data_frames = [all_data[model][time][network] for network in all_data[model][time] if not all_data[model][time][network].empty]\n",
    "        if data_frames:\n",
    "            combined_df = pd.concat(data_frames)\n",
    "            for metric in metrics:\n",
    "                metric_dfs[metric].loc[time, model] = combined_df[metric].mean()\n",
    "# 创建Excel写入器\n",
    "excel_path = 'C:/Users/Administrator/Desktop/4_metrics.xlsx'  # 修改路径为实际用户名\n",
    "writer = pd.ExcelWriter(excel_path, engine='xlsxwriter')\n",
    "\n",
    "# 将每个度量的DataFrame写入不同的工作表\n",
    "for metric, df in metric_dfs.items():\n",
    "    df.to_excel(writer, sheet_name=metric)\n",
    "\n",
    "# 保存并关闭Excel文件\n",
    "writer.save()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (绘图) V1.0 柱状折线图 4*2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.colors import Normalize\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from matplotlib.collections import PolyCollection\n",
    "import matplotlib.colors as mcolors\n",
    "from scipy.stats import gaussian_kde\n",
    "from matplotlib.colors import to_rgba\n",
    "from matplotlib.font_manager import FontProperties\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.lines as mlines\n",
    "import colorsys\n",
    "def aggregate_data(all_data, time_point=None):\n",
    "    data_for_plot = {}\n",
    "    for model in all_data:\n",
    "        combined_df = pd.DataFrame()  # 聚合network和time\n",
    "        time_points = [time_point] if time_point else all_data[model].keys()\n",
    "        for tp in time_points:\n",
    "            for network in all_data[model][tp]:\n",
    "                df = all_data[model][tp][network]\n",
    "                if not df.empty:\n",
    "                    combined_df = pd.concat([combined_df, df], ignore_index=True)\n",
    "        if not combined_df.empty:\n",
    "            data_for_plot[model] = combined_df\n",
    "    return data_for_plot\n",
    "def custom_formatter(x, pos):\n",
    "    if x == 0:\n",
    "        return \"0\"\n",
    "    else:\n",
    "        str_x = \"{:.15g}\".format(x)\n",
    "        if 'e' in str_x:\n",
    "            base, exponent = str_x.split('e')\n",
    "            base = base.rstrip('0').rstrip('.')\n",
    "            exponent = exponent.replace('+', '').lstrip('0')\n",
    "            if exponent:\n",
    "                return f\"{base}e{exponent}\"\n",
    "            else:\n",
    "                return base\n",
    "        else:\n",
    "            return str_x\n",
    "def adjust_color(color, brightness_factor):\n",
    "    rgb = mcolors.hex2color(color)\n",
    "    hls = colorsys.rgb_to_hls(*rgb)\n",
    "    new_hls = (hls[0], max(0, min(1, brightness_factor * hls[1])), hls[2])\n",
    "    # 将调整后的HLS颜色转换回RGB，然后转换为HEX\n",
    "    new_rgb = colorsys.hls_to_rgb(*new_hls)\n",
    "    return mcolors.to_hex(new_rgb)\n",
    "def create_subplots(all_data):\n",
    "    # 创建图形对象,绘制顺序依赖于fig定义顺序\n",
    "    legend_font = FontProperties(family='Times New Roman', size=12)\n",
    "    time_points = ['0', '3', '6', '9', '12', '15', '18', '21']\n",
    "    colors_third = ['#590d22', '#ff4d6d', '#ffccd5']\n",
    "    colors_th = ['#590d22', '#c9184a', '#ff8fa3']\n",
    "    colors_gradient = ['#caf0f8', '#e9f5db']\n",
    "    fig = plt.figure(figsize=(24, 24))\n",
    "\n",
    "    ratio_h = 6.7 # 竖直比例 3\n",
    "    ratio_v = 5.1 # 水平比例\n",
    "\n",
    "    absolute_dis_h = 0.7/ratio_h # 中央子图绝对大小（竖直）0.7\n",
    "    absolute_dis_v = 0.7/ratio_v # 中央子图绝对大小（水平）\n",
    "\n",
    "    edge_h = 0.3803 # 竖直边缘间隙\n",
    "    edge_v = 0.23 # 水平边缘间隙\n",
    "\n",
    "    mid_h = (1-edge_h*2-absolute_dis_h*2)/1 # 竖直间隙,可以通过竖直边缘间隙调节\n",
    "    mid_v = (1-edge_v*2-absolute_dis_v*4)/3 # 水平间隙……\n",
    "\n",
    "    right_offset_first = 0 # 第一层向右偏移量\n",
    "    right_offset_four = 0.05 # 第一层向右偏移量\n",
    "\n",
    "    # 计算第一层图起点:上层子图  左起点、下起点、宽度比例、高度比例 4*2\n",
    "    ax1 = fig.add_axes([edge_v + right_offset_first, 1 - edge_h - absolute_dis_h, absolute_dis_v, absolute_dis_h])\n",
    "    ax2 = fig.add_axes([edge_v + right_offset_first + absolute_dis_v + mid_v, 1 - edge_h - absolute_dis_h, absolute_dis_v, absolute_dis_h])\n",
    "    ax3 = fig.add_axes([edge_v + right_offset_first + 2*(absolute_dis_v + mid_v), 1 - edge_h - absolute_dis_h, absolute_dis_v, absolute_dis_h])\n",
    "    ax4 = fig.add_axes([edge_v + right_offset_first + 3*(absolute_dis_v + mid_v), 1 - edge_h - absolute_dis_h, absolute_dis_v, absolute_dis_h])\n",
    "\n",
    "    ax5 = fig.add_axes([edge_v + right_offset_first, 1 - edge_h - 2*absolute_dis_h - mid_h, absolute_dis_v, absolute_dis_h])\n",
    "    ax6 = fig.add_axes([edge_v + right_offset_first + 1*(absolute_dis_v + mid_v), 1 - edge_h - 2*absolute_dis_h - mid_h, absolute_dis_v, absolute_dis_h])\n",
    "    ax7 = fig.add_axes([edge_v + right_offset_first + 2*(absolute_dis_v + mid_v), 1 - edge_h - 2*absolute_dis_h - mid_h, absolute_dis_v, absolute_dis_h])\n",
    "    ax8 = fig.add_axes([edge_v + right_offset_first + 3*(absolute_dis_v + mid_v), 1 - edge_h - 2*absolute_dis_h - mid_h, absolute_dis_v, absolute_dis_h])\n",
    "\n",
    "    draw_subplot(ax1, all_data, time_point=time_points[0],top_spines=False,right_spines=False,bottom_spines=True,left_spines=True,colors_third = colors_third, start_color_rmse = colors_gradient[0], start_color_ubrmse =  colors_gradient[1],colors_th=colors_th)\n",
    "    draw_subplot(ax2, all_data, time_point=time_points[1],top_spines=False,right_spines=False,bottom_spines=True,left_spines=False,colors_third = colors_third, start_color_rmse = colors_gradient[0], start_color_ubrmse =  colors_gradient[1],colors_th=colors_th)\n",
    "    draw_subplot(ax3, all_data, time_point=time_points[2],top_spines=False,right_spines=False,bottom_spines=True,left_spines=False,colors_third = colors_third, start_color_rmse = colors_gradient[0], start_color_ubrmse =  colors_gradient[1],colors_th=colors_th)\n",
    "    draw_subplot(ax4, all_data, time_point=time_points[3],top_spines=False,right_spines=True,bottom_spines=True,left_spines=False,colors_third = colors_third, start_color_rmse = colors_gradient[0], start_color_ubrmse =  colors_gradient[1],colors_th=colors_th)\n",
    "\n",
    "    draw_subplot(ax5, all_data, time_point=time_points[4],top_spines=True,right_spines=False,bottom_spines=False,left_spines=True,down=True,colors_third = colors_third, start_color_rmse = colors_gradient[0], start_color_ubrmse =  colors_gradient[1],colors_th=colors_th)\n",
    "    draw_subplot(ax6, all_data, time_point=time_points[5],top_spines=True,right_spines=False,bottom_spines=False,left_spines=False,down=True,colors_third = colors_third, start_color_rmse = colors_gradient[0], start_color_ubrmse =  colors_gradient[1],colors_th=colors_th)\n",
    "    draw_subplot(ax7, all_data, time_point=time_points[6],top_spines=True,right_spines=False,bottom_spines=False,left_spines=False,down=True,colors_third = colors_third, start_color_rmse = colors_gradient[0], start_color_ubrmse =  colors_gradient[1],colors_th=colors_th)\n",
    "    draw_subplot(ax8, all_data, time_point=time_points[7],top_spines=True,right_spines=True,bottom_spines=False,left_spines=False,down=True,colors_third = colors_third, start_color_rmse = colors_gradient[0], start_color_ubrmse =  colors_gradient[1],colors_th=colors_th)\n",
    "    \n",
    "    colors_rmse = [adjust_color(colors_gradient[0], 1 - 0.02 * i) for i in range(11)]\n",
    "    colors_ubrmse = [adjust_color(colors_gradient[1], 1 - 0.02 * i) for i in range(11)]\n",
    "    gradient_patch_rmse = mpatches.Patch(color=colors_rmse[5], label='RMSE')\n",
    "    gradient_patch_ubrmse = mpatches.Patch(color=colors_ubrmse[5], label='ubRMSE')\n",
    "    line_r = mlines.Line2D([], [], color='#bc6c25', linestyle='--', marker='X', markeredgecolor='lightgrey', markersize=10, label='R', alpha=0.5)\n",
    "    line_bias = mlines.Line2D([], [], color='purple', linestyle='--', marker='P', markeredgecolor='grey', markersize=10, label='Bias', alpha=0.5)\n",
    "    first_legend_handles = [gradient_patch_rmse, gradient_patch_ubrmse, line_r, line_bias]\n",
    "    plt.figlegend(handles=first_legend_handles, labels=[h.get_label() for h in first_legend_handles], \n",
    "                  loc='upper center', ncol=2, bbox_to_anchor=(0.385, edge_h+0.005), fancybox=False, \n",
    "                  shadow=False, frameon=False, prop=legend_font,\n",
    "                  borderpad=1, labelspacing=0.8,handletextpad=0.5, handlelength=1.5)\n",
    "\n",
    "    colors_top3 = ['#590d22', '#ff4d6d', '#ffccd5']  # 用于条形图\n",
    "    color_X = ['#8A3C23', '#DD5C48', '#D8B08D']  # 用于'X'标记的散点图\n",
    "    color_PLUS = ['#750266', '#B82B76', '#BF65AA']  # 用于'+'标记的散点图\n",
    "    top3_labels = ['1st', '2nd', '3rd']\n",
    "    top3_handles = []\n",
    "    for i in range(3):\n",
    "        bar_handle = mpatches.Patch(color=colors_top3[i], label=' ',alpha =0.8)\n",
    "        scatter_handle_X = mlines.Line2D([], [], color=color_X[i], marker='X', linestyle='None', markersize=10, markeredgecolor='grey', label=top3_labels[i], linewidth=2)  # 实际标签\n",
    "        scatter_handle_PLUS = mlines.Line2D([], [], color=color_PLUS[i], marker='P', linestyle='None', markersize=10, markeredgecolor='grey', label=' ', linewidth=2)\n",
    "        top3_handles.extend([bar_handle, scatter_handle_X, scatter_handle_PLUS])\n",
    "\n",
    "    plt.figlegend(handles=top3_handles, labels=[h.get_label() for h in top3_handles], \n",
    "                  loc='upper center', ncol=3, bbox_to_anchor=(0.29-0.004, edge_h+0.005), \n",
    "                  fancybox=False, shadow=False, frameon=False, prop=legend_font,\n",
    "                  borderpad=1, labelspacing=0,handletextpad=0.5, handlelength=1.5)\n",
    "    \n",
    "    fig.text(0.227, 0.493, 'RMSE', ha='left', size=12, fontproperties=legend_font, rotation=90)\n",
    "    fig.text(0.2168, 0.488, 'ubRMSE', ha='left', size=12, fontproperties=legend_font, rotation=90)\n",
    "    fig.text(0.7685, 0.488, 'PearsonR', ha='left', size=12, fontproperties=legend_font, rotation=90)\n",
    "    fig.text(0.7765, 0.496, 'bias', ha='left', size=12, fontproperties=legend_font, rotation=90)\n",
    "\n",
    "    fig.savefig(r\"C:\\Users\\Administrator\\Desktop\\Draw\\Original_eval.jpeg\", dpi=1000, format='jpeg', bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "def draw_subplot(ax, all_data, time_point=None, width=0.41, start_color_rmse = '#caf0f8', start_color_ubrmse = '#e9f5db',Gradation_degree = 0.02,\n",
    "                 top_spines=True, right_spines=True, bottom_spines=True, left_spines=True,colors_third = None,colors_th=None,\n",
    "                 down=False):\n",
    "\n",
    "    sns.set(style=\"whitegrid\", font='Times New Roman')\n",
    "    matplotlib.rcParams['font.family'] = 'Times New Roman'\n",
    "    matplotlib.rcParams['axes.labelweight'] = 'bold'\n",
    "    font_prop = FontProperties(family='Times New Roman', size=10)\n",
    "\n",
    "    metrics = ['RMSE', 'ubRMSE', 'R', 'Bias']\n",
    "\n",
    "    data_model = aggregate_data(all_data, time_point)\n",
    "    ax1 = ax  # RMSE柱状图\n",
    "    ax2 = ax.twinx()  # ubRMSE柱状图\n",
    "    ax3 = ax.twinx()  # R散点图\n",
    "    ax4 = ax.twinx()  # Bias散点图\n",
    "\n",
    "    x = np.arange(len(data_model.keys()))\n",
    "\n",
    "    x1 = [i - width/2 for i in x]\n",
    "    x2 = [i + width/2 for i in x]\n",
    "    xx = list(data_model.keys())\n",
    "\n",
    "\n",
    "    # 计算指标\n",
    "    rmse_values = [data_model[model]['RMSE'].mean() for model in data_model]\n",
    "    ubrmse_values = [data_model[model]['ubRMSE'].mean() for model in data_model]\n",
    "    r_values = [data_model[model]['R'].mean() for model in data_model]\n",
    "    bias_values = [data_model[model]['Bias'].mean() for model in data_model]\n",
    "\n",
    "    rmse_errors = [(data_model[model]['RMSE'].quantile(0.75) - data_model[model]['RMSE'].quantile(0.25)) for model in data_model]\n",
    "    ubrmse_errors = [(data_model[model]['ubRMSE'].quantile(0.75) - data_model[model]['ubRMSE'].quantile(0.25)) for model in data_model]\n",
    "\n",
    "    # 前三索引\n",
    "    top3_rmse_indices = sorted(range(len(rmse_values)), key=lambda i: rmse_values[i])[:3]\n",
    "    top3_ubrmse_indices = sorted(range(len(ubrmse_values)), key=lambda i: ubrmse_values[i])[:3]\n",
    "    top3_r_indices = sorted(range(len(r_values)), key=lambda i: r_values[i], reverse=True)[:3]\n",
    "    top3_bias_indices = sorted(range(len(bias_values)), key=lambda i: abs(bias_values[i]))[:3]\n",
    "\n",
    "    # 绘制RMSE柱状图以及误差线\n",
    "    colors_rmse = [adjust_color(start_color_rmse, 1 - Gradation_degree * i) for i in range(len(data_model.keys()))]\n",
    "    for i,(value, error_value, pos) in enumerate(zip(rmse_values, rmse_errors, x1)):\n",
    "        if i not in top3_rmse_indices:\n",
    "            ax1.bar(pos, value, yerr=error_value, align='center', alpha=0.8, ecolor='black', capsize=2, width=width, color=colors_rmse[i], label='RMSE', linewidth=0, error_kw={'lw': 1},zorder=11)\n",
    "            ax1.scatter(pos, value, color='black', edgecolors='black', linewidths=0,s=10, alpha=1,zorder=15)\n",
    "\n",
    "    colors_ubrmse = [adjust_color(start_color_ubrmse, 1 - Gradation_degree * i) for i in range(len(data_model.keys()))]\n",
    "    for i,(value, error_value, pos) in enumerate(zip(ubrmse_values, ubrmse_errors, x2)):\n",
    "        if i not in top3_ubrmse_indices:\n",
    "            ax2.bar(pos, value, yerr=error_value, align='center', alpha=0.8, ecolor='black', capsize=2, width=width, color=colors_ubrmse[i], label='ubRMSE', linewidth=0, error_kw={'lw': 1},zorder=11)\n",
    "            ax2.scatter(pos, value, color='black', edgecolors='black', linewidths=0,s=10, alpha=1,zorder=15)\n",
    "\n",
    "    # 绘制R和Bias散点图\n",
    "    ax3.plot(x1, r_values, label='R', linestyle='--', color='#bc6c25', linewidth=1.5, marker='X', markeredgecolor='lightgrey', markersize=10,alpha=0.5,zorder=10)\n",
    "    ax4.plot(x2, bias_values, '--', label='Bias',color='purple', linewidth=1.5, marker='P',markeredgecolor='grey', markersize=10,alpha=0.5,zorder=10) \n",
    "\n",
    "    # 绘制前三\n",
    "    for idx in top3_rmse_indices:\n",
    "        ax1.bar(x1[idx], rmse_values[idx], yerr=rmse_errors[idx], align='center', alpha=0.8, ecolor=colors_th[top3_rmse_indices.index(idx)], capsize=2, width=width, color=colors_third[top3_rmse_indices.index(idx)], label='RMSE' if idx == top3_rmse_indices[0] else \"\", linewidth=0, error_kw={'lw': 1, 'zorder': 13}, zorder=12)\n",
    "        ax1.scatter(x1[idx], rmse_values[idx], color='black', edgecolors='black', linewidths=0,s=10, alpha=1,zorder=15)\n",
    "    for idx in top3_ubrmse_indices:\n",
    "        ax2.bar(x2[idx], ubrmse_values[idx], yerr=ubrmse_errors[idx], align='center', alpha=0.8, ecolor=colors_th[top3_ubrmse_indices.index(idx)], capsize=2, width=width, color=colors_third[top3_ubrmse_indices.index(idx)], label='ubRMSE' if idx == top3_ubrmse_indices[0] else \"\", linewidth=0, error_kw={'lw': 1, 'zorder': 13}, zorder=12)\n",
    "        ax2.scatter(x2[idx], ubrmse_values[idx], color='black', edgecolors='black', linewidths=0,s=10, alpha=1,zorder=15)\n",
    "    for idx in top3_r_indices:\n",
    "        ax3.scatter(x1[idx], r_values[idx], linewidths=1.5, marker='X', color=colors_third[top3_r_indices.index(idx)], edgecolor='lightgrey', alpha=1, s=120, zorder=5)\n",
    "    for idx in top3_bias_indices:\n",
    "        ax4.scatter(x2[idx], bias_values[idx], linewidths=1.5, marker='P', color=colors_third[top3_bias_indices.index(idx)], edgecolor='grey', alpha=1, s=120, zorder=5)\n",
    "\n",
    "    grey_len = np.zeros(len(data_model.keys()))+0.3\n",
    "    ax1.bar(x, grey_len, width=width*2, color='lightgray', alpha=0.2, zorder=0)\n",
    "\n",
    "    labelsize = 10\n",
    "    pad_x = 0\n",
    "    pad_y = 4\n",
    "    for ax in [ax1, ax2, ax3, ax4]:\n",
    "        ax.grid(False)\n",
    "        if left_spines and ax in[ax1, ax2]:\n",
    "            ax.spines['left'].set_visible(True)\n",
    "            ax.spines['left'].set_linewidth(1.5)\n",
    "            ax.spines['left'].set_color('black')\n",
    "            ax.tick_params(axis='y', which='both', colors='black', length=3, width=1.5, labelsize=labelsize, direction='out', pad=pad_y, left=True, labelleft=True)\n",
    "        else:\n",
    "            ax.spines['left'].set_visible(False)\n",
    "            ax.tick_params(axis='y', which='both', colors='black', length=3, width=1.5, labelsize=labelsize, direction='out', pad=pad_y, left=False, labelleft=False)\n",
    "\n",
    "        if right_spines and ax in[ax3, ax4]:\n",
    "            ax.spines['right'].set_visible(True)\n",
    "            ax.spines['right'].set_linewidth(1.5)\n",
    "            ax.spines['right'].set_color('black')\n",
    "            ax.tick_params(axis='y', which='both', colors='black', length=3, width=1.5, labelsize=labelsize, direction='out', pad=pad_y, right=True, labelright=True)\n",
    "        else:\n",
    "            ax.spines['right'].set_visible(False)\n",
    "            ax.tick_params(axis='y', which='both', right=False, labelright=False)\n",
    "\n",
    "        if bottom_spines:\n",
    "            ax.spines['bottom'].set_visible(True)\n",
    "            ax.spines['bottom'].set_linewidth(1.5)\n",
    "            ax.spines['bottom'].set_color('black')\n",
    "            ax.tick_params(axis='x', which='both', colors='black', length=3, width=1.5, labelsize=labelsize, direction='out', pad=pad_x, bottom=True, labelbottom=True)\n",
    "        else:\n",
    "            ax.spines['bottom'].set_visible(False)\n",
    "            ax.tick_params(axis='x', which='both', colors='black', length=3, width=1.5, labelsize=labelsize, direction='out', pad=pad_x, bottom=False, labelbottom=False)\n",
    "\n",
    "        if top_spines:\n",
    "            ax.spines['top'].set_visible(True)\n",
    "            ax.spines['top'].set_linewidth(1.5)\n",
    "            ax.spines['top'].set_color('black')\n",
    "            ax.tick_params(axis='x', which='both', colors='black', length=3, width=1.5, labelsize=labelsize, direction='out', pad=pad_x, top=True, labeltop=False)\n",
    "        else:\n",
    "            ax.spines['top'].set_visible(False)\n",
    "            ax.tick_params(axis='x', which='both', colors='black', length=3, width=1.5, labelsize=labelsize, direction='out', pad=pad_x, top=False, labeltop=False)\n",
    "        if ax == ax1:\n",
    "            ax.spines['left'].set_position(('outward', 0))\n",
    "        if ax == ax2:\n",
    "            ax.spines['left'].set_position(('outward', 15))\n",
    "        if ax == ax3:\n",
    "            ax.spines['right'].set_position(('axes', 1.0))\n",
    "        if ax == ax4:\n",
    "            ax.spines['right'].set_position(('outward', 15))\n",
    "        # 刻度调节\n",
    "        plt.setp(ax.get_yticklabels(), rotation=90,ha=\"center\", rotation_mode=\"anchor\")\n",
    "        # 刻度值格式\n",
    "        ax.yaxis.set_major_formatter(FuncFormatter(custom_formatter))\n",
    "\n",
    "    # 设置x轴名\n",
    "    ax1.set_xticks(x)\n",
    "    ax1.set_xticklabels(xx, fontproperties=font_prop)\n",
    "    plt.setp(ax1.get_xticklabels(), rotation=90,ha=\"center\")\n",
    "\n",
    "    # 设置多轴范围\n",
    "    rmse_min, rmse_max = float('inf'), 0\n",
    "    ubrmse_min, ubrmse_max = float('inf'), 0\n",
    "    r_min, r_max = float('inf'), 0\n",
    "    bias_min, bias_max = float('inf'), 0\n",
    "    data_model = aggregate_data(all_data)\n",
    "    for model, data in data_model.items():\n",
    "        rmse_min = min(rmse_min, data['RMSE'].min())\n",
    "        rmse_max = max(rmse_max, data['RMSE'].max())\n",
    "        ubrmse_min = min(ubrmse_min, data['ubRMSE'].min())\n",
    "        ubrmse_max = max(ubrmse_max, data['ubRMSE'].max())\n",
    "        r_min = min(r_min, data['R'].min())\n",
    "        r_max = max(r_max, data['R'].max())\n",
    "        bias_min = min(bias_min, data['Bias'].min())\n",
    "        bias_max = max(bias_max, data['Bias'].max())\n",
    "    '''\n",
    "    0.02054428651076537 0.2816327750885776\n",
    "    0.01710960187069204 0.1287595993351982\n",
    "    -0.4854811460027492 0.9460179483656507\n",
    "    -0.2324438218623832 0.2723893704606125\n",
    "    down_det = 0.1\n",
    "    ax1.set_ylim(rmse_min - 0.05 * (rmse_max - rmse_min), rmse_max + 0.05 * (rmse_max - rmse_min))\n",
    "    ax2.set_ylim(ubrmse_min - 0.05 * (ubrmse_max - ubrmse_min), ubrmse_max + 0.05 * (ubrmse_max - ubrmse_min))\n",
    "    ax3.set_ylim(r_min - 0.05 * (r_max - r_min), r_max + 0.05 * (r_max - r_min))\n",
    "    ax4.set_ylim(bias_min - 0.05 * (bias_max - bias_min), bias_max + 0.05 * (bias_max - bias_min))\n",
    "    '''\n",
    "    # 绘制文本\n",
    "    hour = int(time_point) % 24\n",
    "    am_pm = \"AM\" if hour < 12 else \"PM\"\n",
    "    hour_formatted = hour if 0 < hour <= 12 else hour - 12\n",
    "    if hour == 0:  # 处理午夜是0点的情况\n",
    "        hour_formatted = 12\n",
    "    prefix = \"(\" + chr(ord('a') + time_points.index(time_point)) + \")\"\n",
    "    time_str = f'{prefix} {hour_formatted}:00 {am_pm}'\n",
    "    \n",
    "\n",
    "    if down:\n",
    "        ax1.set_ylim(0.3, 0.01)\n",
    "        ax2.set_ylim(0.16, 0.01)\n",
    "        ax3.set_ylim(0.75, 0)\n",
    "        ax4.set_ylim(0.15, -0.15)\n",
    "        ax.text(0.03, 0.08, time_str, transform=ax.transAxes, fontsize=14, fontname='Times New Roman', verticalalignment='top')\n",
    "    else:\n",
    "        ax1.set_ylim(0.01, 0.3)\n",
    "        ax2.set_ylim(0.01, 0.16)\n",
    "        ax3.set_ylim(0, 0.75)\n",
    "        ax4.set_ylim(-0.15, 0.15)\n",
    "        ax.text(0.03, 0.999, time_str, transform=ax.transAxes, fontsize=14, fontname='Times New Roman', verticalalignment='top')\n",
    "    ax1.yaxis.set_major_locator(MaxNLocator(prune='lower', nbins=5))\n",
    "    ax2.yaxis.set_major_locator(MaxNLocator(nbins=5))\n",
    "    ax3.yaxis.set_major_locator(MaxNLocator(nbins=5))\n",
    "    ax4.yaxis.set_major_locator(MaxNLocator(prune='lower', nbins=5))\n",
    "\n",
    "    # 控制刻度线上下边界缝隙距离\n",
    "    x_min, x_max = ax1.get_xlim()\n",
    "    padding = (x_max - x_min) * -0.01\n",
    "    ax1.set_xlim(x_min - padding, x_max + padding)\n",
    "\n",
    "    ax1.grid(True, color='lightgray', linestyle='--', alpha=0.5, which='both', axis='x', zorder=0.5)\n",
    "\n",
    "create_subplots(all_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (绘图) V2.0 柱状折线图 4*2 \n",
    "这个版本中仅使用KP-ST、RF、LSTM、TP模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "from matplotlib.colors import Normalize\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from matplotlib.collections import PolyCollection\n",
    "import matplotlib.colors as mcolors\n",
    "from scipy.stats import gaussian_kde\n",
    "from matplotlib.colors import to_rgba\n",
    "from matplotlib.font_manager import FontProperties\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.lines as mlines\n",
    "import colorsys\n",
    "\n",
    "def aggregate_data(all_data, time_point=None):\n",
    "    data_for_plot = {}\n",
    "    for model in all_data:\n",
    "        combined_df = pd.DataFrame()  # 聚合network和time\n",
    "        time_points = [time_point] if time_point else all_data[model].keys()\n",
    "        for tp in time_points:\n",
    "            for network in all_data[model][tp]:\n",
    "                df = all_data[model][tp][network]\n",
    "                if not df.empty:\n",
    "                    combined_df = pd.concat([combined_df, df], ignore_index=True)\n",
    "        if not combined_df.empty:\n",
    "            data_for_plot[model] = combined_df\n",
    "    return data_for_plot\n",
    "def custom_formatter(x, pos):\n",
    "    if x == 0:\n",
    "        return \"0\"\n",
    "    else:\n",
    "        str_x = \"{:.15g}\".format(x)\n",
    "        if 'e' in str_x:\n",
    "            base, exponent = str_x.split('e')\n",
    "            base = base.rstrip('0').rstrip('.')\n",
    "            exponent = exponent.replace('+', '').lstrip('0')\n",
    "            if exponent:\n",
    "                return f\"{base}e{exponent}\"\n",
    "            else:\n",
    "                return base\n",
    "        else:\n",
    "            return str_x\n",
    "def adjust_color(color, brightness_factor):\n",
    "    rgb = mcolors.hex2color(color)\n",
    "    hls = colorsys.rgb_to_hls(*rgb)\n",
    "    new_hls = (hls[0], max(0, min(1, brightness_factor * hls[1])), hls[2])\n",
    "    # 将调整后的HLS颜色转换回RGB，然后转换为HEX\n",
    "    new_rgb = colorsys.hls_to_rgb(*new_hls)\n",
    "    return mcolors.to_hex(new_rgb)\n",
    "def create_subplots(all_data):\n",
    "    # 创建图形对象,绘制顺序依赖于fig定义顺序\n",
    "    legend_font = FontProperties(family='Times New Roman', size=12)\n",
    "    time_points = ['0', '3', '6', '9', '12', '15', '18', '21']\n",
    "    colors_third = ['#590d22', '#ff4d6d', '#ffccd5']\n",
    "    colors_th = ['#590d22', '#c9184a', '#ff8fa3']\n",
    "    colors_gradient = ['#caf0f8', '#e9f5db']\n",
    "    fig = plt.figure(figsize=(24, 24))\n",
    "\n",
    "    ratio_h = 6.7 # 竖直比例 3\n",
    "    ratio_v = 5.1 # 水平比例\n",
    "\n",
    "    absolute_dis_h = 0.7/ratio_h # 中央子图绝对大小（竖直）0.7\n",
    "    absolute_dis_v = 0.5/ratio_v # 中央子图绝对大小（水平）\n",
    "\n",
    "    edge_h = 0.3803 # 竖直边缘间隙\n",
    "    edge_v = 0.304 # 水平边缘间隙\n",
    "\n",
    "    mid_h = (1-edge_h*2-absolute_dis_h*2)/1 # 竖直间隙,可以通过竖直边缘间隙调节\n",
    "    mid_v = (1-edge_v*2-absolute_dis_v*4)/3 # 水平间隙……\n",
    "\n",
    "    right_offset_first = 0 # 第一层向右偏移量\n",
    "    right_offset_four = 0.05 # 第一层向右偏移量\n",
    "\n",
    "    # 计算第一层图起点:上层子图  左起点、下起点、宽度比例、高度比例 4*2\n",
    "    ax1 = fig.add_axes([edge_v + right_offset_first, 1 - edge_h - absolute_dis_h, absolute_dis_v, absolute_dis_h])\n",
    "    ax2 = fig.add_axes([edge_v + right_offset_first + absolute_dis_v + mid_v, 1 - edge_h - absolute_dis_h, absolute_dis_v, absolute_dis_h])\n",
    "    ax3 = fig.add_axes([edge_v + right_offset_first + 2*(absolute_dis_v + mid_v), 1 - edge_h - absolute_dis_h, absolute_dis_v, absolute_dis_h])\n",
    "    ax4 = fig.add_axes([edge_v + right_offset_first + 3*(absolute_dis_v + mid_v), 1 - edge_h - absolute_dis_h, absolute_dis_v, absolute_dis_h])\n",
    "\n",
    "    ax5 = fig.add_axes([edge_v + right_offset_first, 1 - edge_h - 2*absolute_dis_h - mid_h, absolute_dis_v, absolute_dis_h])\n",
    "    ax6 = fig.add_axes([edge_v + right_offset_first + 1*(absolute_dis_v + mid_v), 1 - edge_h - 2*absolute_dis_h - mid_h, absolute_dis_v, absolute_dis_h])\n",
    "    ax7 = fig.add_axes([edge_v + right_offset_first + 2*(absolute_dis_v + mid_v), 1 - edge_h - 2*absolute_dis_h - mid_h, absolute_dis_v, absolute_dis_h])\n",
    "    ax8 = fig.add_axes([edge_v + right_offset_first + 3*(absolute_dis_v + mid_v), 1 - edge_h - 2*absolute_dis_h - mid_h, absolute_dis_v, absolute_dis_h])\n",
    "\n",
    "    draw_subplot(ax1, all_data, time_point=time_points[0],top_spines=False,right_spines=False,bottom_spines=True,left_spines=True,colors_third = colors_third, start_color_rmse = colors_gradient[0], start_color_ubrmse =  colors_gradient[1],colors_th=colors_th)\n",
    "    draw_subplot(ax2, all_data, time_point=time_points[1],top_spines=False,right_spines=False,bottom_spines=True,left_spines=False,colors_third = colors_third, start_color_rmse = colors_gradient[0], start_color_ubrmse =  colors_gradient[1],colors_th=colors_th)\n",
    "    draw_subplot(ax3, all_data, time_point=time_points[2],top_spines=False,right_spines=False,bottom_spines=True,left_spines=False,colors_third = colors_third, start_color_rmse = colors_gradient[0], start_color_ubrmse =  colors_gradient[1],colors_th=colors_th)\n",
    "    draw_subplot(ax4, all_data, time_point=time_points[3],top_spines=False,right_spines=True,bottom_spines=True,left_spines=False,colors_third = colors_third, start_color_rmse = colors_gradient[0], start_color_ubrmse =  colors_gradient[1],colors_th=colors_th)\n",
    "\n",
    "    draw_subplot(ax5, all_data, time_point=time_points[4],top_spines=True,right_spines=False,bottom_spines=False,left_spines=True,down=True,colors_third = colors_third, start_color_rmse = colors_gradient[0], start_color_ubrmse =  colors_gradient[1],colors_th=colors_th)\n",
    "    draw_subplot(ax6, all_data, time_point=time_points[5],top_spines=True,right_spines=False,bottom_spines=False,left_spines=False,down=True,colors_third = colors_third, start_color_rmse = colors_gradient[0], start_color_ubrmse =  colors_gradient[1],colors_th=colors_th)\n",
    "    draw_subplot(ax7, all_data, time_point=time_points[6],top_spines=True,right_spines=False,bottom_spines=False,left_spines=False,down=True,colors_third = colors_third, start_color_rmse = colors_gradient[0], start_color_ubrmse =  colors_gradient[1],colors_th=colors_th)\n",
    "    draw_subplot(ax8, all_data, time_point=time_points[7],top_spines=True,right_spines=True,bottom_spines=False,left_spines=False,down=True,colors_third = colors_third, start_color_rmse = colors_gradient[0], start_color_ubrmse =  colors_gradient[1],colors_th=colors_th)\n",
    "\n",
    "    colors_rmse = [adjust_color(colors_gradient[0], 1 - 0.02 * i) for i in range(11)]\n",
    "    colors_ubrmse = [adjust_color(colors_gradient[1], 1 - 0.02 * i) for i in range(11)]\n",
    "    gradient_patch_rmse = mpatches.Patch(color=colors_rmse[5], label='RMSE')\n",
    "    gradient_patch_ubrmse = mpatches.Patch(color=colors_ubrmse[5], label='ubRMSE')\n",
    "    line_r = mlines.Line2D([], [], color='#bc6c25', linestyle='--', marker='X', markeredgecolor='lightgrey', markersize=10, label='R', alpha=0.5)\n",
    "    line_bias = mlines.Line2D([], [], color='purple', linestyle='--', marker='P', markeredgecolor='grey', markersize=10, label='Bias', alpha=0.5)\n",
    "    first_legend_handles = [gradient_patch_rmse, gradient_patch_ubrmse, line_r, line_bias]\n",
    "    plt.figlegend(handles=first_legend_handles, labels=[h.get_label() for h in first_legend_handles], \n",
    "                  loc='upper center', ncol=2, bbox_to_anchor=(0.455, edge_h+0.005), fancybox=False, \n",
    "                  shadow=False, frameon=False, prop=legend_font,\n",
    "                  borderpad=1, labelspacing=0.8,handletextpad=0.5, handlelength=1.5)\n",
    "\n",
    "    colors_top3 = ['#590d22', '#ff4d6d', '#ffccd5']  # 用于条形图\n",
    "    color_X = ['#8A3C23', '#DD5C48', '#D8B08D']  # 用于'X'标记的散点图\n",
    "    color_PLUS = ['#750266', '#B82B76', '#BF65AA']  # 用于'+'标记的散点图\n",
    "    top3_labels = ['1st', '2nd', '3rd']\n",
    "    top3_handles = []\n",
    "    for i in range(3):\n",
    "        bar_handle = mpatches.Patch(color=colors_top3[i], label=' ',alpha =0.8)\n",
    "        scatter_handle_X = mlines.Line2D([], [], color=color_X[i], marker='X', linestyle='None', markersize=10, markeredgecolor='grey', label=top3_labels[i], linewidth=2)  # 实际标签\n",
    "        scatter_handle_PLUS = mlines.Line2D([], [], color=color_PLUS[i], marker='P', linestyle='None', markersize=10, markeredgecolor='grey', label=' ', linewidth=2)\n",
    "        top3_handles.extend([bar_handle, scatter_handle_X, scatter_handle_PLUS])\n",
    "\n",
    "    plt.figlegend(handles=top3_handles, labels=[h.get_label() for h in top3_handles], \n",
    "                  loc='upper center', ncol=3, bbox_to_anchor=(0.36-0.004, edge_h+0.005), \n",
    "                  fancybox=False, shadow=False, frameon=False, prop=legend_font,\n",
    "                  borderpad=1, labelspacing=0,handletextpad=0.5, handlelength=1.5)\n",
    "    \n",
    "    colors = ['#bc6c25', 'purple']\n",
    "    line_styles = [':', '-.']\n",
    "    labels = ['R Mean', 'Zero Line']\n",
    "    handles = [\n",
    "        mlines.Line2D([], [], color=color, linestyle=line_style,\n",
    "                    linewidth=1.5, label=label)\n",
    "        for color, line_style, label in zip(colors, line_styles, labels)\n",
    "    ]\n",
    "    plt.figlegend(handles=handles, loc='upper center', ncol=1, bbox_to_anchor=(0.525, edge_h+0.005),\n",
    "                  fancybox=False, \n",
    "                  shadow=False, frameon=False, prop=legend_font,\n",
    "                  borderpad=1, labelspacing=0.8,handletextpad=0.5, handlelength=1.5)\n",
    "    \n",
    "    offset = 0.006\n",
    "    fig.text(0.297+offset, 0.493, 'RMSE', ha='left', size=12, fontproperties=legend_font, rotation=90)\n",
    "    fig.text(0.2868+offset, 0.488, 'ubRMSE', ha='left', size=12, fontproperties=legend_font, rotation=90)\n",
    "    fig.text(0.6985-offset, 0.488, 'PearsonR', ha='left', size=12, fontproperties=legend_font, rotation=90)\n",
    "    fig.text(0.7065-offset, 0.496, 'bias', ha='left', size=12, fontproperties=legend_font, rotation=90)\n",
    "\n",
    "    fig.savefig(r\"C:\\Users\\Administrator\\Desktop\\Draw\\Original_eval.jpeg\", dpi=1000, format='jpeg', bbox_inches='tight')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def draw_subplot(ax, all_data, time_point=None, width=0.41, start_color_rmse = '#caf0f8', start_color_ubrmse = '#e9f5db',Gradation_degree = 0.02,\n",
    "                 top_spines=True, right_spines=True, bottom_spines=True, left_spines=True,colors_third = None,colors_th=None,\n",
    "                 down=False):\n",
    "\n",
    "    sns.set(style=\"whitegrid\", font='Times New Roman')\n",
    "    matplotlib.rcParams['font.family'] = 'Times New Roman'\n",
    "    matplotlib.rcParams['axes.labelweight'] = 'bold'\n",
    "    font_prop = FontProperties(family='Times New Roman', size=10)\n",
    "\n",
    "    metrics = ['RMSE', 'ubRMSE', 'R', 'Bias']\n",
    "\n",
    "    data_model = aggregate_data(all_data, time_point)\n",
    "    ax1 = ax  # RMSE柱状图\n",
    "    ax3 = ax.twinx()  # R散点图\n",
    "    ax4 = ax.twinx()  # Bias散点图\n",
    "    ax2 = ax.twinx()  # ubRMSE柱状图\n",
    "\n",
    "\n",
    "    x = np.arange(len(data_model.keys()))\n",
    "\n",
    "    x1 = [i - width/2 for i in x]\n",
    "    x2 = [i + width/2 for i in x]\n",
    "    xx = list(data_model.keys())\n",
    "\n",
    "\n",
    "    # 计算指标\n",
    "    rmse_values = [data_model[model]['RMSE'].mean() for model in data_model]\n",
    "    ubrmse_values = [data_model[model]['ubRMSE'].mean() for model in data_model]\n",
    "    r_values = [data_model[model]['R'].mean() for model in data_model]\n",
    "    bias_values = [data_model[model]['Bias'].mean() for model in data_model]\n",
    "\n",
    "    rmse_errors = [(data_model[model]['RMSE'].quantile(0.75) - data_model[model]['RMSE'].quantile(0.25)) for model in data_model]\n",
    "    ubrmse_errors = [(data_model[model]['ubRMSE'].quantile(0.75) - data_model[model]['ubRMSE'].quantile(0.25)) for model in data_model]\n",
    "\n",
    "    # 前三索引\n",
    "    top3_rmse_indices = sorted(range(len(rmse_values)), key=lambda i: rmse_values[i])[:3]\n",
    "    top3_ubrmse_indices = sorted(range(len(ubrmse_values)), key=lambda i: ubrmse_values[i])[:3]\n",
    "    top3_r_indices = sorted(range(len(r_values)), key=lambda i: r_values[i], reverse=True)[:3]\n",
    "    top3_bias_indices = sorted(range(len(bias_values)), key=lambda i: abs(bias_values[i]))[:3]\n",
    "\n",
    "    # 绘制RMSE柱状图以及误差线\n",
    "    colors_rmse = [adjust_color(start_color_rmse, 1 - Gradation_degree * i) for i in range(len(data_model.keys()))]\n",
    "    for i,(value, error_value, pos) in enumerate(zip(rmse_values, rmse_errors, x1)):\n",
    "        if i not in top3_rmse_indices:\n",
    "            ax1.bar(pos, value, yerr=error_value, align='center', alpha=0.8, ecolor='black', capsize=2, width=width, color=colors_rmse[i], label='RMSE', linewidth=0, error_kw={'lw': 1,'zorder':16},zorder=16)\n",
    "            ax1.scatter(pos, value, color='black', edgecolors='black', linewidths=0,s=10, alpha=1,zorder=16)\n",
    "\n",
    "    colors_ubrmse = [adjust_color(start_color_ubrmse, 1 - Gradation_degree * i) for i in range(len(data_model.keys()))]\n",
    "    for i,(value, error_value, pos) in enumerate(zip(ubrmse_values, ubrmse_errors, x2)):\n",
    "        if i not in top3_ubrmse_indices:\n",
    "            ax2.bar(pos, value, yerr=error_value, align='center', alpha=0.8, ecolor='black', capsize=2, width=width, color=colors_ubrmse[i], label='ubRMSE', linewidth=0, error_kw={'lw': 1,'zorder':16},zorder=16)\n",
    "            ax2.scatter(pos, value, color='black', edgecolors='black', linewidths=0,s=10, alpha=1,zorder=16)\n",
    "\n",
    "    # 绘制R和Bias散点图\n",
    "    ax3.plot(x1, r_values, label='R', linestyle='--', color='#bc6c25', linewidth=1.5, marker='X', markeredgecolor='lightgrey', markersize=10,alpha=0.5,zorder=10)\n",
    "    ax4.plot(x2, bias_values, '--', label='Bias',color='purple', linewidth=1.5, marker='P',markeredgecolor='grey', markersize=10,alpha=0.5,zorder=10) \n",
    "    \n",
    "    # 绘制均值线和0线\n",
    "    r_mean = np.mean(r_values)\n",
    "    ax3.axhline(y=r_mean, label='R mean', linestyle=':', color='#bc6c25', linewidth=1.5, alpha=0.6, zorder=10)\n",
    "    ax4.axhline(y=0, label='Bias mean', linestyle='-.', color='purple', linewidth=1, alpha=0.5, zorder=10)\n",
    "\n",
    "    # 绘制前三\n",
    "    for idx in top3_r_indices:\n",
    "        ax3.scatter(x1[idx], r_values[idx], linewidths=1.5, marker='X', color=colors_third[top3_r_indices.index(idx)], edgecolor='lightgrey', alpha=1, s=120, zorder=5)\n",
    "    for idx in top3_bias_indices:\n",
    "        ax4.scatter(x2[idx], bias_values[idx], linewidths=1.5, marker='P', color=colors_third[top3_bias_indices.index(idx)], edgecolor='grey', alpha=1, s=120, zorder=5)\n",
    "\n",
    "    for idx in top3_rmse_indices:\n",
    "        ax1.bar(x1[idx], rmse_values[idx], yerr=rmse_errors[idx], align='center', alpha=0.8, ecolor=colors_th[top3_rmse_indices.index(idx)], capsize=2, width=width, color=colors_third[top3_rmse_indices.index(idx)], label='RMSE' if idx == top3_rmse_indices[0] else \"\", linewidth=0, error_kw={'lw': 1, 'zorder': 16}, zorder=12)\n",
    "        ax1.scatter(x1[idx], rmse_values[idx], color='black', edgecolors='black', linewidths=0,s=10, alpha=1,zorder=15)\n",
    "    for idx in top3_ubrmse_indices:\n",
    "        ax2.bar(x2[idx], ubrmse_values[idx], yerr=ubrmse_errors[idx], align='center', alpha=0.8, ecolor=colors_th[top3_ubrmse_indices.index(idx)], capsize=2, width=width, color=colors_third[top3_ubrmse_indices.index(idx)], label='ubRMSE' if idx == top3_ubrmse_indices[0] else \"\", linewidth=0, error_kw={'lw': 1, 'zorder': 16}, zorder=12)\n",
    "        ax2.scatter(x2[idx], ubrmse_values[idx], color='black', edgecolors='black', linewidths=0,s=10, alpha=1,zorder=15)\n",
    "\n",
    "    grey_len = np.zeros(len(data_model.keys()))+0.3\n",
    "    ax1.bar(x, grey_len, width=width*2, color='lightgray', alpha=0.2, zorder=0)\n",
    "\n",
    "    labelsize = 10\n",
    "    pad_x = 0\n",
    "    pad_y = 4\n",
    "    for ax in [ax1, ax2, ax3, ax4]:\n",
    "        ax.grid(False)\n",
    "        if left_spines and ax in[ax1, ax2]:\n",
    "            ax.spines['left'].set_visible(True)\n",
    "            ax.spines['left'].set_linewidth(1.5)\n",
    "            ax.spines['left'].set_color('black')\n",
    "            color = '#49E249' if ax in [ax1] else '#48A0CB'\n",
    "            ax.tick_params(axis='y', which='both', colors=color, length=3, width=1.5, labelsize=labelsize, direction='out', pad=pad_y, left=True, labelleft=True)\n",
    "        else:\n",
    "            ax.spines['left'].set_visible(False)\n",
    "            ax.tick_params(axis='y', which='both', colors='black', length=3, width=1.5, labelsize=labelsize, direction='out', pad=pad_y, left=False, labelleft=False)\n",
    "\n",
    "        if right_spines and ax in[ax3, ax4]:\n",
    "            ax.spines['right'].set_visible(True)\n",
    "            ax.spines['right'].set_linewidth(1.5)\n",
    "            ax.spines['right'].set_color('black')\n",
    "            color = '#bc6c25' if ax in [ax3] else 'purple'\n",
    "            ax.tick_params(axis='y', which='both', colors=color, length=3, width=1.5, labelsize=labelsize, direction='out', pad=pad_y, right=True, labelright=True)\n",
    "        else:\n",
    "            ax.spines['right'].set_visible(False)\n",
    "            ax.tick_params(axis='y', which='both', right=False, labelright=False)\n",
    "\n",
    "        if bottom_spines:\n",
    "            ax.spines['bottom'].set_visible(True)\n",
    "            ax.spines['bottom'].set_linewidth(1.5)\n",
    "            ax.spines['bottom'].set_color('black')\n",
    "            ax.tick_params(axis='x', which='both', colors='black', length=3, width=1.5, labelsize=labelsize, direction='out', pad=pad_x, bottom=True, labelbottom=True)\n",
    "        else:\n",
    "            ax.spines['bottom'].set_visible(False)\n",
    "            ax.tick_params(axis='x', which='both', colors='black', length=3, width=1.5, labelsize=labelsize, direction='out', pad=pad_x, bottom=False, labelbottom=False)\n",
    "\n",
    "        if top_spines:\n",
    "            ax.spines['top'].set_visible(True)\n",
    "            ax.spines['top'].set_linewidth(1.5)\n",
    "            ax.spines['top'].set_color('black')\n",
    "            ax.tick_params(axis='x', which='both', colors='black', length=3, width=1.5, labelsize=labelsize, direction='out', pad=pad_x, top=True, labeltop=False)\n",
    "        else:\n",
    "            ax.spines['top'].set_visible(False)\n",
    "            ax.tick_params(axis='x', which='both', colors='black', length=3, width=1.5, labelsize=labelsize, direction='out', pad=pad_x, top=False, labeltop=False)\n",
    "        if ax == ax1:\n",
    "            ax.spines['left'].set_position(('outward', 0))\n",
    "        if ax == ax2:\n",
    "            ax.spines['left'].set_position(('outward', 15))\n",
    "        if ax == ax3:\n",
    "            ax.spines['right'].set_position(('axes', 1.0))\n",
    "        if ax == ax4:\n",
    "            ax.spines['right'].set_position(('outward', 15))\n",
    "        # 刻度调节\n",
    "        plt.setp(ax.get_yticklabels(), rotation=90,ha=\"center\", rotation_mode=\"anchor\")\n",
    "        # 刻度值格式\n",
    "        ax.yaxis.set_major_formatter(FuncFormatter(custom_formatter))\n",
    "\n",
    "    # 设置x轴名\n",
    "    ax1.set_xticks(x)\n",
    "    ax1.set_xticklabels(xx, fontproperties=font_prop)\n",
    "    if bottom_spines:\n",
    "        labels = ax1.get_xticklabels()\n",
    "        labels[0].set_fontsize(7)\n",
    "    plt.setp(ax1.get_xticklabels(), rotation=90,ha=\"center\")\n",
    "\n",
    "    # 设置多轴范围\n",
    "    rmse_min, rmse_max = float('inf'), 0\n",
    "    ubrmse_min, ubrmse_max = float('inf'), 0\n",
    "    r_min, r_max = float('inf'), 0\n",
    "    bias_min, bias_max = float('inf'), 0\n",
    "    data_model = aggregate_data(all_data)\n",
    "    for model, data in data_model.items():\n",
    "        rmse_min = min(rmse_min, data['RMSE'].min())\n",
    "        rmse_max = max(rmse_max, data['RMSE'].max())\n",
    "        ubrmse_min = min(ubrmse_min, data['ubRMSE'].min())\n",
    "        ubrmse_max = max(ubrmse_max, data['ubRMSE'].max())\n",
    "        r_min = min(r_min, data['R'].min())\n",
    "        r_max = max(r_max, data['R'].max())\n",
    "        bias_min = min(bias_min, data['Bias'].min())\n",
    "        bias_max = max(bias_max, data['Bias'].max())\n",
    "\n",
    "    '''\n",
    "    0.02054428651076537 0.2816327750885776\n",
    "    0.01710960187069204 0.1287595993351982\n",
    "    -0.4854811460027492 0.9460179483656507\n",
    "    -0.2324438218623832 0.2723893704606125\n",
    "    down_det = 0.1\n",
    "    ax1.set_ylim(rmse_min - 0.05 * (rmse_max - rmse_min), rmse_max + 0.05 * (rmse_max - rmse_min))\n",
    "    ax2.set_ylim(ubrmse_min - 0.05 * (ubrmse_max - ubrmse_min), ubrmse_max + 0.05 * (ubrmse_max - ubrmse_min))\n",
    "    ax3.set_ylim(r_min - 0.05 * (r_max - r_min), r_max + 0.05 * (r_max - r_min))\n",
    "    ax4.set_ylim(bias_min - 0.05 * (bias_max - bias_min), bias_max + 0.05 * (bias_max - bias_min))\n",
    "    '''\n",
    "\n",
    "    # 绘制文本\n",
    "    hour = int(time_point) % 24\n",
    "    am_pm = \"AM\" if hour < 12 else \"PM\"\n",
    "    hour_formatted = hour if 0 < hour <= 12 else hour - 12\n",
    "    if hour == 0:  # 处理午夜是0点的情况\n",
    "        hour_formatted = 12\n",
    "    prefix = \"(\" + chr(ord('a') + time_points.index(time_point)) + \")\"\n",
    "    time_str = f'{prefix} {hour_formatted}:00 {am_pm}'\n",
    "    \n",
    "    if down:\n",
    "        ax1.set_ylim(0.3, 0.01)\n",
    "        ax2.set_ylim(0.16, 0.01)\n",
    "        ax3.set_ylim(0.75, 0)\n",
    "        ax4.set_ylim(0.15, -0.30)\n",
    "        ax.text(0.03, 0.08, time_str, transform=ax.transAxes, fontsize=14, fontname='Times New Roman', verticalalignment='top')\n",
    "    else:\n",
    "        ax1.set_ylim(0.01, 0.3)\n",
    "        ax2.set_ylim(0.01, 0.16)\n",
    "        ax3.set_ylim(0, 0.75)\n",
    "        ax4.set_ylim(-0.30, 0.15)\n",
    "        ax.text(0.03, 0.999, time_str, transform=ax.transAxes, fontsize=14, fontname='Times New Roman', verticalalignment='top')\n",
    "    ax1.yaxis.set_major_locator(MaxNLocator(prune='lower', nbins=5))\n",
    "    ax2.yaxis.set_major_locator(MaxNLocator(nbins=5))\n",
    "    ax3.yaxis.set_major_locator(MaxNLocator(nbins=5))\n",
    "    ax4.yaxis.set_major_locator(MaxNLocator(prune='lower', nbins=5))\n",
    "\n",
    "    # 控制刻度线上下边界缝隙距离\n",
    "    x_min, x_max = ax1.get_xlim()\n",
    "    padding = (x_max - x_min) * -0.01\n",
    "    ax1.set_xlim(x_min - padding, x_max + padding)\n",
    "\n",
    "    ax1.grid(True, color='lightgray', linestyle='--', alpha=0.5, which='both', axis='x', zorder=0.5)\n",
    "\n",
    "create_subplots(all_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (绘图) V2.0 柱状折线图 4*2 \n",
    "这个版本中仅使用KP-ST、RF、LSTM、TP模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "from matplotlib.colors import Normalize\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from matplotlib.collections import PolyCollection\n",
    "import matplotlib.colors as mcolors\n",
    "from scipy.stats import gaussian_kde\n",
    "from matplotlib.colors import to_rgba\n",
    "from matplotlib.font_manager import FontProperties\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.lines as mlines\n",
    "import colorsys\n",
    "\n",
    "def aggregate_data(all_data, time_point=None):\n",
    "    data_for_plot = {}\n",
    "    for model in all_data:\n",
    "        combined_df = pd.DataFrame()  # 聚合network和time\n",
    "        time_points = [time_point] if time_point else all_data[model].keys()\n",
    "        for tp in time_points:\n",
    "            for network in all_data[model][tp]:\n",
    "                df = all_data[model][tp][network]\n",
    "                if not df.empty:\n",
    "                    combined_df = pd.concat([combined_df, df], ignore_index=True)\n",
    "        if not combined_df.empty:\n",
    "            data_for_plot[model] = combined_df\n",
    "    return data_for_plot\n",
    "def custom_formatter(x, pos):\n",
    "    if x == 0:\n",
    "        return \"0\"\n",
    "    else:\n",
    "        str_x = \"{:.15g}\".format(x)\n",
    "        if 'e' in str_x:\n",
    "            base, exponent = str_x.split('e')\n",
    "            base = base.rstrip('0').rstrip('.')\n",
    "            exponent = exponent.replace('+', '').lstrip('0')\n",
    "            if exponent:\n",
    "                return f\"{base}e{exponent}\"\n",
    "            else:\n",
    "                return base\n",
    "        else:\n",
    "            return str_x\n",
    "def adjust_color(color, brightness_factor):\n",
    "    rgb = mcolors.hex2color(color)\n",
    "    hls = colorsys.rgb_to_hls(*rgb)\n",
    "    new_hls = (hls[0], max(0, min(1, brightness_factor * hls[1])), hls[2])\n",
    "    # 将调整后的HLS颜色转换回RGB，然后转换为HEX\n",
    "    new_rgb = colorsys.hls_to_rgb(*new_hls)\n",
    "    return mcolors.to_hex(new_rgb)\n",
    "def create_subplots(all_data):\n",
    "    # 创建图形对象,绘制顺序依赖于fig定义顺序\n",
    "    legend_font = FontProperties(family='Times New Roman', size=12)\n",
    "    time_points = ['0', '3', '6', '9', '12', '15', '18', '21']\n",
    "    colors_third = ['#590d22', '#ff4d6d', '#ffccd5']\n",
    "    colors_th = ['#590d22', '#c9184a', '#ff8fa3']\n",
    "    colors_gradient = ['#caf0f8', '#e9f5db']\n",
    "    fig = plt.figure(figsize=(24, 24))\n",
    "\n",
    "    ratio_h = 6.7 # 竖直比例 3\n",
    "    ratio_v = 5.1 # 水平比例\n",
    "\n",
    "    absolute_dis_h = 0.7/ratio_h # 中央子图绝对大小（竖直）0.7\n",
    "    absolute_dis_v = 0.57/ratio_v # 中央子图绝对大小（水平）\n",
    "\n",
    "    edge_h = 0.3803 # 竖直边缘间隙\n",
    "    edge_v = 0.28 # 水平边缘间隙\n",
    "\n",
    "    mid_h = (1-edge_h*2-absolute_dis_h*2)/1 # 竖直间隙,可以通过竖直边缘间隙调节\n",
    "    mid_v = (1-edge_v*2-absolute_dis_v*4)/3 # 水平间隙……\n",
    "\n",
    "    right_offset_first = 0 # 第一层向右偏移量\n",
    "    right_offset_four = 0.05 # 第一层向右偏移量\n",
    "\n",
    "    # 计算第一层图起点:上层子图  左起点、下起点、宽度比例、高度比例 4*2\n",
    "    ax1 = fig.add_axes([edge_v + right_offset_first, 1 - edge_h - absolute_dis_h, absolute_dis_v, absolute_dis_h])\n",
    "    ax2 = fig.add_axes([edge_v + right_offset_first + absolute_dis_v + mid_v, 1 - edge_h - absolute_dis_h, absolute_dis_v, absolute_dis_h])\n",
    "    ax3 = fig.add_axes([edge_v + right_offset_first + 2*(absolute_dis_v + mid_v), 1 - edge_h - absolute_dis_h, absolute_dis_v, absolute_dis_h])\n",
    "    ax4 = fig.add_axes([edge_v + right_offset_first + 3*(absolute_dis_v + mid_v), 1 - edge_h - absolute_dis_h, absolute_dis_v, absolute_dis_h])\n",
    "\n",
    "    ax5 = fig.add_axes([edge_v + right_offset_first, 1 - edge_h - 2*absolute_dis_h - mid_h, absolute_dis_v, absolute_dis_h])\n",
    "    ax6 = fig.add_axes([edge_v + right_offset_first + 1*(absolute_dis_v + mid_v), 1 - edge_h - 2*absolute_dis_h - mid_h, absolute_dis_v, absolute_dis_h])\n",
    "    ax7 = fig.add_axes([edge_v + right_offset_first + 2*(absolute_dis_v + mid_v), 1 - edge_h - 2*absolute_dis_h - mid_h, absolute_dis_v, absolute_dis_h])\n",
    "    ax8 = fig.add_axes([edge_v + right_offset_first + 3*(absolute_dis_v + mid_v), 1 - edge_h - 2*absolute_dis_h - mid_h, absolute_dis_v, absolute_dis_h])\n",
    "\n",
    "    draw_subplot(ax1, all_data, time_point=time_points[0],top_spines=False,right_spines=False,bottom_spines=True,left_spines=True,colors_third = colors_third, start_color_rmse = colors_gradient[0], start_color_ubrmse =  colors_gradient[1],colors_th=colors_th)\n",
    "    draw_subplot(ax2, all_data, time_point=time_points[1],top_spines=False,right_spines=False,bottom_spines=True,left_spines=False,colors_third = colors_third, start_color_rmse = colors_gradient[0], start_color_ubrmse =  colors_gradient[1],colors_th=colors_th)\n",
    "    draw_subplot(ax3, all_data, time_point=time_points[2],top_spines=False,right_spines=False,bottom_spines=True,left_spines=False,colors_third = colors_third, start_color_rmse = colors_gradient[0], start_color_ubrmse =  colors_gradient[1],colors_th=colors_th)\n",
    "    draw_subplot(ax4, all_data, time_point=time_points[3],top_spines=False,right_spines=True,bottom_spines=True,left_spines=False,colors_third = colors_third, start_color_rmse = colors_gradient[0], start_color_ubrmse =  colors_gradient[1],colors_th=colors_th)\n",
    "\n",
    "    draw_subplot(ax5, all_data, time_point=time_points[4],top_spines=True,right_spines=False,bottom_spines=False,left_spines=True,down=True,colors_third = colors_third, start_color_rmse = colors_gradient[0], start_color_ubrmse =  colors_gradient[1],colors_th=colors_th)\n",
    "    draw_subplot(ax6, all_data, time_point=time_points[5],top_spines=True,right_spines=False,bottom_spines=False,left_spines=False,down=True,colors_third = colors_third, start_color_rmse = colors_gradient[0], start_color_ubrmse =  colors_gradient[1],colors_th=colors_th)\n",
    "    draw_subplot(ax7, all_data, time_point=time_points[6],top_spines=True,right_spines=False,bottom_spines=False,left_spines=False,down=True,colors_third = colors_third, start_color_rmse = colors_gradient[0], start_color_ubrmse =  colors_gradient[1],colors_th=colors_th)\n",
    "    draw_subplot(ax8, all_data, time_point=time_points[7],top_spines=True,right_spines=True,bottom_spines=False,left_spines=False,down=True,colors_third = colors_third, start_color_rmse = colors_gradient[0], start_color_ubrmse =  colors_gradient[1],colors_th=colors_th)\n",
    "\n",
    "    colors_rmse = [adjust_color(colors_gradient[0], 1 - 0.02 * i) for i in range(11)]\n",
    "    colors_ubrmse = [adjust_color(colors_gradient[1], 1 - 0.02 * i) for i in range(11)]\n",
    "    gradient_patch_rmse = mpatches.Patch(color=colors_rmse[5], label='RMSE')\n",
    "    gradient_patch_ubrmse = mpatches.Patch(color=colors_ubrmse[5], label='ubRMSE')\n",
    "    line_r = mlines.Line2D([], [], color='#bc6c25', linestyle='--', marker='X', markeredgecolor='lightgrey', markersize=10, label='R', alpha=0.5)\n",
    "    line_bias = mlines.Line2D([], [], color='purple', linestyle='--', marker='P', markeredgecolor='grey', markersize=10, label='Bias', alpha=0.5)\n",
    "    first_legend_handles = [gradient_patch_rmse, gradient_patch_ubrmse, line_r, line_bias]\n",
    "    plt.figlegend(handles=first_legend_handles, labels=[h.get_label() for h in first_legend_handles], \n",
    "                  loc='upper center', ncol=2, bbox_to_anchor=(0.455, edge_h+0.005), fancybox=False, \n",
    "                  shadow=False, frameon=False, prop=legend_font,\n",
    "                  borderpad=1, labelspacing=0.8,handletextpad=0.5, handlelength=1.5)\n",
    "\n",
    "    colors_top3 = ['#590d22', '#ff4d6d', '#ffccd5']  # 用于条形图\n",
    "    color_X = ['#8A3C23', '#DD5C48', '#D8B08D']  # 用于'X'标记的散点图\n",
    "    color_PLUS = ['#750266', '#B82B76', '#BF65AA']  # 用于'+'标记的散点图\n",
    "    top3_labels = ['1st', '2nd', '3rd']\n",
    "    top3_handles = []\n",
    "    for i in range(3):\n",
    "        bar_handle = mpatches.Patch(color=colors_top3[i], label=' ',alpha =0.8)\n",
    "        scatter_handle_X = mlines.Line2D([], [], color=color_X[i], marker='X', linestyle='None', markersize=10, markeredgecolor='grey', label=top3_labels[i], linewidth=2)  # 实际标签\n",
    "        scatter_handle_PLUS = mlines.Line2D([], [], color=color_PLUS[i], marker='P', linestyle='None', markersize=10, markeredgecolor='grey', label=' ', linewidth=2)\n",
    "        top3_handles.extend([bar_handle, scatter_handle_X, scatter_handle_PLUS])\n",
    "\n",
    "    plt.figlegend(handles=top3_handles, labels=[h.get_label() for h in top3_handles], \n",
    "                  loc='upper center', ncol=3, bbox_to_anchor=(0.36-0.004, edge_h+0.005), \n",
    "                  fancybox=False, shadow=False, frameon=False, prop=legend_font,\n",
    "                  borderpad=1, labelspacing=0,handletextpad=0.5, handlelength=1.5)\n",
    "    \n",
    "    colors = ['#bc6c25', 'purple']\n",
    "    line_styles = [':', '-.']\n",
    "    labels = ['R Mean', 'Zero Line']\n",
    "    handles = [\n",
    "        mlines.Line2D([], [], color=color, linestyle=line_style,\n",
    "                    linewidth=1.5, label=label)\n",
    "        for color, line_style, label in zip(colors, line_styles, labels)\n",
    "    ]\n",
    "    plt.figlegend(handles=handles, loc='upper center', ncol=1, bbox_to_anchor=(0.525, edge_h+0.005),\n",
    "                  fancybox=False, \n",
    "                  shadow=False, frameon=False, prop=legend_font,\n",
    "                  borderpad=1, labelspacing=0.8,handletextpad=0.5, handlelength=1.5)\n",
    "    \n",
    "    offset = 0.006\n",
    "    fig.text(0.271+offset, 0.493, 'RMSE', ha='left', size=12, fontproperties=legend_font, rotation=90)\n",
    "    fig.text(0.2608+offset, 0.488, 'ubRMSE', ha='left', size=12, fontproperties=legend_font, rotation=90)\n",
    "    fig.text(0.7245-offset, 0.488, 'PearsonR', ha='left', size=12, fontproperties=legend_font, rotation=90)\n",
    "    fig.text(0.7325-offset, 0.496, 'bias', ha='left', size=12, fontproperties=legend_font, rotation=90)\n",
    "\n",
    "    fig.savefig(r\"C:\\Users\\Administrator\\Desktop\\Draw\\Original_eval.jpeg\", dpi=1000, format='jpeg', bbox_inches='tight')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def draw_subplot(ax, all_data, time_point=None, width=0.41, start_color_rmse = '#caf0f8', start_color_ubrmse = '#e9f5db',Gradation_degree = 0.02,\n",
    "                 top_spines=True, right_spines=True, bottom_spines=True, left_spines=True,colors_third = None,colors_th=None,\n",
    "                 down=False):\n",
    "\n",
    "    sns.set(style=\"whitegrid\", font='Times New Roman')\n",
    "    matplotlib.rcParams['font.family'] = 'Times New Roman'\n",
    "    matplotlib.rcParams['axes.labelweight'] = 'bold'\n",
    "    font_prop = FontProperties(family='Times New Roman', size=10)\n",
    "\n",
    "    metrics = ['RMSE', 'ubRMSE', 'R', 'Bias']\n",
    "\n",
    "    data_model = aggregate_data(all_data, time_point)\n",
    "    ax1 = ax  # RMSE柱状图\n",
    "    ax3 = ax.twinx()  # R散点图\n",
    "    ax4 = ax.twinx()  # Bias散点图\n",
    "    ax2 = ax.twinx()  # ubRMSE柱状图\n",
    "\n",
    "\n",
    "    x = np.arange(len(data_model.keys()))\n",
    "\n",
    "    x1 = [i - width/2 for i in x]\n",
    "    x2 = [i + width/2 for i in x]\n",
    "    xx = list(data_model.keys())\n",
    "\n",
    "\n",
    "    # 计算指标\n",
    "    rmse_values = [data_model[model]['RMSE'].mean() for model in data_model]\n",
    "    ubrmse_values = [data_model[model]['ubRMSE'].mean() for model in data_model]\n",
    "    r_values = [data_model[model]['R'].mean() for model in data_model]\n",
    "    bias_values = [data_model[model]['Bias'].mean() for model in data_model]\n",
    "\n",
    "    rmse_errors = [(data_model[model]['RMSE'].quantile(0.75) - data_model[model]['RMSE'].quantile(0.25)) for model in data_model]\n",
    "    ubrmse_errors = [(data_model[model]['ubRMSE'].quantile(0.75) - data_model[model]['ubRMSE'].quantile(0.25)) for model in data_model]\n",
    "\n",
    "    # 前三索引\n",
    "    top3_rmse_indices = sorted(range(len(rmse_values)), key=lambda i: rmse_values[i])[:3]\n",
    "    top3_ubrmse_indices = sorted(range(len(ubrmse_values)), key=lambda i: ubrmse_values[i])[:3]\n",
    "    top3_r_indices = sorted(range(len(r_values)), key=lambda i: r_values[i], reverse=True)[:3]\n",
    "    top3_bias_indices = sorted(range(len(bias_values)), key=lambda i: abs(bias_values[i]))[:3]\n",
    "\n",
    "    # 绘制RMSE柱状图以及误差线\n",
    "    colors_rmse = [adjust_color(start_color_rmse, 1 - Gradation_degree * i) for i in range(len(data_model.keys()))]\n",
    "    for i,(value, error_value, pos) in enumerate(zip(rmse_values, rmse_errors, x1)):\n",
    "        if i not in top3_rmse_indices:\n",
    "            ax1.bar(pos, value, yerr=error_value, align='center', alpha=0.8, ecolor='black', capsize=2, width=width, color=colors_rmse[i], label='RMSE', linewidth=0, error_kw={'lw': 1,'zorder':16},zorder=16)\n",
    "            ax1.scatter(pos, value, color='black', edgecolors='black', linewidths=0,s=10, alpha=1,zorder=16)\n",
    "            \n",
    "    colors_ubrmse = [adjust_color(start_color_ubrmse, 1 - Gradation_degree * i) for i in range(len(data_model.keys()))]\n",
    "    for i,(value, error_value, pos) in enumerate(zip(ubrmse_values, ubrmse_errors, x2)):\n",
    "        if i not in top3_ubrmse_indices:\n",
    "            ax2.bar(pos, value, yerr=error_value, align='center', alpha=0.8, ecolor='black', capsize=2, width=width, color=colors_ubrmse[i], label='ubRMSE', linewidth=0, error_kw={'lw': 1,'zorder':16},zorder=16)\n",
    "            ax2.scatter(pos, value, color='black', edgecolors='black', linewidths=0,s=10, alpha=1,zorder=16)\n",
    "\n",
    "    # 绘制R和Bias散点图\n",
    "    ax3.plot(x1, r_values, label='R', linestyle='--', color='#bc6c25', linewidth=1.5, marker='X', markeredgecolor='lightgrey', markersize=10,alpha=0.5,zorder=10)\n",
    "    ax4.plot(x2, bias_values, '--', label='Bias',color='purple', linewidth=1.5, marker='P',markeredgecolor='grey', markersize=10,alpha=0.5,zorder=10) \n",
    "    \n",
    "    # 绘制均值线和0线\n",
    "    r_mean = np.mean(r_values)\n",
    "    ax3.axhline(y=r_mean, label='R mean', linestyle=':', color='#bc6c25', linewidth=1.5, alpha=0.6, zorder=10)\n",
    "    ax4.axhline(y=0, label='Bias mean', linestyle='-.', color='purple', linewidth=1, alpha=0.5, zorder=10)\n",
    "\n",
    "    # 绘制前三\n",
    "    for idx in top3_r_indices:\n",
    "        ax3.scatter(x1[idx], r_values[idx], linewidths=1.5, marker='X', color=colors_third[top3_r_indices.index(idx)], edgecolor='lightgrey', alpha=1, s=120, zorder=5)\n",
    "    for idx in top3_bias_indices:\n",
    "        ax4.scatter(x2[idx], bias_values[idx], linewidths=1.5, marker='P', color=colors_third[top3_bias_indices.index(idx)], edgecolor='grey', alpha=1, s=120, zorder=5)\n",
    "\n",
    "    for idx in top3_rmse_indices:\n",
    "        ax1.bar(x1[idx], rmse_values[idx], yerr=rmse_errors[idx], align='center', alpha=0.8, ecolor=colors_th[top3_rmse_indices.index(idx)], capsize=2, width=width, color=colors_third[top3_rmse_indices.index(idx)], label='RMSE' if idx == top3_rmse_indices[0] else \"\", linewidth=0, error_kw={'lw': 1, 'zorder': 16}, zorder=12)\n",
    "        ax1.scatter(x1[idx], rmse_values[idx], color='black', edgecolors='black', linewidths=0,s=10, alpha=1,zorder=15)\n",
    "    for idx in top3_ubrmse_indices:\n",
    "        ax2.bar(x2[idx], ubrmse_values[idx], yerr=ubrmse_errors[idx], align='center', alpha=0.8, ecolor=colors_th[top3_ubrmse_indices.index(idx)], capsize=2, width=width, color=colors_third[top3_ubrmse_indices.index(idx)], label='ubRMSE' if idx == top3_ubrmse_indices[0] else \"\", linewidth=0, error_kw={'lw': 1, 'zorder': 16}, zorder=12)\n",
    "        ax2.scatter(x2[idx], ubrmse_values[idx], color='black', edgecolors='black', linewidths=0,s=10, alpha=1,zorder=15)\n",
    "\n",
    "    grey_len = np.zeros(len(data_model.keys()))+0.3\n",
    "    ax1.bar(x, grey_len, width=width*2, color='lightgray', alpha=0.2, zorder=0)\n",
    "\n",
    "    labelsize = 10\n",
    "    pad_x = 0\n",
    "    pad_y = 4\n",
    "    for ax in [ax1, ax2, ax3, ax4]:\n",
    "        ax.grid(False)\n",
    "        if left_spines and ax in[ax1, ax2]:\n",
    "            ax.spines['left'].set_visible(True)\n",
    "            ax.spines['left'].set_linewidth(1.5)\n",
    "            ax.spines['left'].set_color('black')\n",
    "            color = '#49E249' if ax in [ax1] else '#48A0CB'\n",
    "            ax.tick_params(axis='y', which='both', colors=color, length=3, width=1.5, labelsize=labelsize, direction='out', pad=pad_y, left=True, labelleft=True)\n",
    "        else:\n",
    "            ax.spines['left'].set_visible(False)\n",
    "            ax.tick_params(axis='y', which='both', colors='black', length=3, width=1.5, labelsize=labelsize, direction='out', pad=pad_y, left=False, labelleft=False)\n",
    "\n",
    "        if right_spines and ax in[ax3, ax4]:\n",
    "            ax.spines['right'].set_visible(True)\n",
    "            ax.spines['right'].set_linewidth(1.5)\n",
    "            ax.spines['right'].set_color('black')\n",
    "            color = '#bc6c25' if ax in [ax3] else 'purple'\n",
    "            ax.tick_params(axis='y', which='both', colors=color, length=3, width=1.5, labelsize=labelsize, direction='out', pad=pad_y, right=True, labelright=True)\n",
    "        else:\n",
    "            ax.spines['right'].set_visible(False)\n",
    "            ax.tick_params(axis='y', which='both', right=False, labelright=False)\n",
    "\n",
    "        if bottom_spines:\n",
    "            ax.spines['bottom'].set_visible(True)\n",
    "            ax.spines['bottom'].set_linewidth(1.5)\n",
    "            ax.spines['bottom'].set_color('black')\n",
    "            ax.tick_params(axis='x', which='both', colors='black', length=3, width=1.5, labelsize=labelsize, direction='out', pad=pad_x, bottom=True, labelbottom=True)\n",
    "        else:\n",
    "            ax.spines['bottom'].set_visible(False)\n",
    "            ax.tick_params(axis='x', which='both', colors='black', length=3, width=1.5, labelsize=labelsize, direction='out', pad=pad_x, bottom=False, labelbottom=False)\n",
    "\n",
    "        if top_spines:\n",
    "            ax.spines['top'].set_visible(True)\n",
    "            ax.spines['top'].set_linewidth(1.5)\n",
    "            ax.spines['top'].set_color('black')\n",
    "            ax.tick_params(axis='x', which='both', colors='black', length=3, width=1.5, labelsize=labelsize, direction='out', pad=pad_x, top=True, labeltop=False)\n",
    "        else:\n",
    "            ax.spines['top'].set_visible(False)\n",
    "            ax.tick_params(axis='x', which='both', colors='black', length=3, width=1.5, labelsize=labelsize, direction='out', pad=pad_x, top=False, labeltop=False)\n",
    "        if ax == ax1:\n",
    "            ax.spines['left'].set_position(('outward', 0))\n",
    "        if ax == ax2:\n",
    "            ax.spines['left'].set_position(('outward', 15))\n",
    "        if ax == ax3:\n",
    "            ax.spines['right'].set_position(('axes', 1.0))\n",
    "        if ax == ax4:\n",
    "            ax.spines['right'].set_position(('outward', 15))\n",
    "        # 刻度调节\n",
    "        plt.setp(ax.get_yticklabels(), rotation=90,ha=\"center\", rotation_mode=\"anchor\")\n",
    "        # 刻度值格式\n",
    "        ax.yaxis.set_major_formatter(FuncFormatter(custom_formatter))\n",
    "\n",
    "    # 设置x轴名\n",
    "    ax1.set_xticks(x)\n",
    "    ax1.set_xticklabels(xx, fontproperties=font_prop)\n",
    "    if bottom_spines:\n",
    "        labels = ax1.get_xticklabels()\n",
    "        labels[0].set_fontsize(7)\n",
    "        labels[1].set_fontsize(7)\n",
    "    plt.setp(ax1.get_xticklabels(), rotation=90,ha=\"center\")\n",
    "\n",
    "    # 设置多轴范围\n",
    "    rmse_min, rmse_max = float('inf'), 0\n",
    "    ubrmse_min, ubrmse_max = float('inf'), 0\n",
    "    r_min, r_max = float('inf'), 0\n",
    "    bias_min, bias_max = float('inf'), 0\n",
    "    data_model = aggregate_data(all_data)\n",
    "    for model, data in data_model.items():\n",
    "        rmse_min = min(rmse_min, data['RMSE'].min())\n",
    "        rmse_max = max(rmse_max, data['RMSE'].max())\n",
    "        ubrmse_min = min(ubrmse_min, data['ubRMSE'].min())\n",
    "        ubrmse_max = max(ubrmse_max, data['ubRMSE'].max())\n",
    "        r_min = min(r_min, data['R'].min())\n",
    "        r_max = max(r_max, data['R'].max())\n",
    "        bias_min = min(bias_min, data['Bias'].min())\n",
    "        bias_max = max(bias_max, data['Bias'].max())\n",
    "\n",
    "    '''\n",
    "    0.02054428651076537 0.2816327750885776\n",
    "    0.01710960187069204 0.1287595993351982\n",
    "    -0.4854811460027492 0.9460179483656507\n",
    "    -0.2324438218623832 0.2723893704606125\n",
    "    down_det = 0.1\n",
    "    ax1.set_ylim(rmse_min - 0.05 * (rmse_max - rmse_min), rmse_max + 0.05 * (rmse_max - rmse_min))\n",
    "    ax2.set_ylim(ubrmse_min - 0.05 * (ubrmse_max - ubrmse_min), ubrmse_max + 0.05 * (ubrmse_max - ubrmse_min))\n",
    "    ax3.set_ylim(r_min - 0.05 * (r_max - r_min), r_max + 0.05 * (r_max - r_min))\n",
    "    ax4.set_ylim(bias_min - 0.05 * (bias_max - bias_min), bias_max + 0.05 * (bias_max - bias_min))\n",
    "    '''\n",
    "\n",
    "    # 绘制文本\n",
    "    hour = int(time_point) % 24\n",
    "    am_pm = \"AM\" if hour < 12 else \"PM\"\n",
    "    hour_formatted = hour if 0 < hour <= 12 else hour - 12\n",
    "    if hour == 0:  # 处理午夜是0点的情况\n",
    "        hour_formatted = 12\n",
    "    prefix = \"(\" + chr(ord('a') + time_points.index(time_point)) + \")\"\n",
    "    time_str = f'{prefix} {hour_formatted}:00 {am_pm}'\n",
    "    # 绘制RMSE柱状图以及误差线\n",
    "    offset_y1 = 0.03 if bottom_spines is True else 0.025\n",
    "    offset_y2 = 0.0202 if bottom_spines is True else 0.0176\n",
    "    for i,(value, error_value, pos) in enumerate(zip(rmse_values, rmse_errors, x1)):\n",
    "        ax1.text(pos, offset_y1, f'{value:.3f}', ha='center', va='center', fontproperties=font_prop, size=8, color='black', zorder=17, rotation=90, rotation_mode='anchor')\n",
    "    for i,(value, error_value, pos) in enumerate(zip(ubrmse_values, ubrmse_errors, x2)):\n",
    "        ax2.text(pos, offset_y2, f'{value:.3f}', ha='center', va='center', fontproperties=font_prop, size=8, color='black', zorder=17, rotation=90, rotation_mode='anchor')\n",
    "    \n",
    "    offset_y3 = 0.065 if bottom_spines is True else 0.06\n",
    "    offset_y4 = 0.035 if bottom_spines is True else 0.032\n",
    "    for i, (value, pos) in enumerate(zip(r_values, x1)):\n",
    "        ax3.text(pos, value + offset_y3, f'{value:.3f}', ha='center', va='center', fontproperties=font_prop, size=8, color='#bc6c25', zorder=17, rotation=90, rotation_mode='anchor')\n",
    "    for i, (value, pos) in enumerate(zip(bias_values, x2)):\n",
    "        ax4.text(pos, value + offset_y4, f'{value:.3f}', ha='center', va='center', fontproperties=font_prop, size=8, color='purple', zorder=17, rotation=90, rotation_mode='anchor')\n",
    "\n",
    "    if down:\n",
    "        ax1.set_ylim(0.3, 0.01)\n",
    "        ax2.set_ylim(0.16, 0.01)\n",
    "        ax3.set_ylim(0.8, 0)\n",
    "        ax4.set_ylim(0.15, -0.25)\n",
    "        ax.text(0.03, 0.08, time_str, transform=ax.transAxes, fontsize=14, fontname='Times New Roman', verticalalignment='top')\n",
    "    else:\n",
    "        ax1.set_ylim(0.01, 0.3)\n",
    "        ax2.set_ylim(0.01, 0.16)\n",
    "        ax3.set_ylim(0, 0.8)\n",
    "        ax4.set_ylim(-0.25, 0.15)\n",
    "        ax.text(0.03, 0.999, time_str, transform=ax.transAxes, fontsize=14, fontname='Times New Roman', verticalalignment='top')\n",
    "    ax1.yaxis.set_major_locator(MaxNLocator(prune='lower', nbins=5))\n",
    "    ax2.yaxis.set_major_locator(MaxNLocator(nbins=5))\n",
    "    ax3.yaxis.set_major_locator(MaxNLocator(nbins=5))\n",
    "    ax4.yaxis.set_major_locator(MaxNLocator(prune='lower', nbins=5))\n",
    "\n",
    "    # 控制刻度线上下边界缝隙距离\n",
    "    x_min, x_max = ax1.get_xlim()\n",
    "    padding = (x_max - x_min) * -0.01\n",
    "    ax1.set_xlim(x_min - padding, x_max + padding)\n",
    "\n",
    "    ax1.grid(True, color='lightgray', linestyle='--', alpha=0.5, which='both', axis='x', zorder=0.5)\n",
    "\n",
    "create_subplots(all_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (数据处理) 原点网络均值计算(时间动态图)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(r\"C:\\Users\\Administrator\\Desktop\\code\\ST-Conv\")\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from Original_Point_Data.tool.origin_utils import OriginUtils\n",
    "from tool.utils import Util\n",
    "\n",
    "def calculate_network_avg(modes, info_path, find_mode='0.1'):\n",
    "    # 初始化存储最终均值数据的DataFrame\n",
    "    avg_data_df = pd.DataFrame()\n",
    "\n",
    "    for mode in modes:\n",
    "        ali_list = OriginUtils.find_grid_points(mode, info_path, find_mode=find_mode)\n",
    "        point_list = [[item[2], item[1], item[0], item[3]] for item in ali_list]  # 保证顺序：纬度，经度，站点名，文件路径\n",
    "        point_list.sort(key=lambda x: (-x[0], x[1]))  # 根据纬度降序，经度升序排序\n",
    "\n",
    "        point_data_frames = []\n",
    "\n",
    "        for lat, lon, point_name, file_path in point_list:\n",
    "            # 读取并处理每个站点的数据\n",
    "            point_data = pd.read_csv(file_path, header=0)  # 读取CSV文件\n",
    "            point_data['Time'] = pd.to_datetime(point_data.iloc[:, 0])  # 转换时间列为DateTime对象\n",
    "            point_data.set_index('Time', inplace=True)  # 设置时间列为索引\n",
    "            point_data_frames.append(point_data['SM'])  # 假设感兴趣的数据在'SM'列\n",
    "\n",
    "        # 时间对齐并计算均值\n",
    "        if point_data_frames:\n",
    "            aligned_data = pd.concat(point_data_frames, axis=1, join='outer').mean(axis=1)\n",
    "            avg_data_df[mode] = aligned_data\n",
    "\n",
    "    return avg_data_df\n",
    "\n",
    "def main(args):\n",
    "    Original_Point_path_argu = Util.load_config(args.Original_Point_path_config)\n",
    "    path_argu = Util.load_config(args.path_config_path)\n",
    "    return Original_Point_path_argu, path_argu\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--Original_Point_path_config', type=str, default='../Original_Point_Data/config/path_config.yaml')\n",
    "    parser.add_argument('--path_config_path', type=str, default='../config/path_config.yaml')\n",
    "    args = parser.parse_known_args()[0]\n",
    "    Original_Point_path_argu, path_argu = main(args)\n",
    "    modes = ['Shiquanhe', 'Ali', 'Maqu', 'Naqu', 'CTP']  # 所有模式\n",
    "    cal_time = '3'\n",
    "    csv_path = Original_Point_path_argu['info']\n",
    "    avg_data_df = calculate_network_avg(modes, csv_path, find_mode='0.1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (数据处理) 重建数据均值计算(时间动态图)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(r\"C:\\Users\\Administrator\\Desktop\\code\\ST-Conv\")\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from Original_Point_Data.tool.origin_utils import OriginUtils\n",
    "from tool.utils import Util\n",
    "\n",
    "def calculate_reconstruction_avg(modes, info_path, reconstruction_paths, find_mode='0.1'):\n",
    "    all_networks_recon_avg = {}  # 所有网络和所有重建方法的结果\n",
    "\n",
    "    for mode in modes:\n",
    "        print(f\"Processing network: {mode}\")\n",
    "        network_avg_data = {}  # 当前网络的重建数据均值\n",
    "\n",
    "        for method, recon_path in reconstruction_paths.items():\n",
    "            print(f\"Processing reconstruction method: {method}\")\n",
    "            ali_list = OriginUtils.find_grid_points(mode, info_path, find_mode=find_mode)\n",
    "            point_list = [[item[2], item[1], item[0]] for item in ali_list]\n",
    "            point_list.sort(key=lambda x: (-x[0], x[1]))\n",
    "\n",
    "            recon_data_frames = []\n",
    "            recon_data = np.load(recon_path)\n",
    "\n",
    "            for lat, lon, point_name in point_list:\n",
    "                _, (lat_ind, lon_ind) = OriginUtils.find_grid_coordinates(lon, lat, find_mode=find_mode)\n",
    "                point_recon_data = recon_data[:, lat_ind, lon_ind]\n",
    "                time_index = pd.date_range(start='2016-01-01 03:00:00', periods=len(point_recon_data), freq='3H')\n",
    "                point_recon_df = pd.DataFrame(point_recon_data, index=time_index, columns=[point_name])\n",
    "                recon_data_frames.append(point_recon_df)\n",
    "\n",
    "            # 时间对齐并计算均值\n",
    "            if recon_data_frames:\n",
    "                aligned_recon_data = pd.concat(recon_data_frames, axis=1, join='outer').mean(axis=1)\n",
    "                network_avg_data[method] = aligned_recon_data\n",
    "\n",
    "        # 将当前网络的结果添加到所有网络的结果中\n",
    "        all_networks_recon_avg[mode] = network_avg_data\n",
    "\n",
    "    return all_networks_recon_avg\n",
    "\n",
    "\n",
    "def main(args):\n",
    "    Original_Point_path_argu = Util.load_config(args.Original_Point_path_config)\n",
    "    path_argu = Util.load_config(args.path_config_path)\n",
    "    return Original_Point_path_argu, path_argu\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--Original_Point_path_config', type=str, default='../Original_Point_Data/config/path_config.yaml')\n",
    "    parser.add_argument('--path_config_path', type=str, default='../config/path_config.yaml')\n",
    "    args = parser.parse_known_args()[0]\n",
    "    Original_Point_path_argu, path_argu = main(args)\n",
    "    \n",
    "    start_time = '2016-01-01 03:00:00'\n",
    "    modes = ['Shiquanhe', 'Ali', 'Maqu', 'Naqu', 'CTP']\n",
    "    cal_time = '3'\n",
    "    csv_path = Original_Point_path_argu['info']\n",
    "    reconstruction_paths = {\n",
    "        'PSC-FENet-SP': r\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\SP_PSC-FENet.npy\",\n",
    "        'PSC-FENet-ST': r\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\ST_PSC-FENet.npy\",\n",
    "        'LSTM': r\"D:\\Data_Store\\Dataset\\Reconstruction\\LSTM\\LSTM1.npy\",\n",
    "        'RF': r\"D:\\Data_Store\\Dataset\\Reconstruction\\RF\\RF_reconstructed_data.npy\",\n",
    "        'ResNet-SP': r\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\SP_ResNet.npy\",\n",
    "        'ResNet-ST': r\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\ST_ResNet.npy\",\n",
    "        'UNet-SP': r\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\SP_UNet.npy\",\n",
    "        'UNet-ST': r\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\ST_UNet.npy\",\n",
    "        'VIT-SP': r\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\SP_ViT.npy\",\n",
    "        'VIT-ST': r\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\ST_ViT.npy\",\n",
    "        'Temporal': r\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\Temporal.npy\"\n",
    "    }\n",
    "\n",
    "    #selected_model_names = ['PSC-FENet-ST','PSC-FENet-SP','ResNet-ST', 'UNet-ST', 'VIT-ST',  'LSTM', 'RF', 'Temporal']\n",
    "    selected_model_names = ['PSC-FENet-ST','PSC-FENet-SP', 'LSTM', 'RF', 'Temporal']\n",
    "    selected_reconstruction_paths = {name: paths for name, paths in reconstruction_paths.items() if name in selected_model_names}\n",
    "    all_networks_recon_avg = calculate_reconstruction_avg(modes, csv_path , selected_reconstruction_paths, find_mode='0.1')\n",
    "    np.save(r\"D:\\Data_Store\\Dataset\\Results\\all_networks_recon_avg.npy\",all_networks_recon_avg)\n",
    "    \n",
    "    Original_paths = {'Original': r\"D:\\Data_Store\\Dataset\\Reconstruction\\37SM\\soil_moisture.npy\"}\n",
    "    all_networks_origin_avg = calculate_reconstruction_avg(modes, csv_path , Original_paths, find_mode='0.37')\n",
    "    np.save(r\"D:\\Data_Store\\Dataset\\Results\\all_networks_origin_avg.npy\",all_networks_origin_avg)\n",
    "\n",
    "    Precipitation_paths = {'Precipitation': r\"D:\\\\Data_Store\\\\Dataset\\\\Reconstruction\\\\37SM\\\\hourlyPrecipRateGC.npy\"}\n",
    "    all_networks_Precipitation_avg = calculate_reconstruction_avg(modes, csv_path , Precipitation_paths, find_mode='0.1')\n",
    "    np.save(r\"D:\\Data_Store\\Dataset\\Results\\all_networks_Precipitation_avg.npy\",all_networks_Precipitation_avg)\n",
    "    for network, methods_data in all_networks_Precipitation_avg.items():\n",
    "        print(f\"Network: {network}\")\n",
    "        for method, data in methods_data.items():\n",
    "            print(f\"Method: {method}\")\n",
    "            print(data.head())\n",
    "            print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (数据处理) 拼接原位、重建和卫星均值数据(时间动态图)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_all_networks_data(modes, avg_data_df, all_networks_recon_avg, all_networks_origin_avg, all_networks_Precipitation_avg):\n",
    "    all_networks_data = {}\n",
    "    end_time = pd.Timestamp('2018-12-31 21:00:00')\n",
    "\n",
    "    for network in modes:\n",
    "        print(f\"Processing data for network: {network}\")\n",
    "        network_data_frames = []\n",
    "\n",
    "        # 添加原始站点数据\n",
    "        if network in avg_data_df:\n",
    "            filtered_avg_data = avg_data_df[network][avg_data_df[network].index <= end_time]\n",
    "            network_data_frames.append(filtered_avg_data.rename('Original_Data'))\n",
    "\n",
    "        # 添加原始卫星数据\n",
    "        if network in all_networks_origin_avg and 'Original' in all_networks_origin_avg[network]:\n",
    "            filtered_origin_data = all_networks_origin_avg[network]['Original'][all_networks_origin_avg[network]['Original'].index <= end_time]\n",
    "            network_data_frames.append(filtered_origin_data.rename('Satellite_Data'))\n",
    "\n",
    "        # 添加各种重建方法的数据\n",
    "        for method, recon_data in all_networks_recon_avg[network].items():\n",
    "            filtered_recon_data = recon_data[recon_data.index <= end_time]\n",
    "            network_data_frames.append(filtered_recon_data.rename(method))\n",
    "\n",
    "        # 添加降水量数据\n",
    "        if network in all_networks_Precipitation_avg:\n",
    "            filtered_precip_data = all_networks_Precipitation_avg[network]['Precipitation'][all_networks_Precipitation_avg[network]['Precipitation'].index <= end_time]\n",
    "            network_data_frames.append(filtered_precip_data.rename('Precipitation'))\n",
    "\n",
    "        # 合并所有数据帧，确保使用外连接以保留所有时间点\n",
    "        if network_data_frames:\n",
    "            network_combined_data = pd.concat(network_data_frames, axis=1, join='outer')\n",
    "            network_combined_data = network_combined_data[network_combined_data.index <= end_time]\n",
    "            all_networks_data[network] = network_combined_data\n",
    "\n",
    "    return all_networks_data\n",
    "\n",
    "def filter_data_for_spring_summer(all_networks_data):\n",
    "    filtered_networks_data = {}\n",
    "\n",
    "    for network, data in all_networks_data.items():\n",
    "        filtered_data = data[(data.index.month >= 4) & (data.index.month <= 10)]\n",
    "        filtered_networks_data[network] = filtered_data\n",
    "\n",
    "    return filtered_networks_data\n",
    "\n",
    "all_networks_data = calculate_all_networks_data(modes, avg_data_df, all_networks_recon_avg, all_networks_origin_avg, all_networks_Precipitation_avg)\n",
    "filtered_all_networks_data = filter_data_for_spring_summer(all_networks_data)\n",
    "\n",
    "for network, data in filtered_all_networks_data.items():\n",
    "    print(f\"Data for network {network}:\")\n",
    "    print(data.head())\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (绘图) V1.0 时间动态图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib.font_manager import FontProperties\n",
    "from matplotlib.collections import PathCollection\n",
    "\n",
    "def create_and_draw_subplots(filtered_all_networks_data):\n",
    "    # 创建图形对象,绘制顺序依赖于fig定义顺序\n",
    "    fig = plt.figure(figsize=(36, 30))\n",
    "\n",
    "    total_height = 0.8\n",
    "    single_height = total_height / 5\n",
    "    gap = 0.018\n",
    "\n",
    "    ax1 = fig.add_axes([0.05, 0.05 + 4 * (single_height + gap), 0.9, single_height]) # 上1\n",
    "    ax2 = fig.add_axes([0.05, 0.05 + 3 * (single_height + gap), 0.9, single_height]) # 上2\n",
    "    ax3 = fig.add_axes([0.05, 0.05 + 2 * (single_height + gap), 0.9, single_height]) # 上3\n",
    "    ax4 = fig.add_axes([0.05, 0.05 + 1 * (single_height + gap), 0.9, single_height]) # 上4\n",
    "    ax5 = fig.add_axes([0.05, 0.05, 0.9, single_height]) # 上5\n",
    "\n",
    "    # 绘制网络数据\n",
    "    plot_network_data(ax1, filtered_all_networks_data['Shiquanhe'], '(a)Shiquanhe', axhline_low=0.15, axhline_up=0.3, use_legend=True, use_xlabel=False)\n",
    "    plot_network_data(ax2, filtered_all_networks_data['Ali'], '(b)Ali', axhline_low=0.15, axhline_up=0.3, use_legend=False, use_xlabel=False)\n",
    "    plot_network_data(ax3, filtered_all_networks_data['Maqu'], '(c)Maqu', axhline_low=0.2, axhline_up=0.4, use_legend=False, use_xlabel=False)\n",
    "    plot_network_data(ax4, filtered_all_networks_data['Naqu'], '(d)Naqu', axhline_low=0.2, axhline_up=0.4, use_legend=False, use_xlabel=False)\n",
    "    plot_network_data(ax5, filtered_all_networks_data['CTP'], '(e)CTP', axhline_low=0.2, axhline_up=0.4, use_legend=False, use_xlabel=True)\n",
    "\n",
    "    plt.show()\n",
    "def plot_network_data(ax, network_data, network_name,axhline_low=0.2,axhline_up=0.4, use_legend=True, use_xlabel=True):\n",
    "    font = FontProperties()\n",
    "    font.set_family('Times New Roman')\n",
    "    font.set_size(24)\n",
    "    plt.rcParams['font.family'] = 'Times New Roman'\n",
    "    plt.rcParams['font.size'] = 24\n",
    "\n",
    "    dates = network_data.index\n",
    "    dates_shift = np.arange(len(dates))\n",
    "    plot_settings = {\n",
    "        'Original_Data': {'color': 'red', 'label': 'Original_Data', 'linewidth': 2, 'linestyle': '-', 'zorder': 10, 'alpha': 1.0},\n",
    "        'Satellite_Data': {'color': 'black', 'label': 'Satellite_Data', 'linewidth': 2, 'linestyle': '-', 'zorder': 9, 'alpha': 1.0},\n",
    "        'PSC-FENet-ST': {'color': '#a40062', 'label': 'PSC-FENet-ST', 's': 20, 'marker': 'D', 'edgecolors': '#a40062', 'facecolors': 'none', 'zorder': 8, 'alpha': 0.8, 'linewidths': 1},\n",
    "        'RF': {'color': '#0b6909', 'label': 'Random Forest', 's': 20, 'marker': '+', 'edgecolors': '#0b6909', 'facecolors': '#0b6909', 'zorder': 7, 'alpha': 0.85, 'linewidths': 1.5},\n",
    "        'ResNet-ST': {'color': '#005fb8', 'label': 'ResNet-ST', 's': 20, 'marker': '^', 'edgecolors': '#005fb8', 'facecolors': 'none', 'zorder': 6, 'alpha': 0.75, 'linewidths': 1},\n",
    "        'UNet-ST': {'color': '#70e000', 'label': 'UNet-ST', 's': 20, 'marker': 'v', 'edgecolors': '#70e000', 'facecolors': 'none', 'zorder': 5, 'alpha': 0.70, 'linewidths': 1},\n",
    "        'LSTM': {'color': '#694e41', 'label': 'LSTM', 's': 20, 'marker': 'X', 'edgecolors': '#694e41', 'facecolors': 'none', 'zorder': 4, 'alpha': 0.65, 'linewidths': 1},\n",
    "        'VIT-ST': {'color': '#faa307', 'label': 'VIT-ST', 's': 20, 'marker': '*', 'edgecolors': '#faa307', 'facecolors': 'none', 'zorder': 3, 'alpha': 0.60, 'linewidths': 1},\n",
    "        'Temporal': {'color': '#ff8fa3', 'label': 'Temporal', 's': 20, 'marker': 'h', 'edgecolors': '#ff8fa3', 'facecolors': 'none', 'zorder': 2, 'alpha': 0.55, 'linewidths': 1},\n",
    "    }\n",
    "\n",
    "    ax2 = ax.twinx()\n",
    "    for year in range(dates[0].year, dates[-1].year + 1):\n",
    "        start_date = pd.Timestamp(year=year, month=4, day=1)\n",
    "        start_idx = np.where(dates == start_date)[0][0]\n",
    "        # 绘制虚线\n",
    "        ax.axvline(x=dates_shift[start_idx], color='grey', linestyle='--', linewidth=1)\n",
    "\n",
    "    if 'Original_Data' in network_data.columns:\n",
    "        ax.plot(dates_shift, network_data['Original_Data'], **plot_settings['Original_Data'])\n",
    "    \n",
    "    if 'Satellite_Data' in network_data.columns:\n",
    "        os_data = network_data['Satellite_Data'].dropna()\n",
    "        os_dates_shift = dates_shift[network_data.index.isin(os_data.index)]\n",
    "        ax.plot(os_dates_shift, os_data, **plot_settings['Satellite_Data'])\n",
    "\n",
    "    for column in network_data.columns:\n",
    "        if column not in ['Original_Data', 'Satellite_Data', 'Precipitation'] and column in plot_settings:\n",
    "            ax.scatter(dates_shift, network_data[column], **plot_settings[column])\n",
    "\n",
    "    if 'Precipitation' in network_data.columns:\n",
    "        # 绘制柱状图\n",
    "        ax2.patch.set_alpha(0.0)\n",
    "        ax2.bar(dates_shift, network_data['Precipitation'], width=0.02, color='#47a0ff', edgecolor='#47a0ff', label='Precipitation', alpha=0.8, zorder=10)\n",
    "        ax2.set_ylabel('Precipitation (mm)', fontproperties=font, color='#47a0ff')\n",
    "\n",
    "    # 设置时间刻度\n",
    "    ax.set_xticks(dates_shift[::480])\n",
    "    ax.set_xticklabels([date.strftime('%Y-%m') for date in dates[::480]], rotation=0, fontproperties=font)\n",
    "    # 设置刻度\n",
    "    ax.tick_params(axis='x', which='both', colors='black', length=4, width=2, labelsize=24, direction='out', pad=6, bottom=True, top=False, labelbottom=True,labeltop=False)\n",
    "    ax.tick_params(axis='y', which='both', colors='black', length=4, width=2, labelsize=24, direction='out', pad=6, left=True, right=False, labelleft=True,labelright=False)\n",
    "    ax2.tick_params(axis='y', which='both', colors='#47a0ff', length=4, width=2, labelsize=24, direction='out', pad=6, left=False, right=True, labelleft=False,labelright=True)\n",
    "    # 设置图例\n",
    "    lines, labels = ax.get_legend_handles_labels()\n",
    "    lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "    ax.legend(lines + lines2, labels + labels2, loc='upper right', fancybox=True, shadow=True, ncol=5, prop=font)\n",
    "\n",
    "    plt.xlim([dates_shift.min(), dates_shift.max()])\n",
    "    plt.ylim([0, network_data.max().max() + 0.1])\n",
    "\n",
    "    ax.axhline(axhline_low, color='black', linestyle='--', linewidth=1.5)\n",
    "    ax.axhline(axhline_up, color='black', linestyle='--', linewidth=1.5)\n",
    "    # text绘制图名\n",
    "    ax.text(0.001, 0.945, f'{network_name}', transform=ax.transAxes, fontsize=30, fontname='Times New Roman', verticalalignment='top')\n",
    "\n",
    "    if use_legend:\n",
    "        lines, labels = ax.get_legend_handles_labels()\n",
    "        lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "        legend = ax.legend(lines + lines2, labels + labels2, loc='upper left', fancybox=False, shadow=False, ncol=5, frameon=False, prop=font, bbox_to_anchor=(0.065, 1.035))\n",
    "        for legend_handle in legend.legendHandles:\n",
    "            if isinstance(legend_handle, PathCollection):\n",
    "                legend_handle.set_sizes([80])\n",
    "    else:\n",
    "        ax.legend().set_visible(False)\n",
    "        ax2.legend().set_visible(False)\n",
    "    if use_xlabel==True:\n",
    "        ax.set_xlabel('Date', fontsize=26, fontproperties=font)\n",
    "    ax.set_ylabel(r'Soil Moisture ($m^3/m^{-3}$)', fontsize=26, fontproperties=font)\n",
    "    ax2.set_ylabel(r'Precipitation ($mm /3\\,hours$)', fontsize=26, fontproperties=font)\n",
    "    for label in ax.get_xticklabels() + ax.get_yticklabels() + ax2.get_yticklabels():\n",
    "        label.set_fontsize(24)\n",
    "    ax2.grid(False)\n",
    "    ax.grid(False)\n",
    "    plt.savefig(r\"C:\\Users\\Administrator\\Desktop\\Draw\\zhexiantu.jpeg\", dpi=500, format='jpeg', bbox_inches='tight')\n",
    "\n",
    "create_and_draw_subplots(filtered_all_networks_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (绘图) V2.0 时间动态图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib.font_manager import FontProperties\n",
    "from matplotlib.collections import PathCollection\n",
    "\n",
    "def create_and_draw_subplots(filtered_all_networks_data):\n",
    "    # 创建图形对象,绘制顺序依赖于fig定义顺序\n",
    "    fig = plt.figure(figsize=(36, 30))\n",
    "\n",
    "    total_height = 0.8\n",
    "    single_height = total_height / 5\n",
    "    gap = 0.018\n",
    "\n",
    "    ax1 = fig.add_axes([0.05, 0.05 + 4 * (single_height + gap), 0.9, single_height]) # 上1\n",
    "    ax2 = fig.add_axes([0.05, 0.05 + 3 * (single_height + gap), 0.9, single_height]) # 上2\n",
    "    ax3 = fig.add_axes([0.05, 0.05 + 2 * (single_height + gap), 0.9, single_height]) # 上3\n",
    "    ax4 = fig.add_axes([0.05, 0.05 + 1 * (single_height + gap), 0.9, single_height]) # 上4\n",
    "    ax5 = fig.add_axes([0.05, 0.05, 0.9, single_height]) # 上5\n",
    "\n",
    "    # 绘制网络数据\n",
    "    plot_network_data(ax1, filtered_all_networks_data['Shiquanhe'], '(a)Shiquanhe', axhline_low=0.15, axhline_up=0.3, use_legend=True, use_xlabel=False)\n",
    "    plot_network_data(ax2, filtered_all_networks_data['Ali'], '(b)Ali', axhline_low=0.15, axhline_up=0.3, use_legend=False, use_xlabel=False)\n",
    "    plot_network_data(ax3, filtered_all_networks_data['Maqu'], '(c)Maqu', axhline_low=0.2, axhline_up=0.4, use_legend=False, use_xlabel=False)\n",
    "    plot_network_data(ax4, filtered_all_networks_data['Naqu'], '(d)Naqu', axhline_low=0.2, axhline_up=0.4, use_legend=False, use_xlabel=False)\n",
    "    plot_network_data(ax5, filtered_all_networks_data['CTP'], '(e)CTP', axhline_low=0.2, axhline_up=0.4, use_legend=False, use_xlabel=True)\n",
    "\n",
    "    plt.show()\n",
    "def plot_network_data(ax, network_data, network_name,axhline_low=0.2,axhline_up=0.4, use_legend=True, use_xlabel=True):\n",
    "    font = FontProperties()\n",
    "    font.set_family('Times New Roman')\n",
    "    font.set_size(24)\n",
    "    plt.rcParams['font.family'] = 'Times New Roman'\n",
    "    plt.rcParams['font.size'] = 24\n",
    "\n",
    "    dates = network_data.index\n",
    "    dates_shift = np.arange(len(dates))\n",
    "    plot_settings = {\n",
    "        'Original_Data': {'color': 'red', 'label': 'Original_Data', 'linewidth': 2, 'linestyle': '-', 'zorder': 10, 'alpha': 1.0},\n",
    "        'Satellite_Data': {'color': 'black', 'label': 'Satellite_Data', 'linewidth': 2, 'linestyle': '-', 'zorder': 9, 'alpha': 1.0},\n",
    "        'PSC-FENet-ST': {'color': '#a40062', 'label': 'PSC-FENet-ST', 's': 20, 'marker': '^', 'edgecolors': '#a40062', 'facecolors': 'none', 'zorder': 6, 'alpha': 0.9, 'linewidths': 1},\n",
    "        'PSC-FENet-SP': {'color': '#694e41', 'label': 'PSC-FENet-SP', 's': 20, 'marker': 'v', 'edgecolors': '#694e41', 'facecolors': 'none', 'zorder': 6, 'alpha': 0.9, 'linewidths': 1},\n",
    "        'RF': {'color': '#70e000', 'label': 'Random Forest', 's': 20, 'marker': '*', 'edgecolors': '#70e000', 'facecolors': '#0b6909', 'zorder': 8, 'alpha': 0.85, 'linewidths': 1},\n",
    "        'LSTM': {'color': '#005fb8', 'label': 'LSTM', 's': 20, 'marker': 'X', 'edgecolors': '#005fb8', 'facecolors': 'none', 'zorder': 7, 'alpha': 0.80, 'linewidths': 1},\n",
    "        'Temporal': {'color': '#faa307', 'label': 'Temporal', 's': 20, 'marker': 'D', 'edgecolors': '#faa307', 'facecolors': 'none', 'zorder': 4, 'alpha': 0.70, 'linewidths': 1},\n",
    "    }\n",
    "\n",
    "    ax2 = ax.twinx()\n",
    "    for year in range(dates[0].year, dates[-1].year + 1):\n",
    "        start_date = pd.Timestamp(year=year, month=4, day=1)\n",
    "        start_idx = np.where(dates == start_date)[0][0]\n",
    "        # 绘制虚线\n",
    "        ax.axvline(x=dates_shift[start_idx], color='grey', linestyle='--', linewidth=1)\n",
    "\n",
    "    if 'Original_Data' in network_data.columns:\n",
    "        ax.plot(dates_shift, network_data['Original_Data'], **plot_settings['Original_Data'])\n",
    "    \n",
    "    if 'Satellite_Data' in network_data.columns:\n",
    "        os_data = network_data['Satellite_Data'].dropna()\n",
    "        os_dates_shift = dates_shift[network_data.index.isin(os_data.index)]\n",
    "        ax.plot(os_dates_shift, os_data, **plot_settings['Satellite_Data'])\n",
    "\n",
    "    for column in network_data.columns:\n",
    "        if column not in ['Original_Data', 'Satellite_Data', 'Precipitation'] and column in plot_settings:\n",
    "            ax.scatter(dates_shift, network_data[column], **plot_settings[column])\n",
    "\n",
    "    if 'Precipitation' in network_data.columns:\n",
    "        # 绘制柱状图\n",
    "        ax2.patch.set_alpha(0.0)\n",
    "        ax2.bar(dates_shift, network_data['Precipitation'], width=0.02, color='#47a0ff', edgecolor='#47a0ff', label='Precipitation', alpha=0.8, zorder=10)\n",
    "        ax2.set_ylabel('Precipitation (mm)', fontproperties=font, color='#47a0ff')\n",
    "\n",
    "    # 设置时间刻度\n",
    "    ax.set_xticks(dates_shift[::480])\n",
    "    ax.set_xticklabels([date.strftime('%Y-%m') for date in dates[::480]], rotation=0, fontproperties=font)\n",
    "    # 设置刻度\n",
    "    ax.tick_params(axis='x', which='both', colors='black', length=4, width=2, labelsize=24, direction='out', pad=6, bottom=True, top=False, labelbottom=True,labeltop=False)\n",
    "    ax.tick_params(axis='y', which='both', colors='black', length=4, width=2, labelsize=24, direction='out', pad=6, left=True, right=False, labelleft=True,labelright=False)\n",
    "    ax2.tick_params(axis='y', which='both', colors='#47a0ff', length=4, width=2, labelsize=24, direction='out', pad=6, left=False, right=True, labelleft=False,labelright=True)\n",
    "\n",
    "\n",
    "    plt.xlim([dates_shift.min(), dates_shift.max()])\n",
    "    plt.ylim([0, network_data.max().max() + 0.1])\n",
    "\n",
    "    ax.axhline(axhline_low, color='black', linestyle='--', linewidth=1.5)\n",
    "    ax.axhline(axhline_up, color='black', linestyle='--', linewidth=1.5)\n",
    "    # text绘制图名\n",
    "    ax.text(0.001, 0.945, f'{network_name}', transform=ax.transAxes, fontsize=30, fontname='Times New Roman', verticalalignment='top')\n",
    "\n",
    "    # 设置图例\n",
    "    if use_legend:\n",
    "        lines, labels = ax.get_legend_handles_labels()\n",
    "        lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "        legend = ax.legend(lines + lines2, labels + labels2, loc='upper left', fancybox=False, shadow=False, ncol=4, frameon=False, prop=font, bbox_to_anchor=(0.065, 1.035))\n",
    "        for legend_handle in legend.legendHandles:\n",
    "            if isinstance(legend_handle, PathCollection):\n",
    "                legend_handle.set_sizes([80])\n",
    "    else:\n",
    "        ax.legend().set_visible(False)\n",
    "        ax2.legend().set_visible(False)\n",
    "    if use_xlabel==True:\n",
    "        ax.set_xlabel('Date', fontsize=26, fontproperties=font)\n",
    "    ax.set_ylabel(r'Soil Moisture ($m^3/m^{-3}$)', fontsize=26, fontproperties=font)\n",
    "    ax2.set_ylabel(r'Precipitation ($mm /3\\,hours$)', fontsize=26, fontproperties=font)\n",
    "    for label in ax.get_xticklabels() + ax.get_yticklabels() + ax2.get_yticklabels():\n",
    "        label.set_fontsize(24)\n",
    "    ax2.grid(False)\n",
    "    ax.grid(False)\n",
    "    plt.savefig(r\"C:\\Users\\Administrator\\Desktop\\Draw\\zhexiantu.jpeg\", dpi=500, format='jpeg', bbox_inches='tight')\n",
    "\n",
    "create_and_draw_subplots(filtered_all_networks_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (数据处理) 逐站点指标计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def process_station_data(file_path):\n",
    "    networks_data = {\n",
    "        'Other Networks': pd.DataFrame(),\n",
    "        'CTP': pd.DataFrame(),\n",
    "    }\n",
    "    xls = pd.ExcelFile(file_path)\n",
    "    for sheet_name in xls.sheet_names:\n",
    "        df = xls.parse(sheet_name)\n",
    "        split_data = df['站点名'].str.rsplit('_', n=2, expand=True)\n",
    "        df['point_name'] = split_data[0]\n",
    "        df['lon'] = split_data[1].astype(float)\n",
    "        df['lat'] = split_data[2].astype(float)\n",
    "        df_renamed = df.rename(columns={'PearsonR': 'R'})\n",
    "        df_sorted = df_renamed.sort_values(by=['lat', 'lon'], ascending=[True, False])\n",
    "        best_records = df_sorted.groupby(['lat','lon'], as_index=False).apply(lambda x: x.loc[x['R'].idxmax()]).reset_index(drop=True)\n",
    "        best_records = best_records[['point_name', 'lon', 'lat', 'RMSE', 'ubRMSE', 'R', 'Bias']]\n",
    "        if sheet_name in ['Shiquanhe', 'Ali','Maqu', 'Naqu']:\n",
    "            networks_data['Other Networks'] = pd.concat([networks_data['Other Networks'], best_records])\n",
    "        elif sheet_name in ['CTP']:\n",
    "            networks_data[sheet_name] = pd.concat([networks_data[sheet_name], best_records])\n",
    "        if not networks_data['Other Networks'].empty:\n",
    "            networks_data['Other Networks'] = networks_data['Other Networks'].sort_values(by=['lat', 'lon'], ascending=[True, False])\n",
    "        if not networks_data['CTP'].empty:\n",
    "            networks_data['CTP'] = networks_data['CTP'].sort_values(by=['lat', 'lon'], ascending=[True, False])\n",
    "\n",
    "    selected_points = set()\n",
    "    for key in networks_data:\n",
    "        if not networks_data[key].empty:\n",
    "            selected_points.update(networks_data[key]['point_name'].unique())\n",
    "\n",
    "    return networks_data['Other Networks'], networks_data['CTP'], selected_points\n",
    "\n",
    "\n",
    "def process_station_file_paths(file_paths, selected_points):\n",
    "    networks_data_all_files = {\n",
    "        'Other Networks': [],\n",
    "        'CTP': [],\n",
    "    }\n",
    "\n",
    "    for network_name, file_path in file_paths.items():\n",
    "        xls = pd.ExcelFile(file_path)\n",
    "        networks_data_single_file = {\n",
    "            'Other Networks': pd.DataFrame(),\n",
    "            'CTP': pd.DataFrame(),\n",
    "        }\n",
    "        for sheet_name in xls.sheet_names:\n",
    "            df = xls.parse(sheet_name)\n",
    "            split_data = df['站点名'].str.rsplit('_', n=2, expand=True)\n",
    "            df['point_name'] = split_data[0]\n",
    "            df['lon'] = split_data[1].astype(float)\n",
    "            df['lat'] = split_data[2].astype(float)\n",
    "            df_renamed = df.rename(columns={'PearsonR': 'R'})\n",
    "            df_sorted = df_renamed.sort_values(by=['lat', 'lon'], ascending=[True, False])\n",
    "\n",
    "            best_records = df_sorted[df_sorted['point_name'].isin(selected_points)]\n",
    "            best_records = best_records[['point_name', 'lon', 'lat', 'RMSE', 'ubRMSE', 'R', 'Bias']]\n",
    "\n",
    "            if sheet_name in ['Shiquanhe', 'Ali', 'Maqu', 'Naqu']:\n",
    "                networks_data_single_file['Other Networks'] = pd.concat([networks_data_single_file['Other Networks'], best_records])\n",
    "            elif sheet_name in ['CTP']:\n",
    "                networks_data_single_file['CTP'] = pd.concat([networks_data_single_file['CTP'], best_records])\n",
    "                \n",
    "        for network_type in networks_data_single_file:\n",
    "            if not networks_data_single_file[network_type].empty:\n",
    "                networks_data_all_files[network_type].append(networks_data_single_file[network_type])\n",
    "\n",
    "    for network_type in networks_data_all_files:\n",
    "        if networks_data_all_files[network_type]:\n",
    "            combined_data = pd.concat(networks_data_all_files[network_type])\n",
    "            mean_data = combined_data.groupby(['point_name', 'lon', 'lat'], as_index=False).mean()\n",
    "            networks_data_all_files[network_type] = mean_data.sort_values(by=['lat', 'lon'], ascending=[True, False])\n",
    "        else:\n",
    "            networks_data_all_files[network_type] = pd.DataFrame()\n",
    "\n",
    "    return networks_data_all_files['Other Networks'], networks_data_all_files['CTP']\n",
    "'''\n",
    "file_paths = {\n",
    "    'PSC-FENet-ST': r\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\STPSC-FENet_station_metrics.xlsx\",\n",
    "    'PSC-FENet-SP': r\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\SPPSC-FENet_station_metrics.xlsx\",\n",
    "    'PSC-FENet-ST32': r\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\STPSC-FENet36_station_metrics.xlsx\",\n",
    "    'PSC-FENet-SP32': r\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\SPPSC-FENet36_station_metrics.xlsx\",\n",
    "    'ResNet-ST': r\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\STRESNET_station_metrics.xlsx\",\n",
    "    'ResNet-SP': r\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\SPRESNET_station_metrics.xlsx\",\n",
    "    'UNet-ST': r\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\STUNET_station_metrics.xlsx\",\n",
    "    'UNet-SP': r\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\SPUNET_station_metrics.xlsx\",\n",
    "    'VIT-ST': r\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\STVIT_station_metrics.xlsx\",\n",
    "    'VIT-SP': r\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\SPVIT_station_metrics.xlsx\",\n",
    "    'RF': r\"D:\\Data_Store\\Dataset\\Reconstruction\\RF\\RF_station_metrics.xlsx\",\n",
    "    'LSTM': r\"D:\\Data_Store\\Dataset\\Reconstruction\\LSTM\\LSTM_station_metrics.xlsx\",\n",
    "    'Temporal': r\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\TEMPORAL_station_metrics.xlsx\"\n",
    "}\n",
    "'''\n",
    "\n",
    "file_paths = {\n",
    "    'PSC-FENet-ST': r\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\STPSC-FENet_station_metrics.xlsx\",\n",
    "    'RF': r\"D:\\Data_Store\\Dataset\\Reconstruction\\RF\\RF_station_metrics.xlsx\",\n",
    "    'LSTM': r\"D:\\Data_Store\\Dataset\\Reconstruction\\LSTM\\LSTM_station_metrics.xlsx\",\n",
    "    'Temporal': r\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\TEMPORAL_station_metrics.xlsx\"\n",
    "}\n",
    "\n",
    "file_path = r\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\STPSC-FENet_station_metrics.xlsx\"\n",
    "\n",
    "df_other_networks, df_ctp, selected_points = process_station_data(file_path)\n",
    "all_methods_other_networks,all_methods_ctp = process_station_file_paths(file_paths, selected_points)\n",
    "\n",
    "print(all_methods_other_networks.head(5))\n",
    "print(all_methods_ctp.head(5))\n",
    "\n",
    "print(df_other_networks.head(5))\n",
    "print(df_ctp.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (数据处理) 逐站点全时刻指标均值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def create_combined_data(time_points, network_names, file_paths):\n",
    "    all_data = {time: {network: pd.DataFrame() for network in network_names} for time in time_points}\n",
    "    for model, file_path in file_paths.items():\n",
    "        xls = pd.ExcelFile(file_path)\n",
    "        for time_point in time_points:\n",
    "            networks_data = {network: pd.DataFrame() for network in network_names}\n",
    "            for sheet_name in xls.sheet_names:\n",
    "                if f\"_{time_point}h\" in sheet_name:\n",
    "                    df = xls.parse(sheet_name)\n",
    "                    split_data = df['站点名'].str.rsplit('_', n=2, expand=True)\n",
    "                    df['point_name'] = split_data[0]\n",
    "                    df['lon'] = split_data[1].astype(float)\n",
    "                    df['lat'] = split_data[2].astype(float)\n",
    "                    df_renamed = df.rename(columns={'PearsonR': 'R'})\n",
    "                    df_sorted = df_renamed.sort_values(by=['lat', 'lon'], ascending=[False, True])\n",
    "                    best_records = df_sorted.groupby(['lat', 'lon'], as_index=False).apply(lambda x: x.loc[x['R'].idxmax()]).reset_index(drop=True)\n",
    "                    best_records = best_records[['point_name', 'lon', 'lat', 'RMSE', 'ubRMSE', 'R', 'Bias']]\n",
    "                    for network in network_names:\n",
    "                        if network in sheet_name:\n",
    "                            networks_data[network] = pd.concat([networks_data[network], best_records], ignore_index=True)\n",
    "            for network in network_names:\n",
    "                all_data[time_point][network] = pd.concat([all_data[time_point][network], networks_data[network]], ignore_index=True)\n",
    "\n",
    "    return all_data\n",
    "\n",
    "time_points = ['0', '3', '6', '9', '12', '15', '18', '21']\n",
    "network_names = ['Shiquanhe', 'Ali', 'Maqu', 'Naqu', 'CTP']\n",
    "'''\n",
    "file_paths = {\n",
    "    'PSC-FENet-ST': r\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\STPSCFENet_station_time_metrics.xlsx\",\n",
    "    'PSC-FENet-SP': r\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\SPPSCFENet_station_time_metrics.xlsx\",\n",
    "    'PSC-FENet-ST32': r\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\STPSC-FENet36_station_metrics.xlsx\",\n",
    "    'PSC-FENet-SP32': r\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\SPPSC-FENet36_station_metrics.xlsx\",\n",
    "    'ResNet-ST': r\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\STRESNET_station_time_metrics.xlsx\",\n",
    "    'ResNet-SP': r\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\SPRESNET_station_time_metrics.xlsx\",\n",
    "    'UNet-ST': r\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\STUNET_station_time_metrics.xlsx\",\n",
    "    'UNet-SP': r\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\SPUNET_station_time_metrics.xlsx\",\n",
    "    'VIT-ST': r\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\STVIT_station_time_metrics.xlsx\",\n",
    "    'VIT-SP': r\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\SPVIT_station_time_metrics.xlsx\",\n",
    "    'RF': r\"D:\\Data_Store\\Dataset\\Reconstruction\\RF\\RF_station_time_metrics.xlsx\",\n",
    "    'LSTM': r\"D:\\Data_Store\\Dataset\\Reconstruction\\LSTM\\LSTM_station_time_metrics.xlsx\",\n",
    "    'Temporal': r\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\TEMPORAL_station_time_metrics.xlsx\"\n",
    "}\n",
    "'''\n",
    "file_paths = {\n",
    "    'PSC-FENet-ST': r\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\STPSCFENet_station_time_metrics.xlsx\",\n",
    "    'LSTM': r\"D:\\Data_Store\\Dataset\\Reconstruction\\LSTM\\LSTM_station_time_metrics.xlsx\",\n",
    "    'Temporal': r\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\TEMPORAL_station_time_metrics.xlsx\"\n",
    "}\n",
    "#selected_models = ['PSC-FENet-ST', 'PSC-FENet-SP','PSC-FENet-ST32', 'PSC-FENet-SP32','RF',  'LSTM', 'ResNet-SP', 'UNet-SP', 'ResNet-ST', 'UNet-ST', 'VIT-SP','Temporal']\n",
    "selected_models = ['PSC-FENet-ST','RF',  'LSTM','Temporal']\n",
    "selected_reconstruction_paths = {name: paths for name, paths in file_paths.items() if name in selected_models}\n",
    "\n",
    "all_data = create_combined_data(time_points, network_names, selected_reconstruction_paths)\n",
    "\n",
    "df_example = all_data['3']['Ali']  # 获取3:00的CTP网络的DataFrame\n",
    "print(df_example.head(50))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (绘图) V2.0 原位站点误差 柱状-折线图 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from matplotlib.font_manager import FontProperties\n",
    "from matplotlib.ticker import MultipleLocator\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "import colorsys\n",
    "import matplotlib.lines as mlines\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.patches import FancyBboxPatch\n",
    "import matplotlib.colors as mcolors\n",
    "import seaborn as sns\n",
    "\n",
    "np.random.seed(0)  # For reproducibility\n",
    "data3 = pd.DataFrame({\n",
    "    'point_name': [f'SKU{i}' for i in range(1, 11)],\n",
    "    'RMSE': np.random.rand(10) * 10 + 50,\n",
    "    'ubRMSE': np.random.rand(10) * 5 + 25,\n",
    "    'R': np.random.rand(10) * 1,\n",
    "    'Bias': np.random.rand(10) * 0.05\n",
    "})\n",
    "\n",
    "def plot_design_changes_extended(data, all_method_data, height = 0.4, Gradation_degree = 0.014, Gradation_degree_all_method = 0.024, figsize = (6,12), start_colors_rmse = ['#caf0f8','#DFD8FD'], start_colors_ubrmse = ['#e9f5db','#6BF7D4'],name = None, method=None):\n",
    "\n",
    "    def adjust_color(color, brightness_factor):\n",
    "        rgb = mcolors.hex2color(color)\n",
    "        hls = colorsys.rgb_to_hls(*rgb)\n",
    "        new_hls = (hls[0], max(0, min(1, brightness_factor * hls[1])), hls[2])\n",
    "        # 将调整后的HLS颜色转换回RGB，然后转换为HEX\n",
    "        new_rgb = colorsys.hls_to_rgb(*new_hls)\n",
    "        return mcolors.to_hex(new_rgb)\n",
    "    \n",
    "    sns.set(style=\"whitegrid\", font='Times New Roman')\n",
    "    matplotlib.rcParams['font.family'] = 'Times New Roman'\n",
    "    matplotlib.rcParams['axes.labelweight'] = 'bold'\n",
    "    legend_font = FontProperties(family='Times New Roman', size=12)\n",
    "    font_prop = FontProperties(family='Times New Roman', size=12)\n",
    "    fig, ax1 = plt.subplots(figsize=figsize)\n",
    "    for item in ([ax1.xaxis.label, ax1.yaxis.label] + ax1.get_xticklabels() + ax1.get_yticklabels()):\n",
    "        item.set_fontproperties(font_prop)\n",
    "\n",
    "    # 避免重叠\n",
    "    y = np.arange(len(data))\n",
    "    y1 = [i - height/1.8 for i in y]\n",
    "    y2 = [i + height/1.8 for i in y]\n",
    "    yy = data['point_name'].to_list()\n",
    "\n",
    "    # ax1 RMSE\n",
    "    ha= 'left' if method == 'left' else 'right'\n",
    "    colors_rmse = [adjust_color(start_colors_rmse[0], 1 - Gradation_degree * i) for i in range(len(data['RMSE']))]\n",
    "    colors_rmse_all_method = [adjust_color(start_colors_rmse[1], 1 - Gradation_degree_all_method * i) for i in range(len(data['RMSE']))]\n",
    "    for i, (value,all_method_value, pos) in enumerate(zip(data['RMSE'], all_method_data['RMSE'], y1)):\n",
    "        ax1.barh(pos, all_method_value, height=height, color=colors_rmse_all_method[-i-1], alpha=0.6, zorder=5)\n",
    "\n",
    "        ax1.barh(pos, value, height=height, color=colors_rmse[-i-1], alpha=0.6,zorder=5)\n",
    "        ax1.text(0, pos, f'{value:.3f}', size=10, ha=ha, va='center', fontproperties=font_prop, zorder=5)\n",
    "\n",
    "    ax1.set_yticks(y)\n",
    "    ax1.set_yticklabels(yy)\n",
    "\n",
    "    # ax2 ubRMSE\n",
    "    ax2 = ax1.twiny()\n",
    "    colors_ubrmse = [adjust_color(start_colors_ubrmse[0], 1 - Gradation_degree * i) for i in range(len(data['ubRMSE']))]\n",
    "    colors_ubrmse_all_method = [adjust_color(start_colors_ubrmse[1], 1 - Gradation_degree_all_method * i) for i in range(len(data['ubRMSE']))]\n",
    "    for i, (value,all_method_value, pos) in enumerate(zip(data['ubRMSE'], all_method_data['ubRMSE'], y2)):\n",
    "        ax2.barh(pos, all_method_value, height=height, color=colors_ubrmse_all_method[-i-1], alpha=0.6, zorder=5)\n",
    "\n",
    "        ax2.barh(pos, value, height=height, color=colors_ubrmse[-i-1], alpha=0.6, zorder=5)\n",
    "        ax2.text(0, pos, f'{value:.3f}', size=10, ha=ha, va='center', fontproperties=font_prop, zorder=5)\n",
    "\n",
    "    # ax3 R\n",
    "    color_ax3 = ['#FFB110', '#FFF571']\n",
    "    ax3 = ax1.twiny()\n",
    "    ax3.plot(data['R'], y2, label='R',linestyle='--', color=color_ax3[0], linewidth=1.5, marker='X', markeredgecolor='lightgrey', markersize=10,alpha=0.5,zorder=1)\n",
    "    ax3.scatter(all_method_data['R'], y2, marker='X', edgecolors='black', color=color_ax3[1], alpha=0.6, zorder=2, s=40) \n",
    "    for (r_val, y_val), r_all_method in zip(zip(data['R'], y2), all_method_data['R']):\n",
    "        ax3.plot([r_val, r_all_method], [y_val, y_val], color=color_ax3[1], linewidth=1.5, zorder=1.5)\n",
    "\n",
    "    # ax4 Bias\n",
    "    color_ax4 = ['#9602C0', '#C992EC']\n",
    "    ax4 = ax1.twiny()\n",
    "    ax4.plot(data['Bias'], y1, '--', label='Bias',color=color_ax4[0], linewidth=1.5, marker='P',markeredgecolor='grey', markersize=10,alpha=0.5,zorder=1) \n",
    "    ax4.scatter(all_method_data['Bias'], y1, marker='P', edgecolors='black', color=color_ax4[1], alpha=0.6, zorder=2, s=50)\n",
    "    for (bias_val, y_val), bias_all_method in zip(zip(data['Bias'], y1), all_method_data['Bias']):\n",
    "        ax4.plot([bias_val, bias_all_method], [y_val, y_val], color=color_ax4[1], linewidth=1.5, zorder=1.5)\n",
    "\n",
    "    if method == 'left':\n",
    "        split_points = ['SQ14', 'ALI01', 'NST_05']\n",
    "        current_order_index = {name: i for i, name in enumerate(data['point_name'])}\n",
    "        split_indices = [current_order_index[point] for point in split_points if point in current_order_index]\n",
    "        split_indices.append(len(data))\n",
    "\n",
    "        for i in range(len(split_indices)):\n",
    "            # 计算开始和结束位置\n",
    "            start = 0 if i == 0 else split_indices[i - 1]\n",
    "            end = len(data['point_name']) if i == len(split_indices)-1 else split_indices[i]\n",
    "            # 提取对应段的数据\n",
    "            segment = data.iloc[start:end]\n",
    "            mean_R = np.mean(segment['R'])\n",
    "            # 计算y轴的范围\n",
    "            y_start = y1[start] - height / 1.8 if start < len(y1) else y1[-1] - height / 1.8\n",
    "            y_end = y2[end - 1] + height / 1.8 if end - 1 < len(y2) else y2[-1] + height / 1.8\n",
    "            # 绘制线段\n",
    "            ax3.plot([mean_R, mean_R], [y_start, y_end], linestyle=':', color=color_ax3[0], linewidth=1.3, alpha=0.8, zorder=1)\n",
    "            \n",
    "    else:\n",
    "        mean_R = np.mean(data['R'])\n",
    "        ax3.axvline(x=mean_R, linestyle=':', color=color_ax3[0], linewidth=1.3, alpha=0.8, zorder=1)\n",
    "\n",
    "    ax4.axvline(x=0, linestyle=':', color=color_ax4[0], linewidth=1, alpha=0.5, zorder=1)\n",
    "\n",
    "    # 添加文本\n",
    "    ha= 'right' if method == 'left' else 'left'\n",
    "    r_range = data['R'].max() - data['R'].min()\n",
    "    bias_range = data['Bias'].max() - data['Bias'].min()\n",
    "    r_offset = r_range * 0.15 if method =='left' else r_range * 0.065\n",
    "    bias_offset = bias_range * 0.05\n",
    "    for x, y_val in zip(data['R'], y2):\n",
    "        ax3.text(x - r_offset, y_val, f' {x:.3f}', color=color_ax3[0], va='center', ha=ha, fontproperties=font_prop,zorder=2, size=10)\n",
    "    for x, y_val in zip(data['Bias'], y1):\n",
    "        ax4.text(x - bias_offset, y_val, f' {x:.3f}', color='purple', va='center', ha=ha, fontproperties=font_prop,zorder=2, size=10)\n",
    "\n",
    "    # 轴位置控制\n",
    "    ax1.spines['bottom'].set_position(('outward', 0))\n",
    "    ax1.xaxis.set_ticks_position('bottom')\n",
    "    ax1.xaxis.set_label_position('bottom')\n",
    "    ax2.spines['bottom'].set_position(('outward', 12))\n",
    "    ax2.xaxis.set_ticks_position('bottom')\n",
    "    ax2.xaxis.set_label_position('bottom')\n",
    "    ax3.spines['top'].set_position(('axes', 1.0))\n",
    "    ax3.xaxis.set_ticks_position('top')\n",
    "    ax3.xaxis.set_label_position('top')\n",
    "    ax4.spines['top'].set_position(('outward', 12))\n",
    "    ax4.xaxis.set_ticks_position('top')\n",
    "    ax4.xaxis.set_label_position('top')\n",
    "\n",
    "    # 轴范围控制(先计算拓展范围)\n",
    "    expand_ratio = 0.8\n",
    "    data_RMSE_max = data['RMSE'].max()\n",
    "    data_ubRMSE_max = data['ubRMSE'].max()\n",
    "    RMSE_new_max = data_RMSE_max + (data_RMSE_max) * expand_ratio\n",
    "    ubRMSE_new_max = data_ubRMSE_max + (data_ubRMSE_max) * expand_ratio\n",
    "\n",
    "    expand_ratio = 0.5\n",
    "    shift_ratio = 0.1\n",
    "    data_R_min, data_R_max = data['R'].min(), data['R'].max()\n",
    "    R_range = data_R_max - data_R_min\n",
    "    R_new_min = data_R_min - R_range * expand_ratio * (1 - shift_ratio)\n",
    "    R_new_max = data_R_max + R_range * expand_ratio * shift_ratio\n",
    "    data_Bias_min, data_Bias_max = data['Bias'].min(), data['Bias'].max()\n",
    "    Bias_range = data_Bias_max - data_Bias_min\n",
    "    Bias_new_min = data_Bias_min - Bias_range * expand_ratio * (1 - shift_ratio)\n",
    "    Bias_new_max = data_Bias_max + Bias_range * expand_ratio * shift_ratio\n",
    "\n",
    "    ax1.set_xlim([0.0, 0.28])\n",
    "    ax2.set_xlim([0, 0.26])\n",
    "    ax3.set_xlim([-2, 1])\n",
    "    ax4.set_xlim([-0.4, 0.3])\n",
    "    ax1.barh(y, RMSE_new_max, height=height*2, color='lightgray', alpha=0.4, zorder=0)\n",
    "    if method =='left':ax1.yaxis.tick_left()\n",
    "    ax1.tick_params('y', colors='black', length=3, width=1.5, labelsize=12, direction='out',pad=4)\n",
    "    ax1.tick_params('x', colors=colors_rmse[-1], length=3, width=1, labelsize=10,direction='in', pad=1)\n",
    "    ax2.tick_params('x', colors=colors_ubrmse[-1], length=3, width=1, labelsize=10,direction='in', pad=1)\n",
    "    ax3.tick_params(axis='x', colors=color_ax3[0], length=3, width=1, labelsize=10,direction='in', pad=0)\n",
    "    ax4.tick_params('x', colors='purple', length=3, width=1, labelsize=10,direction='in', pad=0)\n",
    "    ax1.xaxis.set_major_locator(MaxNLocator(prune='lower', nbins=5))\n",
    "    ax2.xaxis.set_major_locator(MaxNLocator(prune='lower', nbins=4))\n",
    "    ax3.xaxis.set_major_locator(MaxNLocator(prune='lower', nbins=4))\n",
    "    ax4.xaxis.set_major_locator(MaxNLocator(prune='lower', nbins=3))\n",
    "\n",
    "    # x轴绘制\n",
    "    ax1.set_xlabel('RMSE', fontproperties=font_prop)\n",
    "    ax2.set_xlabel('ubRMSE', fontproperties=font_prop)\n",
    "    ax3.set_xlabel('Bias', fontproperties=font_prop)\n",
    "    ax4.set_xlabel('R', fontproperties=font_prop)\n",
    "\n",
    "    # 刻度线控制\n",
    "    ax2.yaxis.set_ticks_position('none')\n",
    "    ax3.yaxis.set_ticks_position('none')\n",
    "    ax4.yaxis.set_ticks_position('none')\n",
    "    if method == 'right':\n",
    "        for ax in [ax1, ax2, ax3, ax4]:\n",
    "            ax.yaxis.tick_right()\n",
    "            ax.yaxis.set_label_position(\"right\")\n",
    "            ax.invert_xaxis()\n",
    "    # 控制刻度线上下边界缝隙距离\n",
    "    y_min, y_max = ax1.get_ylim()\n",
    "    padding = (y_max - y_min) * -0.04\n",
    "    ax1.set_ylim(y_min - padding, y_max + padding)\n",
    "    if method == 'left':\n",
    "        # 绘制图例\n",
    "        original_handles = []\n",
    "        original_labels = []\n",
    "        for ax in [ax1,ax2, ax3, ax4]:\n",
    "            handles, labels = ax.get_legend_handles_labels()\n",
    "            original_handles.extend(handles)\n",
    "            original_labels.extend(labels)\n",
    "        gradient_patch_rmse = [mpatches.Patch(color=colors_rmse[5], label='PSC-FENet RMSE')]\n",
    "        gradient_patch_ubrmse = [mpatches.Patch(color=colors_ubrmse[5], label='PSC-FENet ubRMSE')]\n",
    "        all_method_gradient_patch_rmse = [mpatches.Patch(color=colors_rmse_all_method[5], label='Mean RMSE')]\n",
    "        all_method_gradient_patch_ubrmse = [mpatches.Patch(color=colors_ubrmse_all_method[5], label='Mean ubRMSE')]\n",
    "\n",
    "        mean_r_scatter = mlines.Line2D([], [], color=color_ax3[1], marker='X', linestyle='None', markersize=10, label='Mean R', alpha=0.6, markeredgewidth=1, markeredgecolor='black')\n",
    "        mean_bias_scatter = mlines.Line2D([], [], color=color_ax4[1], marker='P', linestyle='None', markersize=10, label='Mean Bias', alpha=0.6, markeredgewidth=1, markeredgecolor='black')\n",
    "        zero_line = mlines.Line2D([], [], color=color_ax4[0], linestyle=':', linewidth=1, label='Zero Line', alpha=0.5)\n",
    "        kpnet_mean_r_line = mlines.Line2D([], [], color=color_ax3[0], linestyle=':', linewidth=1, label='PSC-FENet Mean R', alpha=0.5)\n",
    "\n",
    "        final_handles = original_handles + gradient_patch_rmse + gradient_patch_ubrmse + all_method_gradient_patch_rmse + all_method_gradient_patch_ubrmse+[mean_r_scatter, mean_bias_scatter, zero_line, kpnet_mean_r_line]\n",
    "        final_labels = original_labels + ['PSC-FENet RMSE', 'PSC-FENet ubRMSE', 'Mean RMSE', 'Mean ubRMSE', 'Mean R', 'Mean Bias', 'Zero Line', 'PSC-FENet Mean R']\n",
    "        legend = plt.legend(final_handles, final_labels, \n",
    "           loc='upper left', \n",
    "           bbox_to_anchor=(-0.52, 0.995), \n",
    "           fancybox=False, \n",
    "           shadow=False, \n",
    "           ncol=1, \n",
    "           frameon=False, \n",
    "           prop=legend_font, \n",
    "           fontsize=10, \n",
    "           borderpad=0,\n",
    "           labelspacing=1,\n",
    "           handletextpad=.2)\n",
    "        for text, label in zip(legend.get_texts(), final_labels):\n",
    "            if label in ['PSC-FENet Mean R', 'PSC-FENet ubRMSE', 'PSC-FENet RMSE']:\n",
    "                text.set_fontsize(10)\n",
    "    # 文本标签\n",
    "    y_min, y_max = ax1.get_ylim()\n",
    "    bias_label_y = y_max + (y_max - y_min) * 0.02\n",
    "    r_label_y = bias_label_y - (y_max - y_min) * 0.025\n",
    "    rmse_label_y = y_min - (y_max - y_min) * 0.005\n",
    "    ubrmse_label_y = rmse_label_y - (y_max - y_min) * 0.02\n",
    "    x = -0.005\n",
    "\n",
    "    ax1.text(x, bias_label_y, 'Bias', size=12, fontproperties=font_prop, zorder=5, ha=ha)\n",
    "    ax1.text(x, r_label_y, 'PearsonR', size=12, fontproperties=font_prop, zorder=5, ha=ha)\n",
    "    ax1.text(x, rmse_label_y, 'RMSE', size=12, fontproperties=font_prop, zorder=5, ha=ha)\n",
    "    ax1.text(x, ubrmse_label_y, 'ubRMSE', size=12, fontproperties=font_prop, zorder=5, ha=ha)\n",
    "\n",
    "    # 控制y轴间隙\n",
    "    current_num_ticks = len(ax1.get_yticks())\n",
    "    new_num_ticks = int(current_num_ticks * 1.1)\n",
    "    ax1.yaxis.set_major_locator(MaxNLocator(new_num_ticks))\n",
    "\n",
    "    # 共同控制：隐藏右侧指标轴、x轴label\n",
    "    for ax in [ax1, ax2, ax3, ax4]:\n",
    "        ax.spines[ha].set_visible(False)\n",
    "        ax.xaxis.label.set_visible(False)\n",
    "    for ax in [ax1, ax2, ax3, ax4]:ax.grid(False)\n",
    "    ax3.grid(True, color=color_ax3[0], linestyle='--',alpha=0.5,zorder=0.1)\n",
    "    ax4.grid(True, color='purple', linestyle='--',alpha=0.5,zorder=0.1)\n",
    "    for ax in [ax1, ax2, ax3, ax4]:ax.grid(False)\n",
    "    ax1.grid(True, color='lightgray', linestyle='--', alpha=0.5, which='both', axis='y', zorder=0.5)\n",
    "    file_path = r'C:\\Users\\Administrator\\Desktop\\\\' + name + '.jpeg'\n",
    "    fig.savefig(file_path, dpi=1000, format='jpeg', bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "#plot_design_changes_extended(df_other_networks)\n",
    "plot_design_changes_extended(df_ctp, all_methods_ctp, name = 'b',method='right')\n",
    "plot_design_changes_extended(df_other_networks, all_methods_other_networks,name = \"a\",method='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (数据处理) 原位误差平面图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "data_path = r\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\ST_PSC-FENet.npy\"\n",
    "data = np.load(data_path)\n",
    "overall_mean = np.nanmean(data, axis=0)\n",
    "# 时间起点\n",
    "start_time = pd.Timestamp('2016-01-01 03:00')\n",
    "\n",
    "# 生成完整的时间序列\n",
    "times = pd.date_range(start=start_time, periods=data.shape[2], freq='3H')\n",
    "save_path = r\"D:\\Data_Store\\Dataset\\Reconstruction\\RF\\RF_reconstructed_data_cor.npy\"\n",
    "np.save(save_path, overall_mean)\n",
    "print(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import rasterio\n",
    "from rasterio.transform import from_origin\n",
    "lat_min = 25.550117\n",
    "lat_max = 40.450184\n",
    "lon_min = 66.95031\n",
    "lon_max = 104.95048\n",
    "lat_points = 150\n",
    "lon_points = 381\n",
    "\n",
    "new_lat_max = lat_max + ((lat_max - lat_min) / (lat_points - 1) / 2)\n",
    "new_lon_min = lon_min - ((lon_max - lon_min) / (lon_points - 1) / 2)\n",
    "\n",
    "transform = from_origin(new_lon_min, new_lat_max, (lon_max - lon_min) / (lon_points - 1), (lat_max - lat_min) / (lat_points - 1))\n",
    "\n",
    "metadata = {\n",
    "    'driver': 'GTiff',\n",
    "    'height': lat_points,\n",
    "    'width': lon_points,\n",
    "    'count': 1,\n",
    "    'dtype': 'float32',\n",
    "    'crs': '+proj=latlong',\n",
    "    'transform': transform\n",
    "}\n",
    "data = np.load(r\"D:\\Data_Store\\Dataset\\Reconstruction\\RF\\RF_reconstructed_data_cor.npy\")\n",
    "data_flipped = np.flipud(data)\n",
    "with rasterio.open(r'D:\\Data_Store\\Dataset\\Reconstruction\\RF\\RF_reconstructed_data_geotiff.tif', 'w', **metadata) as dst:\n",
    "    dst.write(data_flipped, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型间不确定度  3H/12H不确定度图、全域最小不确定性图"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (数据处理) 3H 全年不确定性计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.optimize as opt\n",
    "from numpy.linalg import inv, det\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def calculate_uncertainty(models, reference, model_names):\n",
    "    combined_data = np.array(models + [reference])\n",
    "    valid_indices = ~np.isnan(combined_data).any(axis=0)\n",
    "    filtered_models = [model[valid_indices] for model in models]\n",
    "    filtered_reference = reference[valid_indices]\n",
    "    deviations = [model - filtered_reference for model in filtered_models]\n",
    "    y = np.array(deviations)\n",
    "    S = np.cov(y)\n",
    "    n = S.shape[0] + 1\n",
    "    R = np.zeros((n, n))\n",
    "    R[n-1, n-1] = 1 / (2 * np.sum(np.sum(inv(S))))\n",
    "    for i in range(n-1):\n",
    "        for j in range(n-1):\n",
    "            R[i, j] = S[i, j] - R[n-1, n-1] + R[i, n-1] + R[j, n-1]\n",
    "    def fun(x):\n",
    "        return np.sum((S - x[-1] + x[:n-1] + x[:n-1, None])**2) + np.sum(x[:n-1]**2)\n",
    "    def mycon(x):\n",
    "        b = x[-1] - x[:n-1]\n",
    "        return np.dot(b, np.dot(inv(S), b)) - x[-1],\n",
    "    x0 = np.zeros(n)\n",
    "    bnds = [(0, None) for _ in range(n)]\n",
    "    cons = {'type': 'eq', 'fun': mycon}\n",
    "    res = opt.minimize(fun, x0, method='SLSQP', bounds=bnds, constraints=cons)\n",
    "    x = res.x\n",
    "    R[:n-1, n-1] = x[:n-1]\n",
    "    R[n-1, :n-1] = x[:n-1]\n",
    "    for i in range(n-1):\n",
    "        for j in range(n-1):\n",
    "            R[i, j] = S[i, j] - R[n-1, n-1] + R[i, n-1] + R[j, n-1]\n",
    "    uncertainties = R.diagonal()[:-1]\n",
    "    sorted_indices = np.argsort(uncertainties)\n",
    "    sorted_models = {model_names[sorted_indices[i]]: i + 1 for i in range(len(uncertainties))}\n",
    "    model_uncertainties = {model_names[i]: uncertainties[i] for i in range(len(uncertainties))}\n",
    "    return sorted_models, model_uncertainties\n",
    "\n",
    "reconstruction_paths = {\n",
    "    'PSC-FENet-SP': r\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\SP_PSC-FENet.npy\",\n",
    "    'PSC-FENet-ST': r\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\ST_PSC-FENet.npy\",\n",
    "    'LSTM': r\"D:\\Data_Store\\Dataset\\Reconstruction\\LSTM\\LSTM1.npy\",\n",
    "    'RF': r\"D:\\Data_Store\\Dataset\\Reconstruction\\RF\\RF_reconstructed_data.npy\",\n",
    "    'ResNet-SP': r\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\SP_ResNet.npy\",\n",
    "    'ResNet-ST': r\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\ST_ResNet.npy\",\n",
    "    'UNet-SP': r\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\SP_UNet.npy\",\n",
    "    'UNet-ST': r\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\ST_UNet.npy\",\n",
    "    'VIT-SP': r\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\SP_ViT.npy\",\n",
    "    'VIT-ST': r\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\ST_ViT.npy\",\n",
    "    'Temporal': r\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\Temporal.npy\"\n",
    "}\n",
    "\n",
    "selected_model_names = ['PSC-FENet-ST', 'PSC-FENet-SP', 'LSTM', 'RF', 'Temporal']\n",
    "selected_paths = {name: reconstruction_paths[name] for name in selected_model_names if name in reconstruction_paths}\n",
    "model_data = [np.load(path)[:8767, :, :] for path in selected_paths.values()]\n",
    "\n",
    "model_names = list(selected_paths.keys())\n",
    "reference_data = np.zeros(model_data[0].shape)\n",
    "time_steps, lons, lats = model_data[0].shape\n",
    "\n",
    "model_ranks = np.zeros((lons, lats, len(model_names)), dtype=int)\n",
    "model_uncertainty_values = np.zeros((lons, lats, len(model_names)))\n",
    "for lon in range(lons):\n",
    "    for lat in range(lats):\n",
    "        current_data = [model[:, lon, lat] for model in model_data]\n",
    "        sorted_models, model_uncertainties = calculate_uncertainty(current_data, reference_data[:, lon, lat], model_names)\n",
    "        # 排名\n",
    "        for model_name, rank in sorted_models.items():\n",
    "            model_index = model_names.index(model_name)\n",
    "            model_ranks[lon, lat, model_index] = rank\n",
    "        # 不确定性\n",
    "        for model_name, uncertainty in model_uncertainties.items():\n",
    "            model_index = model_names.index(model_name)\n",
    "            model_uncertainty_values[lon, lat, model_index] = uncertainty\n",
    "del model_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (数据处理) 12H 全年不确定性计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.optimize as opt\n",
    "from numpy.linalg import inv, det\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "def calculate_uncertainty(models, reference, model_names):\n",
    "    combined_data = np.array(models + [reference])\n",
    "    valid_indices = ~np.isnan(combined_data).any(axis=0)\n",
    "    filtered_models = [model[valid_indices] for model in models]\n",
    "    filtered_reference = reference[valid_indices]\n",
    "    deviations = [model - filtered_reference for model in filtered_models]\n",
    "    y = np.array(deviations)\n",
    "    S = np.cov(y)\n",
    "    n = S.shape[0] + 1\n",
    "    R = np.zeros((n, n))\n",
    "    R[n-1, n-1] = 1 / (2 * np.sum(np.sum(inv(S))))\n",
    "    for i in range(n-1):\n",
    "        for j in range(n-1):\n",
    "            R[i, j] = S[i, j] - R[n-1, n-1] + R[i, n-1] + R[j, n-1]\n",
    "    def fun(x):\n",
    "        return np.sum((S - x[-1] + x[:n-1] + x[:n-1, None])**2) + np.sum(x[:n-1]**2)\n",
    "    def mycon(x):\n",
    "        b = x[-1] - x[:n-1]\n",
    "        return np.dot(b, np.dot(inv(S), b)) - x[-1],\n",
    "    x0 = np.zeros(n)\n",
    "    bnds = [(0, None) for _ in range(n)]\n",
    "    cons = {'type': 'eq', 'fun': mycon}\n",
    "    res = opt.minimize(fun, x0, method='SLSQP', bounds=bnds, constraints=cons)\n",
    "    x = res.x\n",
    "    R[:n-1, n-1] = x[:n-1]\n",
    "    R[n-1, :n-1] = x[:n-1]\n",
    "    for i in range(n-1):\n",
    "        for j in range(n-1):\n",
    "            R[i, j] = S[i, j] - R[n-1, n-1] + R[i, n-1] + R[j, n-1]\n",
    "    uncertainties = R.diagonal()[:-1]\n",
    "    sorted_indices = np.argsort(uncertainties)\n",
    "    sorted_models = {model_names[sorted_indices[i]]: i + 1 for i in range(len(uncertainties))}\n",
    "    model_uncertainties = {model_names[i]: uncertainties[i] for i in range(len(uncertainties))}\n",
    "    return sorted_models, model_uncertainties\n",
    "\n",
    "reconstruction_paths = {\n",
    "    'PSC-FENet-SP': r\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\SP_PSC-FENet.npy\",\n",
    "    'PSC-FENet-ST': r\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\ST_PSC-FENet.npy\",\n",
    "    'LSTM': r\"D:\\Data_Store\\Dataset\\Reconstruction\\LSTM\\LSTM1.npy\",\n",
    "    'RF': r\"D:\\Data_Store\\Dataset\\Reconstruction\\RF\\RF_reconstructed_data.npy\",\n",
    "    'ResNet-SP': r\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\SP_ResNet.npy\",\n",
    "    'ResNet-ST': r\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\ST_ResNet.npy\",\n",
    "    'UNet-SP': r\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\SP_UNet.npy\",\n",
    "    'UNet-ST': r\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\ST_UNet.npy\",\n",
    "    'VIT-SP': r\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\SP_ViT.npy\",\n",
    "    'VIT-ST': r\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\ST_ViT.npy\",\n",
    "    'Temporal': r\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\Temporal.npy\"\n",
    "}\n",
    "\n",
    "dates = pd.date_range(start='2016-01-01 03:00', periods=8768, freq='3H')\n",
    "filter_mask = ((dates.hour == 6) | (dates.hour == 18))\n",
    "filtered_indices = np.where(filter_mask)[0]\n",
    "\n",
    "selected_model_names = ['PSC-FENet-ST', 'PSC-FENet-SP', 'LSTM', 'RF', 'Temporal']\n",
    "\n",
    "#selected_model_names = ['PSC-FENet-ST','PSC-FENet-SP',  'ResNet-ST', 'ResNet-SP', 'UNet-ST', 'UNet-SP', 'VIT-ST', 'VIT-SP',  'LSTM', 'RF', 'Temporal']\n",
    "selected_paths = {name: reconstruction_paths[name] for name in selected_model_names if name in reconstruction_paths}\n",
    "model_data = [np.load(path)[filtered_indices, :, :] for path in selected_paths.values()]\n",
    "\n",
    "model_names = list(selected_paths.keys())\n",
    "reference_data = np.zeros(model_data[0].shape)\n",
    "time_steps, lons, lats = model_data[0].shape\n",
    "\n",
    "model_ranks_12 = np.zeros((lons, lats, len(model_names)), dtype=int)\n",
    "model_uncertainty_values_12 = np.zeros((lons, lats, len(model_names)))\n",
    "\n",
    "for lon in range(lons):\n",
    "    for lat in range(lats):\n",
    "        current_data = [model[:, lon, lat] for model in model_data]\n",
    "        sorted_models, model_uncertainties = calculate_uncertainty(current_data, reference_data[:, lon, lat], model_names)\n",
    "        # 排名\n",
    "        for model_name, rank in sorted_models.items():\n",
    "            model_index = model_names.index(model_name)                                                                                                                                                                                                                                                                                                                                 \n",
    "            model_ranks_12[lon, lat, model_index] = rank\n",
    "        # 不确定性\n",
    "        for model_name, uncertainty in model_uncertainties.items():\n",
    "            model_index = model_names.index(model_name)\n",
    "            model_uncertainty_values_12[lon, lat, model_index] = uncertainty\n",
    "del model_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (数据处理) 每日均值全年不确定性计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.optimize as opt\n",
    "from numpy.linalg import inv, det\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "def calculate_uncertainty(models, reference, model_names):\n",
    "    combined_data = np.array(models + [reference])\n",
    "    valid_indices = ~np.isnan(combined_data).any(axis=0)\n",
    "    filtered_models = [model[valid_indices] for model in models]\n",
    "    filtered_reference = reference[valid_indices]\n",
    "    deviations = [model - filtered_reference for model in filtered_models]\n",
    "    y = np.array(deviations)\n",
    "    S = np.cov(y)\n",
    "    n = S.shape[0] + 1\n",
    "    R = np.zeros((n, n))\n",
    "    R[n-1, n-1] = 1 / (2 * np.sum(np.sum(inv(S))))\n",
    "    for i in range(n-1):\n",
    "        for j in range(n-1):\n",
    "            R[i, j] = S[i, j] - R[n-1, n-1] + R[i, n-1] + R[j, n-1]\n",
    "    def fun(x):\n",
    "        return np.sum((S - x[-1] + x[:n-1] + x[:n-1, None])**2) + np.sum(x[:n-1]**2)\n",
    "    def mycon(x):\n",
    "        b = x[-1] - x[:n-1]\n",
    "        return np.dot(b, np.dot(inv(S), b)) - x[-1],\n",
    "    x0 = np.zeros(n)\n",
    "    bnds = [(0, None) for _ in range(n)]\n",
    "    cons = {'type': 'eq', 'fun': mycon}\n",
    "    res = opt.minimize(fun, x0, method='SLSQP', bounds=bnds, constraints=cons)\n",
    "    x = res.x\n",
    "    R[:n-1, n-1] = x[:n-1]\n",
    "    R[n-1, :n-1] = x[:n-1]\n",
    "    for i in range(n-1):\n",
    "        for j in range(n-1):\n",
    "            R[i, j] = S[i, j] - R[n-1, n-1] + R[i, n-1] + R[j, n-1]\n",
    "    uncertainties = R.diagonal()[:-1]\n",
    "    sorted_indices = np.argsort(uncertainties)\n",
    "    sorted_models = {model_names[sorted_indices[i]]: i + 1 for i in range(len(uncertainties))}\n",
    "    model_uncertainties = {model_names[i]: uncertainties[i] for i in range(len(uncertainties))}\n",
    "    return sorted_models, model_uncertainties\n",
    "\n",
    "reconstruction_paths = {\n",
    "    'PSC-FENet-SP': r\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\SP_PSC-FENet.npy\",\n",
    "    'PSC-FENet-ST': r\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\ST_PSC-FENet.npy\",\n",
    "    'LSTM': r\"D:\\Data_Store\\Dataset\\Reconstruction\\LSTM\\LSTM1.npy\",\n",
    "    'RF': r\"D:\\Data_Store\\Dataset\\Reconstruction\\RF\\RF_reconstructed_data.npy\",\n",
    "    'ResNet-SP': r\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\SP_ResNet.npy\",\n",
    "    'ResNet-ST': r\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\ST_ResNet.npy\",\n",
    "    'UNet-SP': r\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\SP_UNet.npy\",\n",
    "    'UNet-ST': r\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\ST_UNet.npy\",\n",
    "    'VIT-SP': r\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\SP_ViT.npy\",\n",
    "    'VIT-ST': r\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\ST_ViT.npy\",\n",
    "    'Temporal': r\"D:\\Data_Store\\Dataset\\Reconstruction\\ST\\RESALL\\Temporal.npy\"\n",
    "}\n",
    "\n",
    "\n",
    "dates = pd.date_range(start='2016-01-01 03:00', periods=8767, freq='3H')\n",
    "n_per_day = 8\n",
    "\n",
    "full_groups = len(dates) // n_per_day * n_per_day\n",
    "daily_means_indices = [np.arange(i, i + n_per_day) for i in range(0, full_groups, n_per_day)]\n",
    "\n",
    "if len(dates) % n_per_day != 0:\n",
    "    daily_means_indices.append(np.arange(full_groups, len(dates)))\n",
    "\n",
    "daily_means_indices = np.array(daily_means_indices)\n",
    "selected_model_names = ['PSC-FENet-ST', 'PSC-FENet-SP', 'LSTM', 'RF', 'Temporal']\n",
    "selected_paths = {name: reconstruction_paths[name] for name in selected_model_names if name in reconstruction_paths}\n",
    "\n",
    "model_data = [np.load(path)[:8767] for path in selected_paths.values()]\n",
    "\n",
    "daily_means_data = []\n",
    "for data in model_data:\n",
    "    daily_means = np.array([np.mean(data[indices, :, :], axis=0) for indices in daily_means_indices])\n",
    "    daily_means_data.append(daily_means)\n",
    "\n",
    "model_names = list(selected_paths.keys())\n",
    "reference_data = np.zeros(daily_means_data[0].shape)\n",
    "time_steps, lons, lats = daily_means_data[0].shape\n",
    "\n",
    "model_ranks_daily = np.zeros((lons, lats, len(model_names)), dtype=int)\n",
    "model_uncertainty_values_daily = np.zeros((lons, lats, len(model_names)))\n",
    "\n",
    "for lon in range(lons):\n",
    "    for lat in range(lats):\n",
    "        current_data = [model[:, lon, lat] for model in daily_means_data]\n",
    "        sorted_models, model_uncertainties = calculate_uncertainty(current_data, reference_data[:, lon, lat], model_names)\n",
    "        # 排名\n",
    "        for model_name, rank in sorted_models.items():\n",
    "            model_index = model_names.index(model_name)\n",
    "            model_ranks_daily[lon, lat, model_index] = rank\n",
    "        # 不确定性\n",
    "        for model_name, uncertainty in model_uncertainties.items():\n",
    "            model_index = model_names.index(model_name)\n",
    "            model_uncertainty_values_daily[lon, lat, model_index] = uncertainty\n",
    "\n",
    "del model_data, daily_means_data  # 清理内存"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (数据处理) 最优模型计算\n",
    "运行之前先运行前面两步\n",
    "selected_models 已经在前面定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from shapely.geometry import Point\n",
    "\n",
    "def extract_best_model_for_each_point(model_ranks, model_names, selected_models):\n",
    "\n",
    "    model_name_to_original_index = {name: i for i, name in enumerate(model_names)}\n",
    "    model_name_to_selected_index = {name: i for i, name in enumerate(selected_models)}\n",
    "    best_models = np.zeros((model_ranks.shape[0], model_ranks.shape[1]), dtype=int)\n",
    "    for lat in range(model_ranks.shape[0]):\n",
    "        for lon in range(model_ranks.shape[1]):\n",
    "            ranks = model_ranks[lat, lon, :]\n",
    "            best_rank = float('inf')\n",
    "            best_model_index = -1\n",
    "            for model in selected_models:\n",
    "                original_index = model_name_to_original_index[model]\n",
    "                if ranks[original_index] < best_rank:\n",
    "                    best_rank = ranks[original_index]\n",
    "                    best_model_index = model_name_to_selected_index[model]\n",
    "            if best_model_index >= 0:\n",
    "                best_models[lat, lon] = best_model_index + 1\n",
    "\n",
    "    return best_models\n",
    "\n",
    "def extract_best_value_for_each_point(model_uncertainty_values, model_names, selected_models):\n",
    "    model_name_to_original_index = {name: i for i, name in enumerate(model_names)}\n",
    "    best_models_values = np.zeros((model_uncertainty_values.shape[0], model_uncertainty_values.shape[1]))\n",
    "\n",
    "    for lat in range(model_uncertainty_values.shape[0]):\n",
    "        for lon in range(model_uncertainty_values.shape[1]):\n",
    "            best_value = float('inf')\n",
    "            for model in selected_models:\n",
    "                original_index = model_name_to_original_index[model]\n",
    "                value = model_uncertainty_values[lat, lon, original_index]\n",
    "                if value < best_value:\n",
    "                    best_value = value\n",
    "            best_models_values[lat, lon] = best_value\n",
    "\n",
    "    return best_models_values\n",
    "\n",
    "def apply_clipping(data, lon_grid, lat_grid, shp_path):\n",
    "    \"\"\"\n",
    "    使用.shp文件裁剪数据：在边界外的数据设置为NaN。\n",
    "    \"\"\"\n",
    "    gdf = gpd.read_file(shp_path)\n",
    "    # 获取.shp文件的几何形状\n",
    "    poly = gdf.unary_union\n",
    "\n",
    "    clipped_data = np.full(data.shape, np.nan)\n",
    "\n",
    "    for i in range(data.shape[0]):\n",
    "        for j in range(data.shape[1]):\n",
    "            point = Point(lon_grid[i, j], lat_grid[i, j])\n",
    "            if point.within(poly):\n",
    "                clipped_data[i, j] = data[i, j]\n",
    "\n",
    "    return clipped_data\n",
    "\n",
    "lats, lons, _ = model_ranks.shape\n",
    "\n",
    "# 可使用model_names, selected_models匹配model_names中selected_models正确排名，这里使用相同列表\n",
    "best_models = extract_best_model_for_each_point(model_ranks, selected_model_names, selected_model_names)\n",
    "best_models_values = extract_best_value_for_each_point(model_uncertainty_values, selected_model_names, selected_model_names)\n",
    "\n",
    "best_models_12 = extract_best_model_for_each_point(model_ranks_12, selected_model_names, selected_model_names)\n",
    "best_models_values_12 = extract_best_value_for_each_point(model_uncertainty_values_12, selected_model_names, selected_model_names)\n",
    "\n",
    "best_models_daily = extract_best_model_for_each_point(model_ranks_daily, selected_model_names, selected_model_names)\n",
    "best_models_values_daily = extract_best_value_for_each_point(model_uncertainty_values_daily, selected_model_names, selected_model_names)\n",
    "\n",
    "# 数据裁剪\n",
    "lat_min = 25.550117\n",
    "lat_max = 40.450184\n",
    "lon_min = 66.95031\n",
    "lon_max = 104.95048\n",
    "lat_points = 150\n",
    "lon_points = 381\n",
    "lat_span = (lat_max - lat_min) / (lat_points - 1)\n",
    "lon_span = (lon_max - lon_min) / (lon_points - 1)\n",
    "new_lat_min = lat_min - lat_span / 2\n",
    "new_lat_max = lat_max + lat_span / 2\n",
    "new_lon_min = lon_min - lon_span / 2\n",
    "new_lon_max = lon_max + lon_span / 2\n",
    "\n",
    "lon_range = np.linspace(lon_min, lon_max, lon_points)\n",
    "lat_range = np.linspace(lat_min, lat_max, lat_points)\n",
    "lon_grid, lat_grid = np.meshgrid(lon_range, lat_range)\n",
    "\n",
    "shp_path = \"D:\\\\Data_Store\\\\TPBoundary_new(2021)\\\\TPBoundary_new(2021).shp\"\n",
    "clipped_data = apply_clipping(best_models, lon_grid, lat_grid, shp_path)\n",
    "clipped_data_12 = apply_clipping(best_models_12, lon_grid, lat_grid, shp_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (数据处理) 数据裁剪"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "from shapely.geometry import Point\n",
    "from matplotlib import rcParams\n",
    "import geopandas as gpd\n",
    "import matplotlib.colors as mcolors\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "def apply_clipping(data, lon_grid, lat_grid, shp_path):\n",
    "    \"\"\"\n",
    "    使用.shp文件裁剪数据：在边界外的数据设置为NaN。\n",
    "    \"\"\"\n",
    "    gdf = gpd.read_file(shp_path)\n",
    "    # 获取.shp文件的几何形状\n",
    "    poly = gdf.unary_union\n",
    "\n",
    "    clipped_data = np.full(data.shape, np.nan)\n",
    "\n",
    "    for i in range(data.shape[0]):\n",
    "        for j in range(data.shape[1]):\n",
    "            point = Point(lon_grid[i, j], lat_grid[i, j])\n",
    "            if point.within(poly):\n",
    "                clipped_data[i, j] = data[i, j]\n",
    "\n",
    "    return clipped_data\n",
    "\n",
    "lat_min = 25.550117\n",
    "lat_max = 40.450184\n",
    "lon_min = 66.95031\n",
    "lon_max = 104.95048\n",
    "lat_points = 150\n",
    "lon_points = 381\n",
    "\n",
    "lon_range = np.linspace(lon_min, lon_max, lon_points)\n",
    "lat_range = np.linspace(lat_min, lat_max, lat_points)\n",
    "lon_grid, lat_grid = np.meshgrid(lon_range, lat_range)\n",
    "\n",
    "#model_names = ['PSC-FENet-ST', 'ResNet-ST', 'UNet-ST', 'VIT-ST',  'LSTM', 'RF', 'Temporal']\n",
    "model_names = ['PSC-FENet-ST','PSC-FENet-SP',  'ResNet-ST', 'ResNet-SP', 'UNet-ST', 'UNet-SP', 'VIT-ST', 'VIT-SP',  'LSTM', 'RF', 'Temporal']\n",
    "\n",
    "data_pieces,data_pieces_12 = {},{}\n",
    "for i, model_name in enumerate(model_names):\n",
    "    model_uncertainty = model_uncertainty_values[:, :, i]\n",
    "    model_uncertainty_12 = model_uncertainty_values_12[:, :, i]\n",
    "    data_pieces[model_name] = model_uncertainty \n",
    "    data_pieces_12[model_name] = model_uncertainty_12\n",
    "\n",
    "clipped_data_pieces,clipped_data_pieces_12 = {},{}\n",
    "shp_path = \"D:\\\\Data_Store\\\\TPBoundary_new(2021)\\\\TPBoundary_new(2021).shp\"\n",
    "for name, data in data_pieces.items():\n",
    "    if name in ['PSC-FENet-ST', 'ResNet-ST', 'UNet-ST', 'VIT-ST',  'LSTM', 'RF', 'Temporal']:\n",
    "        clipped_data = apply_clipping(data, lon_grid, lat_grid, shp_path)\n",
    "        clipped_data_pieces[name] = clipped_data\n",
    "\n",
    "for name_12, data_12 in data_pieces_12.items():\n",
    "    if name in ['PSC-FENet-ST', 'ResNet-ST', 'UNet-ST', 'VIT-ST',  'LSTM', 'RF', 'Temporal']:\n",
    "        clipped_data_12 = apply_clipping(data_12, lon_grid, lat_grid, shp_path)\n",
    "        clipped_data_pieces_12[name_12] = clipped_data_12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (数据处理)优化后的数据裁剪"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_clipping_mask(lon_grid, lat_grid, shp_path):\n",
    "    \"\"\"\n",
    "    使用.shp文件生成数据掩码：在边界外的数据掩码为0，在边界内的数据掩码为1。\n",
    "    \"\"\"\n",
    "    import geopandas as gpd\n",
    "    from shapely.geometry import Point\n",
    "    import numpy as np\n",
    "\n",
    "    gdf = gpd.read_file(shp_path)\n",
    "    poly = gdf.unary_union\n",
    "    mask = np.zeros_like(lon_grid, dtype=bool)\n",
    "\n",
    "    for i in range(lon_grid.shape[0]):\n",
    "        for j in range(lon_grid.shape[1]):\n",
    "            point = Point(lon_grid[i, j], lat_grid[i, j])\n",
    "            if point.within(poly):\n",
    "                mask[i, j] = True\n",
    "\n",
    "    return mask\n",
    "\n",
    "shp_path = r\"D:\\\\Data_Store\\\\TPBoundary_new(2021)\\\\TPBoundary_new(2021).shp\"\n",
    "model_names = ['PSC-FENet-ST', 'PSC-FENet-SP', 'LSTM', 'RF', 'Temporal']\n",
    "#model_names = ['PSC-FENet-ST','PSC-FENet-SP',  'ResNet-ST', 'ResNet-SP', 'UNet-ST', 'UNet-SP', 'VIT-ST', 'VIT-SP',  'LSTM', 'RF', 'Temporal']\n",
    "\n",
    "settings = {0.1: (25.550117, 40.450184, 66.95031, 104.95048, 150, 381)}\n",
    "\n",
    "\n",
    "data_pieces,data_pieces_12,data_pieces_daily = {},{},{}\n",
    "for i, model_name in enumerate(model_names):\n",
    "    model_uncertainty = model_uncertainty_values[:, :, i]\n",
    "    model_uncertainty_12 = model_uncertainty_values_12[:, :, i]\n",
    "    model_uncertainty_daily = model_uncertainty_values_daily[:, :, i]\n",
    "    data_pieces[model_name] = model_uncertainty \n",
    "    data_pieces_12[model_name] = model_uncertainty_12\n",
    "    data_pieces_daily[model_name] = model_uncertainty_daily\n",
    "\n",
    "masks = {}\n",
    "for mode, (lat_min, lat_max, lon_min, lon_max, lat_points, lon_points) in settings.items():\n",
    "    lon_step = (lon_max - lon_min) / (lon_points - 1)\n",
    "    lat_step = (lat_max - lat_min) / (lat_points - 1)\n",
    "    new_lon_min = lon_min - lon_step / 2\n",
    "    new_lon_max = lon_max + lon_step / 2\n",
    "    new_lat_min = lat_min - lat_step / 2\n",
    "    new_lat_max = lat_max + lat_step / 2\n",
    "    lon_range, lat_range = np.linspace(new_lon_min, new_lon_max, lon_points), np.linspace(new_lat_min, new_lat_max, lat_points)\n",
    "    lon_grid, lat_grid = np.meshgrid(lon_range, lat_range)\n",
    "    masks[mode] = generate_clipping_mask(lon_grid, lat_grid, shp_path)\n",
    "\n",
    "mode = 0.1\n",
    "mask = masks[mode]\n",
    "clipped_data_pieces,clipped_data_pieces_12,clipped_data_pieces_daily = {},{},{}\n",
    "\n",
    "for name, data in data_pieces.items():\n",
    "        clipped_data_pieces[name] = np.where(mask, data, np.nan)\n",
    "for name_12, data_12 in data_pieces_12.items():\n",
    "        clipped_data_pieces_12[name_12] = np.where(mask, data_12, np.nan)\n",
    "for name_daily, data_daily in data_pieces_daily.items():\n",
    "        clipped_data_pieces_daily[name_daily] = np.where(mask, data_daily, np.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (绘图) 多模型不确定度图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.colors as mcolors\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib.colors import Normalize\n",
    "from matplotlib.cm import ScalarMappable\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from matplotlib import rcParams\n",
    "\n",
    "def add_degree_E(x, pos):\n",
    "    return f\"{int(x)}°E\"\n",
    "\n",
    "def add_degree_N(y, pos):\n",
    "    return f\"{int(y)}°N\"\n",
    "def plot_data_slices(data_pieces, lon_range, lat_range, name):\n",
    "    fig = plt.figure(figsize=(36, 24))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    lon_grid, lat_grid = np.meshgrid(lon_range, lat_range)\n",
    "    y_positions = np.arange(len(data_pieces))\n",
    "\n",
    "    # 收集所有数据用于计算颜色范围\n",
    "    #all_data = np.concatenate([data.flatten() for _, data in data_pieces.items()])\n",
    "    vmin = 0\n",
    "    vmax = 0.025\n",
    "    '''\n",
    "    colors = [\"#ffc43d\", \"#60d394\"]\n",
    "    cmap_name = \"custom_orange_green\"\n",
    "    cm = LinearSegmentedColormap.from_list(cmap_name, colors, N=100)\n",
    "\n",
    "    # Create a gradient image\n",
    "    gradient = np.linspace(0, 1, 256)\n",
    "    gradient = np.vstack((gradient, gradient))\n",
    "    '''\n",
    "    # 创建归一化和映射对象\n",
    "    norm = Normalize(vmin=vmin, vmax=vmax)\n",
    "    mappable = ScalarMappable(norm=norm, cmap='RdYlGn_r')\n",
    "\n",
    "    # 绘制每个数据片\n",
    "    for i, (indicator, data) in enumerate(data_pieces.items()):\n",
    "        colors = mappable.to_rgba(data)\n",
    "        colors[np.isnan(data), -1] = 0\n",
    "        ax.plot_surface(lon_grid, np.full(lon_grid.shape, y_positions[i]), lat_grid,\n",
    "                        facecolors=colors, rstride=1, cstride=1, linewidth=0, antialiased=False)\n",
    "\n",
    "    # 设置colorbar\n",
    "    cbar = plt.colorbar(mappable, ax=ax, aspect=20, shrink=0.2, pad=0.02, orientation='vertical', extend='both')\n",
    "    cbar.set_label('Uncertainty ($m^3/m^{-3}$)', rotation=90, labelpad=10, size=16)\n",
    "    cbar.ax.tick_params(labelsize=14)\n",
    "\n",
    "    # 设置刻度和标签\n",
    "    #ax.set_xlabel('Longitude(°)', labelpad=30,size=18, rotation=90)\n",
    "    #ax.set_zlabel('Latitude(°)', labelpad=5,size=18)\n",
    "    ax.tick_params(axis='y', colors='black', length=8, width=1.5, labelsize=16, direction='out', pad=10)\n",
    "    ax.tick_params(axis='x', colors='black', length=8, width=1.5, labelsize=16, direction='out', pad=0)\n",
    "    ax.tick_params(axis='z', colors='black', length=5, width=1.5, labelsize=16, direction='out', pad=5)\n",
    "    ax.set_yticks(y_positions)\n",
    "    ax.set_yticklabels(list(data_pieces.keys()),size=16)\n",
    "\n",
    "    # 设置绘图范围和刻度数目\n",
    "    ax.set_xlim(max(lon_range)+0.01, min(lon_range)-0.01)\n",
    "    ax.set_ylim(min(y_positions)+0.1, max(y_positions)-0.01)\n",
    "    ax.set_zlim(min(lat_range)-0.01, max(lat_range)+0.01)\n",
    "    ax.zaxis.set_major_locator(MaxNLocator(prune='lower', nbins=5))\n",
    "    ax.xaxis.set_major_locator(MaxNLocator(prune='lower', nbins=5))\n",
    "    ax.xaxis.set_major_formatter(FuncFormatter(add_degree_E))\n",
    "    ax.zaxis.set_major_formatter(FuncFormatter(add_degree_N))\n",
    "\n",
    "    # 去除背景色并调整视角\n",
    "    ax.xaxis.pane.fill = False\n",
    "    ax.yaxis.pane.fill = False\n",
    "    ax.zaxis.pane.fill = False\n",
    "    ax.set_proj_type('ortho')\n",
    "    ax.set_box_aspect([3, 6, 1])\n",
    "    ax.view_init(elev=3, azim=28)\n",
    "    plt.savefig(r\"C:\\Users\\Administrator\\Desktop\\Draw\\uncertain\"+name+\".jpeg\", dpi=1000, format='jpeg')\n",
    "    plt.show()\n",
    "\n",
    "rcParams['font.family'] = 'serif'\n",
    "rcParams['font.serif'] = ['Times New Roman']\n",
    "\n",
    "plot_data_slices(clipped_data_pieces, lon_range, lat_range, name='a')\n",
    "#plot_data_slices(clipped_data_pieces_12, lon_range, lat_range, name='b')\n",
    "plot_data_slices(clipped_data_pieces_daily, lon_range, lat_range, name='b')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (图像裁剪)  手动裁剪\n",
    "bbox_inches='tight'不起作用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageFile\n",
    "\n",
    "# 修改Pillow的像素限制（默认是 89478485 pixels，可以根据需要调整这个值）\n",
    "Image.MAX_IMAGE_PIXELS = None  # 移除限制，慎用！\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True  # 允许加载截断的图像\n",
    "\n",
    "def crop_image(input_path, output_path):\n",
    "    img = Image.open(input_path)\n",
    "    width, height = img.size\n",
    "    \n",
    "    # 裁剪参数调整\n",
    "    top = int(height / 2.6)\n",
    "    bottom = height - top\n",
    "    # 左右裁剪参数调整，剪切20%\n",
    "    left = int(width * 0.23)\n",
    "    right = width - int(width * 0.18) \n",
    "    \n",
    "    # 应用裁剪\n",
    "    img_cropped = img.crop((left, top, right, bottom))\n",
    "    img_cropped.save(output_path)\n",
    "\n",
    "input_paths = [\"C:/Users/Administrator/Desktop/Draw/uncertainb.jpeg\", \"C:/Users/Administrator/Desktop/Draw/uncertaina.jpeg\"]\n",
    "output_paths = [\"C:/Users/Administrator/Desktop/Draw/uncertainb_cropped.jpeg\", \"C:/Users/Administrator/Desktop/Draw/uncertaina_cropped.jpeg\"]\n",
    "\n",
    "for input_path, output_path in zip(input_paths, output_paths):\n",
    "    crop_image(input_path, output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (绘图) 全域最小不确定度图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.gridspec as gridspec\n",
    "from shapely.geometry import Point\n",
    "from matplotlib import rcParams\n",
    "import geopandas as gpd\n",
    "import seaborn as sns\n",
    "from matplotlib.font_manager import FontProperties\n",
    "import matplotlib\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from collections import Counter\n",
    "from matplotlib.legend_handler import HandlerTuple\n",
    "from matplotlib.lines import Line2D\n",
    "from matplotlib.patches import Patch\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "\n",
    "sns.set(style=\"whitegrid\", font='Times New Roman')\n",
    "matplotlib.rcParams['font.family'] = 'Times New Roman'\n",
    "#matplotlib.rcParams['axes.labelweight'] = 'bold'\n",
    "legend_font = FontProperties(family='Times New Roman', size=14)\n",
    "font_prop = FontProperties(family='Times New Roman', size=16)\n",
    "def add_degree_E(x, pos):\n",
    "    return f\"{int(x)}°E\"\n",
    "\n",
    "def add_degree_N(y, pos):\n",
    "    return f\"{int(y)}°N\"\n",
    "\n",
    "def create_and_draw_subplots(model_ranks, lon_range, lat_range, selected_models,name):\n",
    "    # 创建图形对象,绘制顺序依赖于fig定义顺序\n",
    "    colors = ['#afd189', '#ffd60a', '#62e3dc', '#2a6858', '#1f7ea1', '#2b0000', '#E0E0E0', '#62e3dc', '#2a6858', '#1f7ea1', '#2b0000', '#E0E0E0']\n",
    "    fig = plt.figure(figsize=(14, 8))\n",
    "    ax1 = fig.add_axes([0.146, 0.4012, 0.510, 0.14]) # 左上\n",
    "    ax2 = fig.add_axes([0.715, 0.400, 0.060, 0.15]) # 右上\n",
    "    ax4 = fig.add_axes([0.655, 0.05, 0.12, 0.35]) # 右下\n",
    "    ax3 = fig.add_axes([0.05, 0.05, 0.7, 0.35]) # 左下\n",
    "\n",
    "    draw_subplot1(ax1, colors, model_ranks, selected_models)\n",
    "    draw_subplot2(ax2, model_ranks, selected_models, colors)\n",
    "    draw_subplot4(ax4, colors, model_ranks, selected_models)\n",
    "    draw_subplot3(ax3, colors, model_ranks, lon_range, lat_range, selected_models,name)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# 折线图（上）\n",
    "def draw_subplot1(ax, colors, model_ranks, selected_models):\n",
    "    _, lons = best_models.shape\n",
    "    x = np.arange(lons)\n",
    "    row_list = np.zeros((lons, len(selected_models)), dtype=int)\n",
    "    for i in range(lons):\n",
    "        # 排除NaN值\n",
    "        valid_indices = model_ranks[:, i][~np.isnan(model_ranks[:, i])]\n",
    "        model_counts = Counter(valid_indices.astype(int))\n",
    "        for model_idx, count in model_counts.items():\n",
    "            # 确保model_idx在selected_models的索引范围内\n",
    "            if 0 < model_idx <= len(selected_models):\n",
    "                row_list[i, model_idx - 1] = count\n",
    "    \n",
    "    for i, model in enumerate(selected_models):\n",
    "        if i < len(colors):\n",
    "            ax.plot(x, row_list[:, i], linestyle='-', color=colors[i], linewidth=1, zorder=1, label=model)\n",
    "    \n",
    "    ax.set_xlim(0, lons)\n",
    "    ax.tick_params(axis='x', which='both', colors='black', length=3, width=1.5, labelsize=14, bottom=False, top=False, labelbottom=False, labeltop=False)\n",
    "    ax.tick_params(axis='y', which='both', colors='black', length=3, width=1.5, labelsize=14, direction='out', pad=6, left=True, right=False, labelright=False)\n",
    "    plt.setp(ax.get_yticklabels(), rotation=90,ha=\"center\", rotation_mode=\"anchor\")\n",
    "    ax.xaxis.set_major_locator(MaxNLocator(prune='both', nbins=4))\n",
    "    ax.set_ylabel('Count',size=18)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['bottom'].set_color('black')\n",
    "    ax.spines['bottom'].set_linewidth(1.5)\n",
    "    ax.spines['left'].set_color('black')\n",
    "    ax.spines['left'].set_linewidth(1.5)\n",
    "    ax.grid(False)\n",
    "\n",
    "# 折线图（右）\n",
    "def draw_subplot4(ax, colors, model_ranks, selected_models):\n",
    "    lats, _ = model_ranks.shape\n",
    "    x = np.arange(lats)\n",
    "    row_list = np.zeros((lats, len(selected_models)), dtype=int)\n",
    "\n",
    "    for i in range(lats):\n",
    "        valid_indices = model_ranks[i, :][~np.isnan(model_ranks[i, :])]\n",
    "        model_counts = Counter(valid_indices.astype(int))\n",
    "        for model_idx, count in model_counts.items():\n",
    "            if 0 < model_idx <= len(selected_models):\n",
    "                row_list[i, model_idx - 1] = count\n",
    "\n",
    "    for i, model in enumerate(selected_models):\n",
    "        if i < len(colors):\n",
    "            ax.plot(row_list[:, i], x, linestyle='-', color=colors[i], linewidth=1, zorder=1, label=model)\n",
    "    ax.invert_yaxis()\n",
    "    ax.set_ylim(0, lats)\n",
    "    ax.tick_params(axis='x', which='both', colors='black', length=3, width=1.5, labelsize=14, direction='out', pad=3, bottom=True, top=False, labelbottom=True)\n",
    "    ax.tick_params(axis='y', which='both', colors='black', length=3, width=1.5, labelsize=14, direction='in', left=False, right=False, labelright=False,labelleft=False)\n",
    "    ax.xaxis.set_major_locator(MaxNLocator(nbins=4))\n",
    "    ax.yaxis.set_major_locator(MaxNLocator(nbins=5))\n",
    "    ax.set_xlabel('Count',size=18)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['bottom'].set_color('black')\n",
    "    ax.spines['bottom'].set_linewidth(1.5)\n",
    "    ax.spines['left'].set_color('black')\n",
    "    ax.spines['left'].set_linewidth(1.5)\n",
    "    ax.grid(False)\n",
    "\n",
    "# 图例\n",
    "'''\n",
    "def draw_subplot2(ax, model_ranks, selected_models, colors):\n",
    "    line_handles = [Line2D([0], [0], color=color, linewidth=3) for color in colors]\n",
    "    patch_handles = [Patch(color=color) for color in colors]\n",
    "    legend_elements = [(line, patch) for line, patch in zip(line_handles, patch_handles)]\n",
    "    ax.legend(legend_elements, selected_models, handler_map={tuple: HandlerTuple(ndivide=None)}, handlelength=1.5, loc='center',fancybox=False, shadow=False, ncol=2, frameon=False, prop=legend_font, columnspacing=-0.3, handletextpad=0.5, labelspacing=0.7)\n",
    "    ax.axis('off')\n",
    "'''\n",
    "def draw_subplot2(ax, model_ranks, selected_models, colors):\n",
    "    total_points = (~np.isnan(model_ranks)).sum()\n",
    "    model_counts = Counter(model_ranks.flatten()) \n",
    "    percentages = [model_counts.get(i, 0) / total_points for i in range(1, len(selected_models) + 1)]\n",
    "    bar_width = 0.6\n",
    "    for i, model in enumerate(selected_models):\n",
    "        if i < len(colors):\n",
    "            ax.barh(i+bar_width/2, percentages[i], color=colors[i], height=bar_width)\n",
    "            pct_text = \"{:.1%}\".format(percentages[i])\n",
    "            ax.text(percentages[i] + 0.01, i + bar_width / 2, pct_text, va='center',fontproperties=font_prop, size=14)\n",
    "\n",
    "    ax.set_yticks(np.arange(len(selected_models)) + bar_width / 2)\n",
    "    yticklabels = ax.set_yticklabels(selected_models)\n",
    "    \n",
    "    # Set properties individually\n",
    "    for label, model in zip(yticklabels, selected_models):\n",
    "        if model in ['PSC-FENet-ST', 'PSC-FENet-SP']:\n",
    "            label.set_fontproperties(FontProperties(family='Times New Roman', size=10))\n",
    "        else:\n",
    "            label.set_fontproperties(FontProperties(family='Times New Roman', size=16))\n",
    "\n",
    "    ax.set_xlabel('Percentage')\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['left'].set_visible(True)\n",
    "    ax.spines['bottom'].set_visible(False)\n",
    "    ax.tick_params('y', colors='black', length=3, width=1.5, direction='out',pad=2, which='both', left=True, right=False)\n",
    "    ax.tick_params(axis='x', which='both', bottom=False, top=False, labeltop=False)\n",
    "    ax.set_facecolor('none')\n",
    "    ax.axis('on')\n",
    "    ax.grid(False)\n",
    "\n",
    "def draw_subplot3(ax, colors, model_ranks, lon_range, lat_range, selected_models,name):\n",
    "    color_list = [colors[i] for i in range(len(selected_models))]\n",
    "    # 颜色映射\n",
    "    cmap = mcolors.ListedColormap(color_list)\n",
    "    gdf = gpd.read_file(shp_path)\n",
    "    gdf.plot(ax=ax, edgecolor='black', facecolor='none', linewidth=2)\n",
    "    image = ax.imshow(model_ranks, cmap=cmap, origin='lower', extent=[new_lon_min, new_lon_max, new_lat_min, new_lat_max])\n",
    "    ax.set_xlabel('Longitude(°)',size=18)\n",
    "    ax.set_ylabel('Latitude(°)',size=18)\n",
    "    ax.xaxis.set_major_locator(MaxNLocator(nbins=10))\n",
    "    ax.yaxis.set_major_locator(MaxNLocator(nbins=6))\n",
    "    ax.xaxis.set_major_formatter(FuncFormatter(add_degree_E))\n",
    "    ax.yaxis.set_major_formatter(FuncFormatter(add_degree_N))\n",
    "    ax.tick_params(axis='x', which='both', colors='black', length=3, width=1.5, labelsize=14, direction='out', pad=2, bottom=True, top=True, labelbottom=True,labeltop=False)\n",
    "    ax.tick_params(axis='y', which='both', colors='black', length=3, width=1.5, labelsize=14, direction='out', pad=6, left=True, right=True, labelleft=True,labelright=False)\n",
    "    plt.setp(ax.get_xticklabels(), rotation=0,rotation_mode=\"anchor\")\n",
    "    plt.setp(ax.get_yticklabels(), rotation=90,ha=\"center\", rotation_mode=\"anchor\")\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['bottom'].set_color('black')\n",
    "    ax.spines['bottom'].set_linewidth(1.5)\n",
    "    ax.spines['left'].set_color('black')\n",
    "    ax.spines['left'].set_linewidth(1.5)\n",
    "    ax.grid(False)\n",
    "    #ax.grid(True, color='black', linestyle='--',alpha=0.5,zorder=0.1)\n",
    "    plt.savefig(r\"C:\\Users\\Administrator\\Desktop\\Draw\\min_uncertain\"+name+\".jpeg\", dpi=1000, format='jpeg', bbox_inches='tight')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "colors = ['#afd189', '#88352b', '#2a6858', '#62e3dc', '#1f7ea1', '#2b0000', '#F0F0F0', '#62e3dc', '#1f7ea1', '#2b0000', '#F0F0F0', '#2b0000', '#F0F0F0']\n",
    "create_and_draw_subplots(clipped_data, lon_range, lat_range, selected_model_names, name='a')\n",
    "create_and_draw_subplots(clipped_data_12, lon_range, lat_range, selected_model_names, name='b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.gridspec as gridspec\n",
    "from shapely.geometry import Point\n",
    "from matplotlib import rcParams\n",
    "import geopandas as gpd\n",
    "import seaborn as sns\n",
    "from matplotlib.font_manager import FontProperties\n",
    "import matplotlib\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from collections import Counter\n",
    "from matplotlib.legend_handler import HandlerTuple\n",
    "from matplotlib.lines import Line2D\n",
    "from matplotlib.patches import Patch\n",
    "from matplotlib.ticker import FixedLocator, FixedFormatter\n",
    "\n",
    "sns.set(style=\"whitegrid\", font='Times New Roman')\n",
    "matplotlib.rcParams['font.family'] = 'Times New Roman'\n",
    "#matplotlib.rcParams['axes.labelweight'] = 'bold'\n",
    "legend_font = FontProperties(family='Times New Roman', size=14)\n",
    "font_prop = FontProperties(family='Times New Roman', size=16)\n",
    "def add_degree_E(x, pos):\n",
    "    return f\"{int(x)}°E\"\n",
    "\n",
    "def add_degree_N(y, pos):\n",
    "    return f\"{int(y)}°N\"\n",
    "\n",
    "def create_and_draw_subplots(model_ranks, lon_range, lat_range, selected_models,name):\n",
    "    # 创建图形对象,绘制顺序依赖于fig定义顺序\n",
    "    colors = ['#afd189', '#ffd60a', '#62e3dc', '#2a6858', '#1f7ea1', '#2b0000', '#E0E0E0', '#62e3dc', '#2a6858', '#1f7ea1', '#2b0000', '#E0E0E0']\n",
    "    fig = plt.figure(figsize=(14, 8))\n",
    "    ax1 = fig.add_axes([0.146, 0.4012, 0.510, 0.14]) # 左上\n",
    "    ax2 = fig.add_axes([0.715, 0.400, 0.060, 0.15]) # 右上\n",
    "    ax4 = fig.add_axes([0.655, 0.05, 0.12, 0.35]) # 右下\n",
    "    ax3 = fig.add_axes([0.05, 0.05, 0.7, 0.35]) # 左下\n",
    "\n",
    "    draw_subplot1(ax1, colors, model_ranks, selected_models)\n",
    "    draw_subplot2(ax2, model_ranks, selected_models, colors)\n",
    "    draw_subplot4(ax4, colors, model_ranks, selected_models)\n",
    "    draw_subplot3(ax3, colors, model_ranks, lon_range, lat_range, selected_models,name)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# 折线图（上）\n",
    "def draw_subplot1(ax, colors, model_ranks, selected_models):\n",
    "    _, lons = best_models.shape\n",
    "    x = np.arange(lons)\n",
    "    row_list = np.zeros((lons, len(selected_models)), dtype=int)\n",
    "    for i in range(lons):\n",
    "        valid_indices = model_ranks[:, i][~np.isnan(model_ranks[:, i])]\n",
    "        total_valid = len(valid_indices)\n",
    "        if total_valid > 0:\n",
    "            model_counts = Counter(valid_indices.astype(int))\n",
    "        \n",
    "            for model_idx, count in model_counts.items():\n",
    "                if 0 < model_idx <= len(selected_models):\n",
    "                    row_list[i, model_idx - 1] = (count / total_valid) * 100\n",
    "        else:\n",
    "            row_list[i, :] = 0\n",
    "    for i, model in enumerate(selected_models):\n",
    "        if i < len(colors):\n",
    "            ax.plot(x, row_list[:, i], linestyle='-', color=colors[i], linewidth=1, zorder=1, label=model)\n",
    "    \n",
    "    ax.set_xlim(0, lons)\n",
    "    ax.tick_params(axis='x', which='both', colors='black', length=3, width=1.5, labelsize=14, bottom=False, top=False, labelbottom=False, labeltop=False)\n",
    "    ax.tick_params(axis='y', which='both', colors='black', length=3, width=1.5, labelsize=14, direction='out', pad=6, left=True, right=False, labelright=False)\n",
    "    plt.setp(ax.get_yticklabels(), rotation=90,ha=\"center\", rotation_mode=\"anchor\")\n",
    "    ax.yaxis.set_major_locator(FixedLocator([lats * 0.25, lats * 0.5, lats * 0.75, lats - 1]))\n",
    "    ax.yaxis.set_major_formatter(FixedFormatter(['25%', '50%', '75%', '100%']))\n",
    "    ax.xaxis.set_major_locator(MaxNLocator(prune='both', nbins=4))\n",
    "    ax.set_ylabel('Count',size=18)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['bottom'].set_color('black')\n",
    "    ax.spines['bottom'].set_linewidth(1.5)\n",
    "    ax.spines['left'].set_color('black')\n",
    "    ax.spines['left'].set_linewidth(1.5)\n",
    "    ax.grid(False)\n",
    "\n",
    "# 折线图（右）\n",
    "def draw_subplot4(ax, colors, model_ranks, selected_models):\n",
    "    lats, _ = model_ranks.shape\n",
    "    x = np.arange(lats)\n",
    "    row_list = np.zeros((lats, len(selected_models)), dtype=float)  # 存储百分比，使用float类型\n",
    "\n",
    "    for i in range(lats):\n",
    "        valid_indices = model_ranks[i, :][~np.isnan(model_ranks[i, :])]\n",
    "        total_valid = len(valid_indices)\n",
    "        model_counts = Counter(valid_indices.astype(int)) if total_valid > 0 else {}\n",
    "        \n",
    "        for model_idx, count in model_counts.items():\n",
    "            if 0 < model_idx <= len(selected_models):\n",
    "                # 转换为百分比\n",
    "                row_list[i, model_idx - 1] = (count / total_valid * 100) if total_valid > 0 else 0\n",
    "\n",
    "    for i, model in enumerate(selected_models):\n",
    "        if i < len(colors):\n",
    "            ax.plot(row_list[:, i],x, linestyle='-', color=colors[i], linewidth=1, zorder=1, label=model)\n",
    "\n",
    "    ax.invert_yaxis()\n",
    "    ax.set_ylim(0, lats)\n",
    "\n",
    "    ax.tick_params(axis='x', which='both', colors='black', length=3, width=1.5, labelsize=14, direction='out', pad=3, bottom=True, top=False, labelbottom=True)\n",
    "    ax.tick_params(axis='y', which='both', colors='black', length=3, width=1.5, labelsize=14, direction='in', left=False, right=False, labelright=False,labelleft=False)\n",
    "    ax.xaxis.set_major_locator(FixedLocator([lats * 0.25, lats * 0.5, lats * 0.75, lats - 1]))\n",
    "    ax.xaxis.set_major_formatter(FixedFormatter(['25%', '50%', '75%', '100%']))\n",
    "    ax.yaxis.set_major_locator(MaxNLocator(nbins=5))\n",
    "    ax.set_xlabel('Count',size=18)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['bottom'].set_color('black')\n",
    "    ax.spines['bottom'].set_linewidth(1.5)\n",
    "    ax.spines['left'].set_color('black')\n",
    "    ax.spines['left'].set_linewidth(1.5)\n",
    "    ax.grid(False)\n",
    "\n",
    "# 图例\n",
    "'''\n",
    "def draw_subplot2(ax, model_ranks, selected_models, colors):\n",
    "    line_handles = [Line2D([0], [0], color=color, linewidth=3) for color in colors]\n",
    "    patch_handles = [Patch(color=color) for color in colors]\n",
    "    legend_elements = [(line, patch) for line, patch in zip(line_handles, patch_handles)]\n",
    "    ax.legend(legend_elements, selected_models, handler_map={tuple: HandlerTuple(ndivide=None)}, handlelength=1.5, loc='center',fancybox=False, shadow=False, ncol=2, frameon=False, prop=legend_font, columnspacing=-0.3, handletextpad=0.5, labelspacing=0.7)\n",
    "    ax.axis('off')\n",
    "'''\n",
    "def draw_subplot2(ax, model_ranks, selected_models, colors):\n",
    "    total_points = (~np.isnan(model_ranks)).sum()\n",
    "    model_counts = Counter(model_ranks.flatten()) \n",
    "    percentages = [model_counts.get(i, 0) / total_points for i in range(1, len(selected_models) + 1)]\n",
    "    bar_width = 0.6\n",
    "    for i, model in enumerate(selected_models):\n",
    "        if i < len(colors):\n",
    "            ax.barh(i+bar_width/2, percentages[i], color=colors[i], height=bar_width)\n",
    "            pct_text = \"{:.1%}\".format(percentages[i])\n",
    "            ax.text(percentages[i] + 0.01, i + bar_width / 2, pct_text, va='center',fontproperties=font_prop, size=14)\n",
    "\n",
    "    ax.set_yticks(np.arange(len(selected_models)) + bar_width / 2)\n",
    "    yticklabels = ax.set_yticklabels(selected_models)\n",
    "    \n",
    "    # Set properties individually\n",
    "    for label, model in zip(yticklabels, selected_models):\n",
    "        if model in ['PSC-FENet-ST', 'PSC-FENet-SP']:\n",
    "            label.set_fontproperties(FontProperties(family='Times New Roman', size=10))\n",
    "        else:\n",
    "            label.set_fontproperties(FontProperties(family='Times New Roman', size=16))\n",
    "\n",
    "    ax.set_xlabel('Percentage')\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['left'].set_visible(True)\n",
    "    ax.spines['bottom'].set_visible(False)\n",
    "    ax.tick_params('y', colors='black', length=3, width=1.5, direction='out',pad=2, which='both', left=True, right=False)\n",
    "    ax.tick_params(axis='x', which='both', bottom=False, top=False, labeltop=False)\n",
    "    ax.set_facecolor('none')\n",
    "    ax.axis('on')\n",
    "    ax.grid(False)\n",
    "\n",
    "def draw_subplot3(ax, colors, model_ranks, lon_range, lat_range, selected_models,name):\n",
    "    color_list = [colors[i] for i in range(len(selected_models))]\n",
    "    # 颜色映射\n",
    "    cmap = mcolors.ListedColormap(color_list)\n",
    "    gdf = gpd.read_file(shp_path)\n",
    "    gdf.plot(ax=ax, edgecolor='black', facecolor='none', linewidth=2)\n",
    "    image = ax.imshow(model_ranks, cmap=cmap, origin='lower', extent=[new_lon_min, new_lon_max, new_lat_min, new_lat_max])\n",
    "    ax.set_xlabel('Longitude(°)',size=18)\n",
    "    ax.set_ylabel('Latitude(°)',size=18)\n",
    "    ax.xaxis.set_major_locator(MaxNLocator(nbins=10))\n",
    "    ax.yaxis.set_major_locator(MaxNLocator(nbins=6))\n",
    "    ax.xaxis.set_major_formatter(FuncFormatter(add_degree_E))\n",
    "    ax.yaxis.set_major_formatter(FuncFormatter(add_degree_N))\n",
    "    ax.tick_params(axis='x', which='both', colors='black', length=3, width=1.5, labelsize=14, direction='out', pad=2, bottom=True, top=True, labelbottom=True,labeltop=False)\n",
    "    ax.tick_params(axis='y', which='both', colors='black', length=3, width=1.5, labelsize=14, direction='out', pad=6, left=True, right=True, labelleft=True,labelright=False)\n",
    "    plt.setp(ax.get_xticklabels(), rotation=0,rotation_mode=\"anchor\")\n",
    "    plt.setp(ax.get_yticklabels(), rotation=90,ha=\"center\", rotation_mode=\"anchor\")\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['bottom'].set_color('black')\n",
    "    ax.spines['bottom'].set_linewidth(1.5)\n",
    "    ax.spines['left'].set_color('black')\n",
    "    ax.spines['left'].set_linewidth(1.5)\n",
    "    ax.grid(False)\n",
    "    #ax.grid(True, color='black', linestyle='--',alpha=0.5,zorder=0.1)\n",
    "    plt.savefig(r\"C:\\Users\\Administrator\\Desktop\\Draw\\min_uncertain2\"+name+\".jpeg\", dpi=1000, format='jpeg', bbox_inches='tight')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "colors = ['#afd189', '#88352b', '#2a6858', '#62e3dc', '#1f7ea1', '#2b0000', '#F0F0F0', '#62e3dc', '#1f7ea1', '#2b0000', '#F0F0F0', '#2b0000', '#F0F0F0']\n",
    "create_and_draw_subplots(clipped_data, lon_range, lat_range, selected_model_names, name='a')\n",
    "create_and_draw_subplots(clipped_data_12, lon_range, lat_range, selected_model_names, name='b')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## colorbar测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "colors = [\"#ffc43d\", \"#60d394\"] \n",
    "cmap_name = \"custom_orange_green\"\n",
    "cm = LinearSegmentedColormap.from_list(cmap_name, colors, N=100)\n",
    "\n",
    "gradient = np.linspace(0, 1, 256)\n",
    "gradient = np.vstack((gradient, gradient))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 2))\n",
    "ax.imshow(gradient, aspect='auto', cmap=cm)\n",
    "ax.set_axis_off()\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
