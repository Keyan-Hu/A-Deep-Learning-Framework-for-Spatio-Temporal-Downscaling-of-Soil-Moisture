{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m_pydevd_bundle/pydevd_cython.pyx:1788\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.ThreadTracer.__call__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'c:\\\\Anaconda\\\\envs\\\\MLL\\\\lib\\\\site-packages\\\\torch\\\\optim\\\\swa_utils.py'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Anaconda\\envs\\MLL\\lib\\site-packages\\debugpy\\_vendored\\pydevd\\pydevd_file_utils.py:912\u001b[0m, in \u001b[0;36mget_abs_path_real_path_and_base_from_frame\u001b[1;34m(frame, NORM_PATHS_AND_BASE_CONTAINER)\u001b[0m\n\u001b[0;32m    911\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 912\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mNORM_PATHS_AND_BASE_CONTAINER\u001b[49m\u001b[43m[\u001b[49m\u001b[43mframe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf_code\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mco_filename\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m    913\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m    914\u001b[0m     \u001b[38;5;66;03m# This one is just internal (so, does not need any kind of client-server translation)\u001b[39;00m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'c:\\\\Anaconda\\\\envs\\\\MLL\\\\lib\\\\site-packages\\\\torch\\\\optim\\\\swa_utils.py'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Anaconda\\envs\\MLL\\lib\\site-packages\\debugpy\\_vendored\\pydevd\\pydevd_file_utils.py:876\u001b[0m, in \u001b[0;36mget_abs_path_real_path_and_base_from_file\u001b[1;34m(filename, NORM_PATHS_AND_BASE_CONTAINER)\u001b[0m\n\u001b[0;32m    875\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 876\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mNORM_PATHS_AND_BASE_CONTAINER\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n",
      "\u001b[1;31mKeyError\u001b[0m: 'c:\\\\Anaconda\\\\envs\\\\MLL\\\\lib\\\\site-packages\\\\torch\\\\optim\\\\swa_utils.py'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Anaconda\\envs\\MLL\\lib\\site-packages\\debugpy\\_vendored\\pydevd\\pydevd_file_utils.py:430\u001b[0m, in \u001b[0;36m_abs_and_canonical_path\u001b[1;34m(filename, NORM_PATHS_CONTAINER)\u001b[0m\n\u001b[0;32m    429\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 430\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mNORM_PATHS_CONTAINER\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m    431\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n",
      "\u001b[1;31mKeyError\u001b[0m: 'c:\\\\Anaconda\\\\envs\\\\MLL\\\\lib\\\\site-packages\\\\torch\\\\optim\\\\swa_utils.py'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwandb\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlogging\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01margparse\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Anaconda\\envs\\MLL\\lib\\site-packages\\torch\\__init__.py:1429\u001b[0m\n\u001b[0;32m   1427\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m nn \u001b[38;5;28;01mas\u001b[39;00m nn\n\u001b[0;32m   1428\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msignal\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m windows \u001b[38;5;28;01mas\u001b[39;00m windows\n\u001b[1;32m-> 1429\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m optim \u001b[38;5;28;01mas\u001b[39;00m optim\n\u001b[0;32m   1430\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptim\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_multi_tensor\u001b[39;00m\n\u001b[0;32m   1431\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m multiprocessing \u001b[38;5;28;01mas\u001b[39;00m multiprocessing\n",
      "File \u001b[1;32mc:\\Anaconda\\envs\\MLL\\lib\\site-packages\\torch\\optim\\__init__.py:23\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlbfgs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LBFGS\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m lr_scheduler\n\u001b[1;32m---> 23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m swa_utils\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m adadelta\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m adagrad\n",
      "File \u001b[1;32mc:\\Anaconda\\envs\\MLL\\lib\\site-packages\\torch\\optim\\swa_utils.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mitertools\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmath\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcopy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m deepcopy\n",
      "File \u001b[1;32m_pydevd_bundle/pydevd_cython.pyx:1457\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.SafeCallWrapper.__call__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m_pydevd_bundle/pydevd_cython.pyx:1790\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.ThreadTracer.__call__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Anaconda\\envs\\MLL\\lib\\site-packages\\debugpy\\_vendored\\pydevd\\pydevd_file_utils.py:930\u001b[0m, in \u001b[0;36mget_abs_path_real_path_and_base_from_frame\u001b[1;34m(frame, NORM_PATHS_AND_BASE_CONTAINER)\u001b[0m\n\u001b[0;32m    927\u001b[0m     i \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(f\u001b[38;5;241m.\u001b[39mrfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m), f\u001b[38;5;241m.\u001b[39mrfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m    928\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f, f, f[i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m:]\n\u001b[1;32m--> 930\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[43mget_abs_path_real_path_and_base_from_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    931\u001b[0m \u001b[38;5;66;03m# Also cache based on the frame.f_code.co_filename (if we had it inside build/bdist it can make a difference).\u001b[39;00m\n\u001b[0;32m    932\u001b[0m NORM_PATHS_AND_BASE_CONTAINER[frame\u001b[38;5;241m.\u001b[39mf_code\u001b[38;5;241m.\u001b[39mco_filename] \u001b[38;5;241m=\u001b[39m ret\n",
      "File \u001b[1;32mc:\\Anaconda\\envs\\MLL\\lib\\site-packages\\debugpy\\_vendored\\pydevd\\pydevd_file_utils.py:897\u001b[0m, in \u001b[0;36mget_abs_path_real_path_and_base_from_file\u001b[1;34m(filename, NORM_PATHS_AND_BASE_CONTAINER)\u001b[0m\n\u001b[0;32m    894\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m f\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m$py.class\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m    895\u001b[0m         f \u001b[38;5;241m=\u001b[39m f[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m$py.class\u001b[39m\u001b[38;5;124m'\u001b[39m)] \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.py\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m--> 897\u001b[0m abs_path, canonical_normalized_filename \u001b[38;5;241m=\u001b[39m \u001b[43m_abs_and_canonical_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    899\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    900\u001b[0m     base \u001b[38;5;241m=\u001b[39m os_path_basename(canonical_normalized_filename)\n",
      "File \u001b[1;32mc:\\Anaconda\\envs\\MLL\\lib\\site-packages\\debugpy\\_vendored\\pydevd\\pydevd_file_utils.py:456\u001b[0m, in \u001b[0;36m_abs_and_canonical_path\u001b[1;34m(filename, NORM_PATHS_CONTAINER)\u001b[0m\n\u001b[0;32m    453\u001b[0m abs_path \u001b[38;5;241m=\u001b[39m _apply_func_and_normalize_case(filename, os_path_abspath, isabs, normalize)\n\u001b[0;32m    455\u001b[0m normalize \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 456\u001b[0m real_path \u001b[38;5;241m=\u001b[39m \u001b[43m_apply_func_and_normalize_case\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mos_path_real_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43misabs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    458\u001b[0m \u001b[38;5;66;03m# cache it for fast access later\u001b[39;00m\n\u001b[0;32m    459\u001b[0m NORM_PATHS_CONTAINER[filename] \u001b[38;5;241m=\u001b[39m abs_path, real_path\n",
      "File \u001b[1;32mc:\\Anaconda\\envs\\MLL\\lib\\site-packages\\debugpy\\_vendored\\pydevd\\pydevd_file_utils.py:483\u001b[0m, in \u001b[0;36m_apply_func_and_normalize_case\u001b[1;34m(filename, func, isabs, normalize_case, os_path_exists, join)\u001b[0m\n\u001b[0;32m    478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filename\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m    479\u001b[0m     \u001b[38;5;66;03m# Not really a file, rather a synthetic name like <string> or <ipython-...>;\u001b[39;00m\n\u001b[0;32m    480\u001b[0m     \u001b[38;5;66;03m# shouldn't be normalized.\u001b[39;00m\n\u001b[0;32m    481\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m filename\n\u001b[1;32m--> 483\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    485\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m isabs:\n\u001b[0;32m    486\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os_path_exists(r):\n",
      "File \u001b[1;32mc:\\Anaconda\\envs\\MLL\\lib\\ntpath.py:689\u001b[0m, in \u001b[0;36mrealpath\u001b[1;34m(path, strict)\u001b[0m\n\u001b[0;32m    687\u001b[0m     path \u001b[38;5;241m=\u001b[39m join(cwd, path)\n\u001b[0;32m    688\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 689\u001b[0m     path \u001b[38;5;241m=\u001b[39m \u001b[43m_getfinalpathname\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    690\u001b[0m     initial_winerror \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    691\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ex:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import wandb\n",
    "import logging\n",
    "import torch\n",
    "import argparse\n",
    "import numpy as np\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import sys\n",
    "sys.path.append(r\"C:\\Users\\Administrator\\Desktop\\code\\ST-Conv\")\n",
    "from dataset.dataset import STConvDataset\n",
    "from log.logger import setup_logger\n",
    "from train.train_val import test_apply,train_apply,val_apply\n",
    "from tool.utils import Util,ModelManager\n",
    "from metrics.metrics import CustomMetricCollection, CustomRMSELoss, CombinedLoss\n",
    "from models.model import STConvNet\n",
    "from models.block import BasicBlock,Bottleneck\n",
    "\n",
    "os.environ[\"WANDB_MODE\"] = 'offline'\n",
    "##1!!!\n",
    "def load_data(pre_train=True, train=True, seed=42, batch_size=256, missing_rate_threshold=1, **kwargs):\n",
    "    half_day_data = np.load(kwargs['Half_Day_data_dir'], allow_pickle=True)\n",
    "    #samples time_steps lat lon channels\n",
    "    central_index = half_day_data.shape[1] // 2\n",
    "    missing_rates = np.mean(half_day_data[:, central_index, :, :, 0] == 0, axis=(1, 2))\n",
    "    valid_indices = np.where(missing_rates < missing_rate_threshold)[0]\n",
    "\n",
    "    static_data = np.load(kwargs['Static_data_dir'], allow_pickle=True)[valid_indices]\n",
    "    day_data = np.load(kwargs['Day_data_dir'], allow_pickle=True)[valid_indices]\n",
    "    hour_data = np.load(kwargs['Hour_data_dir'], allow_pickle=True)[valid_indices]\n",
    "    half_day_data = half_day_data[valid_indices]\n",
    "\n",
    "    dataset_size = len(valid_indices)\n",
    "\n",
    "    train_indices, val_indices, test_indices = Util.split_dataset_indices(dataset_size, seed=seed, train_ratio=kwargs.get('train_ratio', 0.7), val_ratio=kwargs.get('val_ratio', 0.15), test_ratio=kwargs.get('test_ratio', 0.15))\n",
    "\n",
    "    train_dataset = STConvDataset(static_data, day_data, half_day_data, hour_data, pre_train=pre_train, train=train, \n",
    "                                  seed=seed, indices=train_indices, mode='train', **kwargs)\n",
    "    val_dataset = STConvDataset(static_data, day_data, half_day_data, hour_data, pre_train=pre_train, train=False, \n",
    "                                seed=seed, indices=val_indices, mode='val', **kwargs)\n",
    "    test_dataset = STConvDataset(static_data, day_data, half_day_data, hour_data, pre_train=False, train=False, \n",
    "                                 seed=seed, indices=test_indices, mode='test', **kwargs)\n",
    "\n",
    "    loader_args = dict(batch_size=batch_size, num_workers=kwargs.get('num_workers', 1), prefetch_factor=kwargs.get('prefetch_factor', 2), persistent_workers=kwargs.get('persistent_workers', False))\n",
    "    train_loader = DataLoader(train_dataset, shuffle=True, drop_last=True, **loader_args)\n",
    "    val_loader = DataLoader(val_dataset, shuffle=False, drop_last=False, **loader_args)\n",
    "    test_loader = DataLoader(test_dataset, shuffle=False, drop_last=False, **loader_args)\n",
    "    del static_data, day_data, half_day_data, hour_data\n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "def train(project_name:str='ST-Conv',use_freeze_decoder:bool=True,spatial_name:str=None,usage_model:str=None,train_dict:dict=None,logger=None,model=None,criterion=CombinedLoss(),wandb=wandb,       # necessary parameter\n",
    "          checkpoint_path:str=None,best_model_path:str=None,train_loader=None,val_loader=None,test_loader=None,                                                                 # Path & Dataloader\n",
    "          algorithm_type ='normal',mean=None,std=None,train:bool=True,                                                                                                          # model parameter\n",
    "          learning_rate:float=1e-3,weight_decay:float=1e-6,warm_up_step:int=1000,epochs:int=100,evaluate_epoch:int=0,test_epoch:int=10,amp:bool=True,**kwargs):                 # train parameter\n",
    "    \n",
    "    # 设定设备\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    std = torch.tensor(np.load(std,allow_pickle=True), dtype=torch.float)[0].to(device)\n",
    "    mean = torch.tensor(np.load(mean,allow_pickle=True), dtype=torch.float)[0].to(device)\n",
    "    model = model.to(device)\n",
    "\n",
    "    # 设置优化器\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)  # 优化器\n",
    "    warmup_lr = np.arange(1e-7, learning_rate, (learning_rate - 1e-7) / warm_up_step)\n",
    "    grad_scaler = torch.cuda.amp.GradScaler() if amp else None\n",
    "\n",
    "    # 加载模型、指标收集器、最佳指标字典、无改进步计数、模型管理器、总步数\n",
    "    Util.load_model_and_optimizer(model, optimizer, device, best_model_path, logger=logger, model_index=-1,model_name=project_name)\n",
    "    metric_collection = CustomMetricCollection()\n",
    "    best_metrics = dict.fromkeys(['best_RMSE', 'best_r2','best_r','best_MAE','best_MSE','best_ubRMSE'], 0.5)  # 最佳评估指标\n",
    "    non_improved_epoch = 0\n",
    "    ModelManager.start_new_training_session()\n",
    "    total_step = 0\n",
    "    \n",
    "    # 预训练、验证、测试\n",
    "    for epoch in range(epochs):\n",
    "        train_loader.dataset.update_dataset_parameters(epoch, epochs)\n",
    "        model, optimizer, _, _ = \\\n",
    "        train_apply(model=model, dataloader=train_loader, epoch=epoch, optimizer=optimizer, grad_scaler=grad_scaler, criterion=criterion,usage_model=usage_model,\n",
    "                    metric_collection=metric_collection, device=device, std=std, mean=mean, logger=logger, wandb=wandb, total_step=total_step, \n",
    "                    warmup_lr=warmup_lr, algorithm_type=algorithm_type, use_freeze_decoder=use_freeze_decoder,spatial_name=spatial_name,train=train,**train_dict)\n",
    "        with torch.no_grad():\n",
    "            if epoch>=evaluate_epoch:\n",
    "                    model, optimizer, best_metrics, non_improved_epoch = val_apply(project_name=project_name,model=model,epoch=epoch,non_improved_epoch=non_improved_epoch,logger=logger,optimizer=optimizer,\n",
    "                            checkpoint_path=checkpoint_path,best_model_path=best_model_path,\n",
    "                            best_metrics=best_metrics,device=device,metric_collection=metric_collection,dataloader=val_loader\n",
    "                            ,wandb=wandb,std=std,mean=mean,**train_dict)\n",
    "            if  epoch>=test_epoch:\n",
    "                    test_metrics = test_apply(model=model, dataloader=test_loader, criterion=criterion,metric_collection=metric_collection, device=device,logger=logger,wandb=wandb,std=std,mean=mean)\n",
    "            \n",
    "\n",
    "def main(args):\n",
    "    models_config = Util.load_config(args.models_config_path)\n",
    "    train_config = models_config['train_shared_parameter'] \n",
    "    path_config = Util.load_config(args.path_config_path)\n",
    "    logger = setup_logger(path_config['log'],append=True)\n",
    "    return train_config, models_config, path_config, logger\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--models_config_path', type=str, default='../config/models_config.yaml')\n",
    "    parser.add_argument('--path_config_path', type=str, default='../config/path_config.yaml')\n",
    "    args = parser.parse_known_args()[0]\n",
    "\n",
    "    train_dict, models_config, path_config, logger = main(args)\n",
    "    Util.random_seed(seed=train_dict['seed'])\n",
    "    # 解包model_config\n",
    "    shared_parameters = models_config['Shared_parameter']       # 全局共享参数\n",
    "    experiment_groups = models_config['experiment_groups']      # 实验组参数-实验组共享参数+实验组模型参数\n",
    "\n",
    "    for group in experiment_groups:\n",
    "        if group['group_name'] == 'Experiment Group 3':\n",
    "            print(f\"Running experiments for group: {group['group_name']}\")\n",
    "            experiment_shared_params = group.get('experiment_shared_parameter', {})\n",
    "\n",
    "            # 如果模式是train==True,修改为False，保存为临时变量，先以false训练再以True训练\n",
    "            temp_train = False\n",
    "            if experiment_shared_params['train'] == True:\n",
    "                experiment_shared_params['train'] = False\n",
    "                experiment_shared_params['use_freeze_decoder'] = False\n",
    "                temp_train = True\n",
    "\n",
    "            # 定义损失函数、加载数据\n",
    "            criterion = CombinedLoss(**experiment_shared_params)\n",
    "            train_loader,val_loader,test_loader = load_data(**experiment_shared_params,**path_config,**train_dict)\n",
    "            # 训练实验组中每个模型\n",
    "\n",
    "            for model_config in group['models']:\n",
    "                wandb_config = {'seed': train_dict['seed'],\n",
    "                                'learning_rate': train_dict['learning_rate'],\n",
    "                                'weight_decay': train_dict['weight_decay'],\n",
    "                                'warm_up_step': train_dict['warm_up_step'],\n",
    "                                'epochs': train_dict['epochs'],\n",
    "                                'amp': train_dict['amp'],\n",
    "                                'batch_size': train_dict['batch_size']}\n",
    "                \n",
    "                wandb_config.update(model_config)\n",
    "                wandb.init(project='ST-Conv', config=wandb_config, name=model_config['model_name'], group=group['group_name'])\n",
    "                model = STConvNet(**shared_parameters, **experiment_shared_params, **model_config['parameters'])\n",
    "                train(project_name=model_config['model_name'],spatial_name = model_config['parameters']['spatial_block'],usage_model = model_config['parameters']['usage_model'],train_dict=train_dict,logger=logger,model = model,criterion=criterion,wandb=wandb,\n",
    "                                    train_loader=train_loader,val_loader=val_loader,test_loader=test_loader,**experiment_shared_params,**train_dict)\n",
    "            \n",
    "            if temp_train == True:\n",
    "                # 微调decoder训练100次\n",
    "                train_dict['epochs'] = 100\n",
    "                experiment_shared_params['train'] = True\n",
    "                experiment_shared_params['use_freeze_decoder'] = True\n",
    "                # 训练实验组中每个模型\n",
    "                for model_config in group['models']:\n",
    "                    model = STConvNet(**shared_parameters, **experiment_shared_params, **model_config['parameters'])\n",
    "                    # 设定设备\n",
    "                    if torch.cuda.device_count() > 1:\n",
    "                        print(f\"Let's use {torch.cuda.device_count()} GPUs!\")\n",
    "                    # 封装模型以在多GPU上运行\n",
    "                    model = nn.DataParallel(model)\n",
    "                    model = torch.compile(model,mode = 'max-autotune')\n",
    "                    train(project_name=model_config['model_name'],spatial_name = model_config['parameters']['spatial_block'],usage_model = model_config['parameters']['usage_model'],train_dict=train_dict,logger=logger,model = model,criterion=criterion,wandb=wandb,\n",
    "                                        train_loader=train_loader,val_loader=val_loader,test_loader=test_loader,**experiment_shared_params,**train_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
