{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据标准化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.backends.zarr.ZarrStore at 0x1b3e1b8c340>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "def normalize_data(std_dict, mean_dict, data):\n",
    "    # 提前定义分块策略\n",
    "    chunks = {'time': 1}  # 例如，这里我们将时间维度的块大小设置为1\n",
    "\n",
    "    for var in data.data_vars:\n",
    "        if var in ['lat', 'lon', 'time']:\n",
    "            continue\n",
    "        mean = mean_dict[var]\n",
    "        std = std_dict[var]\n",
    "\n",
    "        # 避免除以零\n",
    "        std = std if std != 0 else np.nan\n",
    "\n",
    "        # 为变量应用分块策略\n",
    "        temp_data = data[var].chunk(chunks)\n",
    "\n",
    "        # 执行标准化，并重新赋值给数据集\n",
    "        data[var] = ((temp_data - mean) / std).chunk(chunks)\n",
    "    \n",
    "    return data\n",
    "data = xr.open_dataset(r\"D:\\Data_Store\\Dataset\\Original_Data\\0.1_Data.zarr\")\n",
    "std_dict = np.load(r\"D:\\Data_Store\\Dataset\\ST_Conv\\std_mean\\std_dict.npy\", allow_pickle=True).item()\n",
    "mean_dict = np.load(r\"D:\\Data_Store\\Dataset\\ST_Conv\\std_mean\\mean_dict.npy\", allow_pickle=True).item()\n",
    "\n",
    "data = normalize_data(std_dict, mean_dict, data)\n",
    "data = data.isel(time=slice(None, -1))\n",
    "data.to_zarr('D:/Data_Store/Dataset/Original_Data/0.1_normal_Data.zarr', mode='w')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 样本的并行生成办法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variables reordered according to the configuration.\n",
      "处理时间段 0 到 50 完成. 总耗时：949.10s\n",
      "处理时间段 33 到 83 完成. 总耗时：11.80s\n",
      "处理时间段 66 到 116 完成. 总耗时：11.38s\n",
      "处理时间段 99 到 149 完成. 总耗时：11.17s\n",
      "处理时间段 132 到 182 完成. 总耗时：11.63s\n",
      "处理时间段 165 到 215 完成. 总耗时：10.96s\n",
      "处理时间段 198 到 248 完成. 总耗时：10.95s\n",
      "处理时间段 231 到 281 完成. 总耗时：10.92s\n",
      "处理时间段 264 到 314 完成. 总耗时：10.94s\n",
      "处理时间段 297 到 347 完成. 总耗时：10.91s\n",
      "处理时间段 330 到 380 完成. 总耗时：9.05s\n",
      "处理时间段 363 到 413 完成. 总耗时：11.19s\n",
      "处理时间段 396 到 446 完成. 总耗时：11.40s\n",
      "处理时间段 429 到 479 完成. 总耗时：11.15s\n",
      "处理时间段 462 到 512 完成. 总耗时：10.59s\n",
      "处理时间段 495 到 545 完成. 总耗时：10.95s\n",
      "处理时间段 528 到 578 完成. 总耗时：10.69s\n",
      "处理时间段 561 到 611 完成. 总耗时：10.73s\n",
      "处理时间段 594 到 644 完成. 总耗时：10.58s\n",
      "处理时间段 627 到 677 完成. 总耗时：10.67s\n",
      "处理时间段 660 到 710 完成. 总耗时：10.49s\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import torch\n",
    "import argparse\n",
    "import numpy as np\n",
    "import xarray as xr \n",
    "from torch import optim\n",
    "import yaml\n",
    "import zarr\n",
    "import torch.nn as nn\n",
    "\n",
    "import sys\n",
    "sys.path.append(r\"C:\\Users\\Administrator\\Desktop\\code\\ST-Conv\")\n",
    "from tool.utils import Util\n",
    "\n",
    "def reorder_variables(zarr_file_path, config_file_path):\n",
    "    # 加载配置文件\n",
    "    with open(config_file_path, 'r') as file:\n",
    "        vars_config = yaml.safe_load(file)\n",
    "\n",
    "    # 提取变量顺序\n",
    "    vars_order = list(vars_config.keys())\n",
    "\n",
    "    # 加载zarr文件\n",
    "    data = xr.open_zarr(zarr_file_path,consolidated=True)\n",
    "\n",
    "    # 确保所有配置文件中的变量都在zarr文件中\n",
    "    missing_vars = [var for var in vars_order if var not in data.data_vars]\n",
    "    if missing_vars:\n",
    "        print(f\"Warning: The following variables from the config are not present in the zarr file: {missing_vars}\")\n",
    "    \n",
    "    # 按照配置文件中的顺序重新排列变量\n",
    "    data_reordered = xr.Dataset({var: data[var] for var in vars_order if var in data.data_vars})\n",
    "\n",
    "    print(f\"Variables reordered according to the configuration.\")\n",
    "    return data_reordered\n",
    "\n",
    "\n",
    "\n",
    "def split_dataset_filling_faster(vars_index_map, vars_day_argu, Day_time_step, vars_half_day_argu, Half_Day_time_step, vars_hour_argu, Hour_time_step, vars_static_argu, data):\n",
    "    # channels, time, lat, lon\n",
    "    time, lon, lat = data.shape[1], data.shape[3], data.shape[2]\n",
    "    vars_day_list,vars_half_day_list,vars_hour_list, vars_static_list = list(vars_day_argu.keys()), list(vars_half_day_argu.keys()) ,list(vars_hour_argu.keys()), list(vars_static_argu.keys())\n",
    "    sm_index = vars_index_map[vars_half_day_list[1]]\n",
    "    sm_data = data[sm_index, :, :, :]\n",
    "    valid_counts_per_time_step = np.sum(~np.isnan(sm_data), axis=(1, 2))\n",
    "    total_count = 16 * 16 * 0.8\n",
    "\n",
    "    complete_time_indices = np.where(valid_counts_per_time_step > total_count)[0]\n",
    "\n",
    "    if complete_time_indices.shape[0] == 0:\n",
    "        return np.array([]), np.array([]), np.array([]), np.array([])\n",
    "    day_increment = 24//3  # 24小时频率，每3小时一个时间点\n",
    "    half_day_increment = 12//3  # 12小时频率\n",
    "    hour_increment = 1  # 3小时频率\n",
    "\n",
    "    # 初始化半天，每天和每小时的数据列表\n",
    "    doy_index = vars_index_map['DOY']  # 假设DOY是变量名\n",
    "    valid_time_length = len(complete_time_indices)\n",
    "    half_day_data = torch.zeros((valid_time_length, 2, Half_Day_time_step, lon, lat), dtype=torch.float32)\n",
    "    day_data = torch.zeros((valid_time_length, len(vars_day_list)-1, Day_time_step, lon, lat), dtype=torch.float32)\n",
    "    hour_data = torch.zeros((valid_time_length, len(vars_hour_list)-1, Hour_time_step, lon, lat), dtype=torch.float32)\n",
    "    static_data = torch.zeros((valid_time_length, len(vars_static_list)-1, lon, lat), dtype=torch.float32)\n",
    "\n",
    "    # 计算每日、每半日和每小时数据的offsets和indices\n",
    "    day_offsets = np.arange(-(Day_time_step // 2), (Day_time_step + 1) // 2)\n",
    "    half_day_offsets = np.arange(-(Half_Day_time_step // 2), (Half_Day_time_step + 1) // 2)\n",
    "    hour_offsets = np.arange(-(Hour_time_step // 2), (Hour_time_step + 1) // 2)\n",
    "\n",
    "    # 索引矩阵\n",
    "    day_indices = np.add.outer(complete_time_indices, day_offsets * day_increment)\n",
    "    half_day_indices = np.add.outer(complete_time_indices, half_day_offsets * half_day_increment)\n",
    "    hour_indices = np.add.outer(complete_time_indices, hour_offsets * hour_increment)\n",
    "    static_indices = complete_time_indices\n",
    "\n",
    "    # 确保所有索引都在有效范围内\n",
    "    day_indices = np.clip(day_indices, 0, time - 1)\n",
    "    half_day_indices = np.clip(half_day_indices, 0, time - 1)\n",
    "    hour_indices = np.clip(hour_indices, 0, time - 1)\n",
    "\n",
    "    # 变量的索引列表\n",
    "    day_var_indices = [vars_index_map[var] for var in vars_day_list[1:]]\n",
    "    hour_var_indices = [vars_index_map[var] for var in vars_hour_list[1:]]\n",
    "    static_var_indices = [vars_index_map[var] for var in vars_static_list[1:]]\n",
    "\n",
    "    # 使用索引列表直接提取每日数据\n",
    "    day_data_slices = data[day_var_indices][:, day_indices, :, :]\n",
    "    day_data_slices = np.nan_to_num(day_data_slices)\n",
    "    day_data = torch.from_numpy(day_data_slices).permute(1, 0, 2, 3, 4).float()\n",
    "\n",
    "    # 使用索引列表直接提取每小时数据\n",
    "    hour_data_slices = data[hour_var_indices][:, hour_indices, :, :]\n",
    "    hour_data_slices = np.nan_to_num(hour_data_slices)\n",
    "    hour_data = torch.from_numpy(hour_data_slices).permute(1, 0, 2, 3, 4).float()\n",
    "\n",
    "    # 遍历每半日数据变量\n",
    "    # 第一个片为SM空数据，第二片为DOY数据\n",
    "    doy_data_slices = data[doy_index, half_day_indices, :, :]\n",
    "    sm_half_day_data_slices = data[sm_index, half_day_indices, :, :]\n",
    "    half_day_data[:, 0, :, :, :] = torch.from_numpy(np.nan_to_num(sm_half_day_data_slices))\n",
    "    half_day_data[:, 1, :, :, :] = torch.from_numpy(np.nan_to_num(doy_data_slices))\n",
    "\n",
    "    # 静态数据\n",
    "    static_data_slices = data[static_var_indices][:, static_indices, :, :]\n",
    "    static_data_slices = np.nan_to_num(static_data_slices)\n",
    "    static_data = torch.from_numpy(static_data_slices).permute(1, 0, 2, 3).float()\n",
    "    \n",
    "    # batch_size,channels,time,lat,lon\n",
    "    return day_data.numpy(), half_day_data.numpy(), hour_data.numpy(), static_data.numpy()\n",
    "\n",
    "def split_data(vars_index_map, vars_day_argu, Day_time_step, vars_half_day_argu, Half_Day_time_step, vars_hour_argu, Hour_time_step, vars_static_argu, data, patch_size=16, stride=8, time_batch_size=200, time_overlap=17):\n",
    "    times, lat, lon = data.time.shape[0], data.lat.shape[0], data.lon.shape[0]\n",
    "\n",
    "    # 定义数据保存路径\n",
    "    save_path_day = \"D:/Data_Store/Dataset/ST_Data/Day\"\n",
    "    save_path_half_day = \"D:/Data_Store/Dataset/ST_Data/Half_Day\"\n",
    "    save_path_hour = \"D:/Data_Store/Dataset/ST_Data/Hour\"\n",
    "    save_path_static = \"D:/Data_Store/Dataset/ST_Data/Static\"\n",
    "\n",
    "    # 确保保存路径存在\n",
    "    os.makedirs(save_path_day, exist_ok=True)\n",
    "    os.makedirs(save_path_half_day, exist_ok=True)\n",
    "    os.makedirs(save_path_hour, exist_ok=True)\n",
    "    os.makedirs(save_path_static, exist_ok=True)\n",
    "\n",
    "    for time_start in range(0, times, time_batch_size - time_overlap):\n",
    "        batch_start_time = time.time()\n",
    "        time_end = min(time_start + time_batch_size, times)\n",
    "        data_batch = data.isel(time=slice(time_start, time_end)).to_array().compute().values\n",
    "\n",
    "        day_samples, half_day_samples, hour_samples, static_samples = None, None, None, None\n",
    "\n",
    "        # 从左下开始预测（示例中未显示预测逻辑，仅数据切分和保存）\n",
    "        for i in range(0, lat - patch_size + 1, stride):\n",
    "            for j in range(0, lon - patch_size + 1, stride):\n",
    "                data_chunk = data_batch[:, :, i:i+patch_size, j:j+patch_size]\n",
    "                day_sample, half_day_sample, hour_sample, static_sample = split_dataset_filling_faster(vars_index_map, vars_day_argu, Day_time_step, vars_half_day_argu, Half_Day_time_step, vars_hour_argu, Hour_time_step, vars_static_argu, data_chunk)\n",
    "                if day_sample.size > 0:\n",
    "                    day_samples = np.concatenate((day_samples, day_sample), axis=0) if day_samples is not None else day_sample\n",
    "                    half_day_samples = np.concatenate((half_day_samples, half_day_sample), axis=0) if half_day_samples is not None else half_day_sample\n",
    "                    hour_samples = np.concatenate((hour_samples, hour_sample), axis=0) if hour_samples is not None else hour_sample\n",
    "                    static_samples = np.concatenate((static_samples, static_sample), axis=0) if static_samples is not None else static_sample\n",
    "\n",
    "        # 保存数据到指定路径，每个时间批次保存一次，无需再次合并\n",
    "        np.save(os.path.join(save_path_day, f'day_data_{time_start}_{time_end}.npy'), day_samples)\n",
    "        np.save(os.path.join(save_path_half_day, f'half_day_data_{time_start}_{time_end}.npy'), half_day_samples)\n",
    "        np.save(os.path.join(save_path_hour, f'hour_data_{time_start}_{time_end}.npy'), hour_samples)\n",
    "        np.save(os.path.join(save_path_static, f'static_data_{time_start}_{time_end}.npy'), static_samples)\n",
    "\n",
    "        batch_end_time = time.time()\n",
    "        print(f\"处理时间段 {time_start} 到 {time_end} 完成. 总耗时：{batch_end_time - batch_start_time:.2f}s\")\n",
    "\n",
    "    print(\"数据切分和保存完成\")\n",
    "\n",
    "def main(args):\n",
    "    models_config = Util.load_config(args.models_config_path)\n",
    "    path_config = Util.load_config(args.path_config_path)\n",
    "    vars_day_argu = Util.load_config(args.vars_day_config_path)\n",
    "    vars_half_day_argu = Util.load_config(args.vars_half_day_config_path)\n",
    "    vars_hour_argu = Util.load_config(args.vars_hour_config_path)\n",
    "    vars_static_argu = Util.load_config(args.vars_static_config_path)\n",
    "    data = reorder_variables(path_config['0.37_normal_Data'], \"C:\\\\Users\\\\Administrator\\\\Desktop\\\\code\\\\ST-Conv\\\\config\\\\construct_Data\\\\vars_config.yaml\")\n",
    "    vars_index_map = {var: i for i, var in enumerate(data.data_vars)}\n",
    "    # 数据分割和保存\n",
    "    split_data(vars_index_map, vars_day_argu, 5, vars_half_day_argu, 5, vars_hour_argu, 5, vars_static_argu, data, patch_size=32, stride=6, time_batch_size=50, time_overlap=17)\n",
    "    print(\"数据切分和保存过程完成。\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--models_config_path', type=str, default='../config/models_config.yaml')\n",
    "    parser.add_argument('--path_config_path', type=str, default='../config/path_config.yaml')\n",
    "    parser.add_argument('--vars_day_config_path', type=str, default='../config/construct_Data/vars_day_config.yaml')\n",
    "    parser.add_argument('--vars_half_day_config_path', type=str, default='../config/construct_Data/vars_half_day_config.yaml')\n",
    "    parser.add_argument('--vars_hour_config_path', type=str, default='../config/construct_Data/vars_hour_config.yaml')\n",
    "    parser.add_argument('--vars_static_config_path', type=str, default='../config/construct_Data/vars_static_config.yaml')\n",
    "    args = parser.parse_known_args()[0]\n",
    "\n",
    "    main(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 拼接数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "def list_and_sort_files(directory):\n",
    "    # 列出目录下的所有文件\n",
    "    files = os.listdir(directory)\n",
    "    # 完整路径\n",
    "    files = [os.path.join(directory, file) for file in files]\n",
    "    # 排序\n",
    "    files.sort(key=lambda x: os.path.getmtime(x))\n",
    "    return files\n",
    "def concatenate_data(files):\n",
    "    if not files:\n",
    "        return np.array([])  # 如果没有文件，返回空数组\n",
    "\n",
    "    # 读取第一个文件以初始化数据\n",
    "    data = np.load(files[0])\n",
    "    \n",
    "    # 读取剩余文件并拼接\n",
    "    for file in files[1:]:\n",
    "        next_data = np.load(file)\n",
    "        data = np.concatenate((data, next_data), axis=0)\n",
    "    \n",
    "    return data\n",
    "def save_concatenated_data(directory):\n",
    "    files = list_and_sort_files(directory)\n",
    "    data = concatenate_data(files)\n",
    "    if data.size > 0:  # 确保数据不为空\n",
    "        # 构造保存文件的路径和名称\n",
    "        base_name = os.path.basename(directory)  # 获取文件夹的最后一部分作为基础名\n",
    "        save_path = os.path.join(directory, base_name + \".npy\")  # 构造保存路径\n",
    "        np.save(save_path, data)  # 保存数据\n",
    "        print(f\"Data saved to {save_path}\")\n",
    "\n",
    "directories = [\n",
    "    \"D:\\\\Data_Store\\\\Dataset\\\\ST_Conv\\\\Data\\\\Day\",\n",
    "    \"D:\\\\Data_Store\\\\Dataset\\\\ST_Conv\\\\Data\\\\Half_Day\",\n",
    "    \"D:\\\\Data_Store\\\\Dataset\\\\ST_Conv\\\\Data\\\\Hour\",\n",
    "    \"D:\\\\Data_Store\\\\Dataset\\\\ST_Conv\\\\Data\\\\Static\"\n",
    "]\n",
    "\n",
    "for directory in directories:\n",
    "    save_concatenated_data(directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 查看数据分布情况"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 加载数据\n",
    "ST_Static_Data_path = 'D:/Data_Store/Dataset/ST_Data/Static/Static_Data.npy'\n",
    "ST_Day_Data_path = 'D:/Data_Store/Dataset/ST_Data/Day/Day_Data.npy'\n",
    "ST_Half_Day_Data_path = \"D:\\Data_Store\\Dataset\\ST_Data\\Half_Day\\Half_Day.npy\"\n",
    "\n",
    "ST_Half_Day_Data = np.load(ST_Half_Day_Data_path)\n",
    "\n",
    "\n",
    "# 计算 ST_Half_Day_Data 中每个样本的缺失率\n",
    "# 缺失数据定义为 NaN\n",
    "relevant_slice = ST_Half_Day_Data[:, 0, 4, :, :]\n",
    "zero_nan_proportions = np.mean((relevant_slice == 0) | np.isnan(relevant_slice), axis=(1, 2))\n",
    "# 获取缺失率大于 40% 的索引\n",
    "high_missing_indices = np.where(zero_nan_proportions > 0.2)[0]\n",
    "\n",
    "# 绘制连线图\n",
    "plt.hist(zero_nan_proportions, bins=np.arange(0, 1.01, 0.01), edgecolor='black')\n",
    "plt.xlabel('0和NaN数据占比')\n",
    "plt.ylabel('样本数')\n",
    "plt.title('ST_Half_Day_Data[:, 4, :, :, :]中0和NaN数据占比分布')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 维度转换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "directories = [\n",
    "    \"/root/autodl-tmp/ST_Data/Day\",\n",
    "    \"/root/autodl-tmp/ST_Data/Half_Day\",\n",
    "    \"/root/autodl-tmp/ST_Data/Hour\",\n",
    "    \"/root/autodl-tmp/ST_Data/Static\"\n",
    "]\n",
    "for directory in directories:\n",
    "    # 获取目录的名称\n",
    "    dirname = os.path.basename(directory)\n",
    "    # 构造完整路径\n",
    "    file_path = os.path.join(directory, dirname + '.npy')\n",
    "    \n",
    "    # 检查文件\n",
    "    if os.path.exists(file_path):\n",
    "        data = np.load(file_path)\n",
    "        print(f\"Original dimensions for {file_path}: {data.shape}\")\n",
    "        \n",
    "        # 检查数据维度，确保它至少有两个维度\n",
    "        if data.ndim >= 2:\n",
    "            new_data = np.moveaxis(data, 1, -1)\n",
    "            print(f\"New dimensions for {file_path}: {new_data.shape}\")\n",
    "            np.save(file_path, new_data)\n",
    "            print(f\"Updated file saved to {file_path}\")\n",
    "        else:\n",
    "            print(f\"File {file_path} does not have enough dimensions to reorder.\")\n",
    "    else:\n",
    "        print(f\"File {file_path} does not exist.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
