{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 无重叠重建：处理后交接"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import torch\n",
    "import argparse\n",
    "import numpy as np\n",
    "import xarray as xr \n",
    "from torch import optim\n",
    "import yaml\n",
    "import zarr\n",
    "import torch.nn as nn\n",
    "\n",
    "import sys\n",
    "sys.path.append(r\"/root/ST-Conv\")\n",
    "from tool.utils import Util\n",
    "from model.model import STConvNet\n",
    "from model.block import BasicBlock\n",
    "from dataset.dataset_process import data_preparation\n",
    "from log.logger import setup_logger\n",
    "def reorder_variables(zarr_file_path, config_file_path):\n",
    "    # 加载配置文件\n",
    "    with open(config_file_path, 'r') as file:\n",
    "        vars_config = yaml.safe_load(file)\n",
    "\n",
    "    # 提取变量顺序\n",
    "    vars_order = list(vars_config.keys())\n",
    "\n",
    "    # 加载zarr文件\n",
    "    data = xr.open_zarr(zarr_file_path,consolidated=True)\n",
    "\n",
    "    # 确保所有配置文件中的变量都在zarr文件中\n",
    "    missing_vars = [var for var in vars_order if var not in data.data_vars]\n",
    "    if missing_vars:\n",
    "        print(f\"Warning: The following variables from the config are not present in the zarr file: {missing_vars}\")\n",
    "    \n",
    "    # 按照配置文件中的顺序重新排列变量\n",
    "    data_reordered = xr.Dataset({var: data[var] for var in vars_order if var in data.data_vars})\n",
    "\n",
    "    print(f\"Variables reordered according to the configuration.\")\n",
    "    return data_reordered\n",
    "\n",
    "def normalize_data(std_values, mean_values, data):\n",
    "    \"\"\"\n",
    "    标准化 NetCDF 数据集中的变量。\n",
    "\n",
    "    :param std_path: 包含标准差的文件路径。\n",
    "    :param mean_path: 包含均值的文件路径。\n",
    "    :param data: xarray 数据集。\n",
    "    :return: 标准化后的 xarray 数据集。\n",
    "    \"\"\"\n",
    "    # 检查长度是否匹配\n",
    "    if len(std_values) != len(mean_values) or len(std_values) != len(data.variables)-3:\n",
    "        raise ValueError(\"标准差、均值的长度与数据集中的变量数不匹配\")\n",
    "\n",
    "    # 对每个变量应用标准化\n",
    "    for var, std, mean in zip(data.variables, std_values, mean_values):\n",
    "        if var in ['lat', 'lon', 'time']:\n",
    "            continue  # 跳过非数据维度\n",
    "        data[var] = (data[var] - mean) / std\n",
    "\n",
    "    return data\n",
    "\n",
    "def batch_process(model, day_sample, half_day_sample, hour_sample, static_sample, batch_size, device, std=None, mean=None,mode=None):\n",
    "    time = day_sample.shape[0]\n",
    "    outputs = []\n",
    "    if mode == 'ALL':\n",
    "        day_sample, half_day_sample, hour_sample, static_sample = day_sample.to(device), half_day_sample.to(device), hour_sample.to(device), static_sample.to(device)\n",
    "        output = model(hour_input=hour_sample, day_input=day_sample, half_day_input=half_day_sample, static_input=static_sample)\n",
    "        output = output[0].detach().cpu().numpy() if isinstance(output, tuple) else output.detach().cpu().numpy()\n",
    "        if std is not None and mean is not None:\n",
    "            output = output * std + mean\n",
    "        outputs = np.squeeze(output, axis=1)\n",
    "    else:\n",
    "        for start_idx in range(0, time, batch_size):\n",
    "            end_idx = min(start_idx + batch_size, time)\n",
    "            day_sample, half_day_sample, hour_sample, static_sample = day_sample.to(device), half_day_sample.to(device), hour_sample.to(device), static_sample.to(device)\n",
    "            current_day_sample, current_half_day_sample, current_hour_sample, current_static_sample = day_sample[start_idx:end_idx], half_day_sample[start_idx:end_idx], hour_sample[start_idx:end_idx], static_sample[start_idx:end_idx]\n",
    "            current_day_sample,current_half_day_sample,current_hour_sample,current_static_sample = current_day_sample.to(device),current_half_day_sample.to(device),current_hour_sample.to(device),current_static_sample.to(device)\n",
    "            output = model(hour_input=current_hour_sample, day_input=current_day_sample, half_day_input=current_half_day_sample, static_input=current_static_sample)\n",
    "            output = output[0].detach().cpu().numpy() if isinstance(output, tuple) else output.detach().cpu().numpy()\n",
    "            if std is not None and mean is not None:\n",
    "                output = output * std + mean\n",
    "            output = np.squeeze(output, axis=1)\n",
    "            outputs.append(output)\n",
    "        outputs = np.concatenate(outputs, axis=0)\n",
    "\n",
    "    return outputs\n",
    "\n",
    "def split_dataset_filling_faster(vars_index_map, vars_day_argu, Day_time_step, vars_half_day_argu, Half_Day_time_step, vars_hour_argu, Hour_time_step, vars_static_argu, data):\n",
    "    # channels, time, lat, lon\n",
    "    time, lon, lat = data.shape[1], data.shape[3], data.shape[2]\n",
    "    vars_day_list, vars_hour_list, vars_static_list = list(vars_day_argu.keys()), list(vars_hour_argu.keys()), list(vars_static_argu.keys())\n",
    "    \n",
    "    day_increment = 24//3  # 24小时频率，每3小时一个时间点\n",
    "    half_day_increment = 12//3  # 12小时频率\n",
    "    hour_increment = 1  # 3小时频率\n",
    "\n",
    "    # 初始化半天，每天和每小时的数据列表\n",
    "    doy_index = vars_index_map['DOY']  # 假设DOY是变量名\n",
    "    half_day_data = torch.zeros((time, 2, Half_Day_time_step, lat, lon), dtype=torch.float32)\n",
    "    day_data = torch.zeros((time, len(vars_day_list)-1, Day_time_step, lat, lon), dtype=torch.float32)\n",
    "    hour_data = torch.zeros((time, len(vars_hour_list)-1, Hour_time_step, lat, lon), dtype=torch.float32)\n",
    "    static_data = torch.zeros((time, len(vars_static_list)-1, lat, lon), dtype=torch.float32)\n",
    "\n",
    "    # 计算每日、每半日和每小时数据的offsets和indices\n",
    "    day_offsets = np.arange(-(Day_time_step // 2), (Day_time_step + 1) // 2)\n",
    "    half_day_offsets = np.arange(-(Half_Day_time_step // 2), (Half_Day_time_step + 1) // 2)\n",
    "    hour_offsets = np.arange(-(Hour_time_step // 2), (Hour_time_step + 1) // 2)\n",
    "\n",
    "    # 索引矩阵\n",
    "    day_indices = np.add.outer(np.arange(time), day_offsets * day_increment)\n",
    "    half_day_indices = np.add.outer(np.arange(time), half_day_offsets * half_day_increment)\n",
    "    hour_indices = np.add.outer(np.arange(time), hour_offsets * hour_increment)\n",
    "    static_indices = np.arange(time)\n",
    "\n",
    "    # 确保所有索引都在有效范围内\n",
    "    day_indices = np.clip(day_indices, 0, time - 1)\n",
    "    half_day_indices = np.clip(half_day_indices, 0, time - 1)\n",
    "    hour_indices = np.clip(hour_indices, 0, time - 1)\n",
    "\n",
    "    # 变量的索引列表\n",
    "    day_var_indices = [vars_index_map[var] for var in vars_day_list[1:]]\n",
    "    hour_var_indices = [vars_index_map[var] for var in vars_hour_list[1:]]\n",
    "    static_var_indices = [vars_index_map[var] for var in vars_static_list[1:]]\n",
    "\n",
    "    # 使用索引列表直接提取每日数据\n",
    "    day_data_slices = data[day_var_indices][:, day_indices, :, :]\n",
    "    day_data_slices = np.nan_to_num(day_data_slices)\n",
    "    day_data = torch.from_numpy(day_data_slices).permute(1, 0, 2, 3, 4).float()\n",
    "\n",
    "    # 使用索引列表直接提取每小时数据\n",
    "    hour_data_slices = data[hour_var_indices][:, hour_indices, :, :]\n",
    "    hour_data_slices = np.nan_to_num(hour_data_slices)\n",
    "    hour_data = torch.from_numpy(hour_data_slices).permute(1, 0, 2, 3, 4).float()\n",
    "\n",
    "    # 遍历每半日数据变量\n",
    "    # 第一个片为SM空数据，第二片为DOY数据\n",
    "    doy_data_slices = data[doy_index, half_day_indices, :, :]\n",
    "    half_day_data[:, 1, :, :, :] = torch.from_numpy(np.nan_to_num(doy_data_slices))\n",
    "\n",
    "    # 静态数据\n",
    "    static_data_slices = data[static_var_indices][:, static_indices, :, :]\n",
    "    static_data_slices = np.nan_to_num(static_data_slices)\n",
    "    static_data = torch.from_numpy(static_data_slices).permute(1, 0, 2, 3).float()\n",
    "    \n",
    "    # batch_size,channels,time,lat,lon\n",
    "    return day_data, half_day_data, hour_data, static_data\n",
    "\n",
    "# 重叠区域计算函数\n",
    "def calculate_overlap(first_data, second_data):\n",
    "    '''\n",
    "    对于重叠区域，计算两个数据块的均值。\n",
    "    '''\n",
    "    overlap_mask = np.where(first_data != 0, 1, 0)\n",
    "    inv_overlap_mask = 1 - overlap_mask\n",
    "    average_data = ((first_data * overlap_mask + second_data * overlap_mask) / 2) + inv_overlap_mask * second_data\n",
    "    return average_data\n",
    "\n",
    "def reconstruct_data(file_path,vars_index_map,vars_day_argu, Day_time_step, vars_half_day_argu, Half_Day_time_step, vars_hour_argu,Hour_time_step, vars_static_argu, data, model, patch_size=16, stride=12,std = None,mean = None,time_batch_size=100, time_overlap=18, mode=None):\n",
    "    '''7\n",
    "    先挨个将数据集从从左下开始划分，划分之后直接进行预测，填充回原数据集，\n",
    "    划分不完整的数据，最后重新从右上开始划分，全部预测后，只填充没有被划分到数据\n",
    "    '''\n",
    "    times, lat, lon = data.time.shape[0], data.lat.shape[0], data.lon.shape[0]\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    reconstructed_data = np.zeros((times, lat, lon))\n",
    "    time_shift = max(int(Day_time_step // 2), int(Half_Day_time_step // 2), int(Hour_time_step // 2))\n",
    "        # 从左下开始预测\n",
    "    for time_start in range(0, times, time_batch_size - time_overlap):\n",
    "        batch_start_time = time.time()  # 开始计时\n",
    "\n",
    "        # 偏移量计算\n",
    "        effective_start = time_start if time_start == 0 else time_start + time_shift\n",
    "        effective_end = time_end\n",
    "\n",
    "        time_end = min(time_start + time_batch_size, times)\n",
    "        data_batch = data.isel(time=slice(time_start, time_end)).to_array().compute().values  # channels, times, lat, lon\n",
    "        if mode is None:\n",
    "            # 从左下开始预测\n",
    "            for i in range(0, lat - patch_size + 1, stride):\n",
    "                for j in range(0, lon - patch_size + 1, stride):\n",
    "                    data_chunk = data_batch[:, :, i:i+patch_size, j:j+patch_size]  # channels, times, lat, lon\n",
    "                    day_sample, half_day_sample, hour_sample, static_sample = split_dataset_filling_faster(vars_index_map, vars_day_argu, Day_time_step, vars_half_day_argu, Half_Day_time_step, vars_hour_argu, Hour_time_step, vars_static_argu, data_chunk)\n",
    "                    output = batch_process(model, day_sample, half_day_sample, hour_sample, static_sample, batch_size=16, device=device, std=std, mean=mean)\n",
    "                    reconstructed_data[effective_start:effective_end, i:i+patch_size, j:j+patch_size] = output[(time_shift if time_start + time_shift < time_end else 0):] if time_start != 0 else output\n",
    "\n",
    "            # 纵向不完整处理\n",
    "            if lon % patch_size != 0:\n",
    "                for j in range(0, lat - patch_size + 1, stride):\n",
    "                    data_chunk = data_batch[:, :, j:j+patch_size, lon - patch_size:lon]\n",
    "                    day_sample, half_day_sample, hour_sample, static_sample = split_dataset_filling_faster(vars_index_map, vars_day_argu, Day_time_step, vars_half_day_argu, Half_Day_time_step, vars_hour_argu, Hour_time_step, vars_static_argu, data_chunk)\n",
    "                    output = batch_process(model, day_sample, half_day_sample, hour_sample, static_sample, batch_size=16, device=device, std=std, mean=mean)\n",
    "                    reconstructed_data[effective_start:effective_end, j:j+patch_size, lon - patch_size:lon] = output[(time_shift if time_start + time_shift < time_end else 0):] if time_start != 0 else output\n",
    "\n",
    "            # 横向不完整处理\n",
    "            if lat % patch_size != 0:\n",
    "                for i in range(0, lon - patch_size + 1, stride):\n",
    "                    data_chunk = data_batch[:, :, lat - patch_size:lat, i:i+patch_size]\n",
    "                    day_sample, half_day_sample, hour_sample, static_sample = split_dataset_filling_faster(vars_index_map, vars_day_argu, Day_time_step, vars_half_day_argu, Half_Day_time_step, vars_hour_argu, Hour_time_step, vars_static_argu, data_chunk)\n",
    "                    output = batch_process(model, day_sample, half_day_sample, hour_sample, static_sample, batch_size=16, device=device, std=std, mean=mean)\n",
    "                    reconstructed_data[effective_start:effective_end, lat - patch_size:lat, i:i+patch_size] = output[(time_shift if time_start + time_shift < time_end else 0):] if time_start != 0 else output\n",
    "\n",
    "            if lon % patch_size != 0 and lat % patch_size != 0:\n",
    "                data_chunk = data_batch[:, :, lat - patch_size:lat, lon - patch_size:lon]\n",
    "                day_sample, half_day_sample, hour_sample, static_sample = split_dataset_filling_faster(vars_index_map, vars_day_argu, Day_time_step, vars_half_day_argu, Half_Day_time_step, vars_hour_argu, Hour_time_step, vars_static_argu, data_chunk)\n",
    "                output = batch_process(model, day_sample, half_day_sample, hour_sample, static_sample, batch_size=16, device=device, std=std, mean=mean)  \n",
    "                reconstructed_data[effective_start:effective_end, lat - patch_size:lat, lon - patch_size:lon] = output[(time_shift if time_start + time_shift < time_end else 0):] if time_start != 0 else output\n",
    "\n",
    "        elif mode == 'all':\n",
    "            day_sample, half_day_sample, hour_sample, static_sample = split_dataset_filling_faster(vars_index_map, vars_day_argu, Day_time_step, vars_half_day_argu, Half_Day_time_step, vars_hour_argu, Hour_time_step, vars_static_argu, data_batch)\n",
    "            output = batch_process(model, day_sample, half_day_sample, hour_sample, static_sample, batch_size=2, device=device, std=std, mean=mean)\n",
    "            reconstructed_data[effective_start:effective_end, :, :] = output[(time_shift if time_start + time_shift < time_end else 0):] if time_start != 0 else output\n",
    "            \n",
    "        batch_end_time = time.time()  # 结束计时\n",
    "        print(f\"处理时间段 {time_start} 到 {time_end} 完成. 总耗时：{batch_end_time - batch_start_time:.2f}s\")\n",
    "        reconstructed_data = np.clip(reconstructed_data, 0, 1)\n",
    "        np.save(file_path, reconstructed_data)\n",
    "    return reconstructed_data\n",
    "\n",
    "def main(args):\n",
    "    models_config = Util.load_config(args.models_config_path)\n",
    "    train_config = models_config['train_shared_parameter'] \n",
    "    path_config = Util.load_config(args.path_config_path)\n",
    "    dataloader_hyperparameter_config = Util.load_config(args.dataloader_hyperparameter_config_path)\n",
    "    vars_argu = Util.load_config(args.vars_config_path)\n",
    "    vars_day_argu = Util.load_config(args.vars_day_config_path)\n",
    "    vars_half_day_argu = Util.load_config(args.vars_half_day_config_path)\n",
    "    vars_hour_argu = Util.load_config(args.vars_hour_config_path)\n",
    "    vars_static_argu = Util.load_config(args.vars_static_config_path)\n",
    "    logger = setup_logger(path_config['log'],append=True)\n",
    "    var_list = list(vars_argu.keys())\n",
    "    return train_config, models_config, path_config, dataloader_hyperparameter_config, logger, var_list, vars_argu, vars_day_argu, vars_half_day_argu, vars_hour_argu, vars_static_argu\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--models_config_path', type=str, default='../config/models_config.yaml')\n",
    "    parser.add_argument('--path_config_path', type=str, default='../config/path_config.yaml')\n",
    "    parser.add_argument('--dataloader_hyperparameter_config_path', type=str, default='../config/dataloader_hyperparameter_config.yaml')\n",
    "    parser.add_argument('--vars_config_path', type=str, default='../config/construct_Data/vars_config.yaml')\n",
    "    parser.add_argument('--vars_day_config_path', type=str, default='../config/construct_Data/vars_day_config.yaml')\n",
    "    parser.add_argument('--vars_half_day_config_path', type=str, default='../config/construct_Data/vars_half_day_config.yaml')\n",
    "    parser.add_argument('--vars_hour_config_path', type=str, default='../config/construct_Data/vars_hour_config.yaml')\n",
    "    parser.add_argument('--vars_static_config_path', type=str, default='../config/construct_Data/vars_static_config.yaml')\n",
    "    args = parser.parse_known_args()[0]\n",
    "\n",
    "    train_dict, models_config, path_config, dataloader_hyperparameter_config, logger, var_list, vars_argu, vars_day_argu, vars_half_day_argu, vars_hour_argu, vars_static_argu = main(args)\n",
    "    Util.random_seed(seed=train_dict['seed'])\n",
    "    data = reorder_variables(path_config['0.37_normal_Data'], r\"/root/ST-Conv/config/construct_Data/vars_config.yaml\")\n",
    "    vars_index_map = {var: i for i, var in enumerate(data.data_vars)}\n",
    "    # 解包model_config\n",
    "    experiment_groups = models_config['experiment_groups']      # 实验组参数-实验组共享参数+实验组模型参数\n",
    "    std = np.load(\"/root/autodl-fs/ST_Data/std_mean/std.npy\")\n",
    "    mean = np.load(\"/root/autodl-fs/ST_Data/std_mean/mean.npy\")\n",
    "\n",
    "    for group in experiment_groups:\n",
    "        if group['group_name'] == \"Experiment Group 7\":\n",
    "            print(f\"Running experiments for group: {group['group_name']}\")\n",
    "            shared_parameters = models_config['Shared_parameter'] \n",
    "            experiment_shared_params = group.get('experiment_shared_parameter', {})\n",
    "\n",
    "            # 训练实验组中每个模型\n",
    "            for model_config in group['models']:\n",
    "                model = STConvNet(**shared_parameters, **experiment_shared_params, **model_config['parameters'])\n",
    "                device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "                model = model.to(device)\n",
    "                if torch.cuda.device_count() > 1:\n",
    "                    print(f\"Let's use {torch.cuda.device_count()} GPUs!\")\n",
    "                model = nn.DataParallel(model)\n",
    "                # 设置优化器\n",
    "                optimizer = optim.AdamW(model.parameters(), lr=0, weight_decay=0)  # 优化器\n",
    "                Util.load_model_and_optimizer(model, optimizer, device, experiment_shared_params['best_model_path'], logger=logger, model_index=-1,model_name=model_config['model_name'])\n",
    "                file_path = os.path.join(path_config['reconstruct_data_path'], model_config['model_name'] + '16.npy')\n",
    "                reconstructed_data = reconstruct_data(file_path, vars_index_map, vars_day_argu, shared_parameters['day_step'], vars_half_day_argu, shared_parameters['half_day_step'], vars_hour_argu,shared_parameters['hour_step'], vars_static_argu, data, model, patch_size=16, stride=16, std = std[0],mean = mean[0],mode = 'all')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 重叠重建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import torch\n",
    "import argparse\n",
    "import numpy as np\n",
    "import xarray as xr \n",
    "from torch import optim\n",
    "import yaml\n",
    "import zarr\n",
    "import torch.nn as nn\n",
    "\n",
    "import sys\n",
    "sys.path.append(r\"/root/ST-Conv\")\n",
    "from tool.utils import Util\n",
    "from model.model import STConvNet\n",
    "from model.block import BasicBlock\n",
    "from dataset.dataset_process import data_preparation\n",
    "from log.logger import setup_logger\n",
    "def reorder_variables(zarr_file_path, config_file_path):\n",
    "    # 加载配置文件\n",
    "    with open(config_file_path, 'r') as file:\n",
    "        vars_config = yaml.safe_load(file)\n",
    "\n",
    "    # 提取变量顺序\n",
    "    vars_order = list(vars_config.keys())\n",
    "\n",
    "    # 加载zarr文件\n",
    "    data = xr.open_zarr(zarr_file_path,consolidated=True)\n",
    "\n",
    "    # 确保所有配置文件中的变量都在zarr文件中\n",
    "    missing_vars = [var for var in vars_order if var not in data.data_vars]\n",
    "    if missing_vars:\n",
    "        print(f\"Warning: The following variables from the config are not present in the zarr file: {missing_vars}\")\n",
    "    \n",
    "    # 按照配置文件中的顺序重新排列变量\n",
    "    data_reordered = xr.Dataset({var: data[var] for var in vars_order if var in data.data_vars})\n",
    "\n",
    "    print(f\"Variables reordered according to the configuration.\")\n",
    "    return data_reordered\n",
    "\n",
    "def normalize_data(std_values, mean_values, data):\n",
    "    \"\"\"\n",
    "    标准化 NetCDF 数据集中的变量。\n",
    "\n",
    "    :param std_path: 包含标准差的文件路径。\n",
    "    :param mean_path: 包含均值的文件路径。\n",
    "    :param data: xarray 数据集。\n",
    "    :return: 标准化后的 xarray 数据集。\n",
    "    \"\"\"\n",
    "    # 检查长度是否匹配\n",
    "    if len(std_values) != len(mean_values) or len(std_values) != len(data.variables)-3:\n",
    "        raise ValueError(\"标准差、均值的长度与数据集中的变量数不匹配\")\n",
    "\n",
    "    # 对每个变量应用标准化\n",
    "    for var, std, mean in zip(data.variables, std_values, mean_values):\n",
    "        if var in ['lat', 'lon', 'time']:\n",
    "            continue  # 跳过非数据维度\n",
    "        data[var] = (data[var] - mean) / std\n",
    "\n",
    "    return data\n",
    "\n",
    "def batch_process(model, day_sample, half_day_sample, hour_sample, static_sample, batch_size, device, std=None, mean=None,mode=None):\n",
    "    time = day_sample.shape[0]\n",
    "    outputs = []\n",
    "    if mode == 'ALL':\n",
    "        day_sample, half_day_sample, hour_sample, static_sample = day_sample.to(device), half_day_sample.to(device), hour_sample.to(device), static_sample.to(device)\n",
    "        output = model(hour_input=hour_sample, day_input=day_sample, half_day_input=half_day_sample, static_input=static_sample)\n",
    "        output = output[0].detach().cpu().numpy() if isinstance(output, tuple) else output.detach().cpu().numpy()\n",
    "        if std is not None and mean is not None:\n",
    "            output = output * std + mean\n",
    "        outputs = np.squeeze(output, axis=1)\n",
    "    else:\n",
    "        for start_idx in range(0, time, batch_size):\n",
    "            end_idx = min(start_idx + batch_size, time)\n",
    "            day_sample, half_day_sample, hour_sample, static_sample = day_sample.to(device), half_day_sample.to(device), hour_sample.to(device), static_sample.to(device)\n",
    "            current_day_sample, current_half_day_sample, current_hour_sample, current_static_sample = day_sample[start_idx:end_idx], half_day_sample[start_idx:end_idx], hour_sample[start_idx:end_idx], static_sample[start_idx:end_idx]\n",
    "            current_day_sample,current_half_day_sample,current_hour_sample,current_static_sample = current_day_sample.to(device),current_half_day_sample.to(device),current_hour_sample.to(device),current_static_sample.to(device)\n",
    "            output = model(hour_input=current_hour_sample, day_input=current_day_sample, half_day_input=current_half_day_sample, static_input=current_static_sample)\n",
    "            output = output[0].detach().cpu().numpy() if isinstance(output, tuple) else output.detach().cpu().numpy()\n",
    "            if std is not None and mean is not None:\n",
    "                output = output * std + mean\n",
    "            output = np.squeeze(output, axis=1)\n",
    "            outputs.append(output)\n",
    "        outputs = np.concatenate(outputs, axis=0)\n",
    "\n",
    "    return outputs\n",
    "\n",
    "def split_dataset_filling_faster(vars_index_map, vars_day_argu, Day_time_step, vars_half_day_argu, Half_Day_time_step, vars_hour_argu, Hour_time_step, vars_static_argu, data):\n",
    "    # channels, time, lat, lon\n",
    "    time, lon, lat = data.shape[1], data.shape[3], data.shape[2]\n",
    "    vars_day_list, vars_hour_list, vars_static_list = list(vars_day_argu.keys()), list(vars_hour_argu.keys()), list(vars_static_argu.keys())\n",
    "    \n",
    "    day_increment = 24//3  # 24小时频率，每3小时一个时间点\n",
    "    half_day_increment = 12//3  # 12小时频率\n",
    "    hour_increment = 1  # 3小时频率\n",
    "\n",
    "    # 初始化半天，每天和每小时的数据列表\n",
    "    doy_index = vars_index_map['DOY']  # 假设DOY是变量名\n",
    "    half_day_data = torch.zeros((time, 2, Half_Day_time_step, lat, lon), dtype=torch.float32)\n",
    "    day_data = torch.zeros((time, len(vars_day_list)-1, Day_time_step, lat, lon), dtype=torch.float32)\n",
    "    hour_data = torch.zeros((time, len(vars_hour_list)-1, Hour_time_step, lat, lon), dtype=torch.float32)\n",
    "    static_data = torch.zeros((time, len(vars_static_list)-1, lat, lon), dtype=torch.float32)\n",
    "\n",
    "    # 计算每日、每半日和每小时数据的offsets和indices\n",
    "    day_offsets = np.arange(-(Day_time_step // 2), (Day_time_step + 1) // 2)\n",
    "    half_day_offsets = np.arange(-(Half_Day_time_step // 2), (Half_Day_time_step + 1) // 2)\n",
    "    hour_offsets = np.arange(-(Hour_time_step // 2), (Hour_time_step + 1) // 2)\n",
    "\n",
    "    # 索引矩阵\n",
    "    day_indices = np.add.outer(np.arange(time), day_offsets * day_increment)\n",
    "    half_day_indices = np.add.outer(np.arange(time), half_day_offsets * half_day_increment)\n",
    "    hour_indices = np.add.outer(np.arange(time), hour_offsets * hour_increment)\n",
    "    static_indices = np.arange(time)\n",
    "\n",
    "    # 确保所有索引都在有效范围内\n",
    "    day_indices = np.clip(day_indices, 0, time - 1)\n",
    "    half_day_indices = np.clip(half_day_indices, 0, time - 1)\n",
    "    hour_indices = np.clip(hour_indices, 0, time - 1)\n",
    "\n",
    "    # 变量的索引列表\n",
    "    day_var_indices = [vars_index_map[var] for var in vars_day_list[1:]]\n",
    "    hour_var_indices = [vars_index_map[var] for var in vars_hour_list[1:]]\n",
    "    static_var_indices = [vars_index_map[var] for var in vars_static_list[1:]]\n",
    "\n",
    "    # 使用索引列表直接提取每日数据\n",
    "    day_data_slices = data[day_var_indices][:, day_indices, :, :]\n",
    "    day_data_slices = np.nan_to_num(day_data_slices)\n",
    "    day_data = torch.from_numpy(day_data_slices).permute(1, 0, 2, 3, 4).float()\n",
    "\n",
    "    # 使用索引列表直接提取每小时数据\n",
    "    hour_data_slices = data[hour_var_indices][:, hour_indices, :, :]\n",
    "    hour_data_slices = np.nan_to_num(hour_data_slices)\n",
    "    hour_data = torch.from_numpy(hour_data_slices).permute(1, 0, 2, 3, 4).float()\n",
    "\n",
    "    # 遍历每半日数据变量\n",
    "    # 第一个片为SM空数据，第二片为DOY数据\n",
    "    doy_data_slices = data[doy_index, half_day_indices, :, :]\n",
    "    half_day_data[:, 1, :, :, :] = torch.from_numpy(np.nan_to_num(doy_data_slices))\n",
    "\n",
    "    # 静态数据\n",
    "    static_data_slices = data[static_var_indices][:, static_indices, :, :]\n",
    "    static_data_slices = np.nan_to_num(static_data_slices)\n",
    "    static_data = torch.from_numpy(static_data_slices).permute(1, 0, 2, 3).float()\n",
    "    \n",
    "    # batch_size,channels,time,lat,lon\n",
    "    return day_data, half_day_data, hour_data, static_data\n",
    "\n",
    "# 重叠区域计算函数\n",
    "def calculate_overlap(first_data, second_data):\n",
    "    '''\n",
    "    对于重叠区域，计算两个数据块的均值。\n",
    "    '''\n",
    "    overlap_mask = np.where(first_data != 0, 1, 0)\n",
    "    inv_overlap_mask = 1 - overlap_mask\n",
    "    average_data = ((first_data * overlap_mask + second_data * overlap_mask) / 2) + inv_overlap_mask * second_data\n",
    "    return average_data\n",
    "\n",
    "def reconstruct_data(file_path,vars_index_map,vars_day_argu, Day_time_step, vars_half_day_argu, Half_Day_time_step, vars_hour_argu,Hour_time_step, vars_static_argu, data, model, patch_size=16, stride=12,std = None,mean = None,time_batch_size=100, time_overlap=18, mode=None):\n",
    "    '''\n",
    "    先挨个将数据集从从左下开始划分，划分之后直接进行预测，填充回原数据集，\n",
    "    划分不完整的数据，最后重新从右上开始划分，全部预测后，只填充没有被划分到数据\n",
    "    '''\n",
    "    times, lat, lon = data.time.shape[0], data.lat.shape[0], data.lon.shape[0]\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    reconstructed_data = np.zeros((times, lat, lon))\n",
    "    time_shift = max(int(Day_time_step // 2), int(Half_Day_time_step // 2), int(Hour_time_step // 2))\n",
    "        # 从左下开始预测\n",
    "    for time_start in range(0, times, time_batch_size - time_overlap):\n",
    "        batch_start_time = time.time()  # 开始计时\n",
    "\n",
    "        # 偏移量计算\n",
    "        effective_start = time_start if time_start == 0 else time_start + time_shift\n",
    "        effective_end = time_end\n",
    "\n",
    "        time_end = min(time_start + time_batch_size, times)\n",
    "        data_batch = data.isel(time=slice(time_start, time_end)).to_array().compute().values  # channels, times, lat, lon\n",
    "        if mode is None:\n",
    "            # 从左下开始预测\n",
    "            for i in range(0, lat - patch_size + 1, stride):\n",
    "                for j in range(0, lon - patch_size + 1, stride):\n",
    "                    data_chunk = data_batch[:, :, i:i+patch_size, j:j+patch_size]  # channels, times, lat, lon\n",
    "                    day_sample, half_day_sample, hour_sample, static_sample = split_dataset_filling_faster(vars_index_map, vars_day_argu, Day_time_step, vars_half_day_argu, Half_Day_time_step, vars_hour_argu, Hour_time_step, vars_static_argu, data_chunk)\n",
    "                    output = batch_process(model, day_sample, half_day_sample, hour_sample, static_sample, batch_size=16, device=device, std=std, mean=mean)\n",
    "                    reconstructed_data[time_start:time_end, i:i+patch_size, j:j+patch_size] = calculate_overlap(reconstructed_data[time_start:time_end, i:i+patch_size, j:j+patch_size], output)\n",
    "                    reconstructed_data[effective_start:effective_end, i:i+patch_size, j:j+patch_size] = calculate_overlap(reconstructed_data[effective_start:effective_end, i:i+patch_size, j:j+patch_size], output, time_shift if time_start != 0 else 0)\n",
    "\n",
    "            # 纵向不完整处理\n",
    "            if lon % patch_size != 0:\n",
    "                for j in range(0, lat - patch_size + 1, stride):\n",
    "                    data_chunk = data_batch[:, :, j:j+patch_size, lon - patch_size:lon]\n",
    "                    day_sample, half_day_sample, hour_sample, static_sample = split_dataset_filling_faster(vars_index_map, vars_day_argu, Day_time_step, vars_half_day_argu, Half_Day_time_step, vars_hour_argu, Hour_time_step, vars_static_argu, data_chunk)\n",
    "                    output = batch_process(model, day_sample, half_day_sample, hour_sample, static_sample, batch_size=16, device=device, std=std, mean=mean)\n",
    "                    reconstructed_data[time_start:time_end, j:j+patch_size, lon - patch_size:lon] = calculate_overlap(reconstructed_data[time_start:time_end, j:j+patch_size, lon - patch_size:lon], output)\n",
    "            # 横向不完整处理\n",
    "            if lat % patch_size != 0:\n",
    "                for i in range(0, lon - patch_size + 1, stride):\n",
    "                    data_chunk = data_batch[:, :, lat - patch_size:lat, i:i+patch_size]\n",
    "                    day_sample, half_day_sample, hour_sample, static_sample = split_dataset_filling_faster(vars_index_map, vars_day_argu, Day_time_step, vars_half_day_argu, Half_Day_time_step, vars_hour_argu, Hour_time_step, vars_static_argu, data_chunk)\n",
    "                    output = batch_process(model, day_sample, half_day_sample, hour_sample, static_sample, batch_size=16, device=device, std=std, mean=mean)\n",
    "                    reconstructed_data[time_start:time_end, lat - patch_size:lat, i:i+patch_size] = calculate_overlap(reconstructed_data[time_start:time_end, lat - patch_size:lat, i:i+patch_size], output)\n",
    "            if lon % patch_size != 0 and lat % patch_size != 0:\n",
    "                data_chunk = data_batch[:, :, lat - patch_size:lat, lon - patch_size:lon]\n",
    "                day_sample, half_day_sample, hour_sample, static_sample = split_dataset_filling_faster(vars_index_map, vars_day_argu, Day_time_step, vars_half_day_argu, Half_Day_time_step, vars_hour_argu, Hour_time_step, vars_static_argu, data_chunk)\n",
    "                output = batch_process(model, day_sample, half_day_sample, hour_sample, static_sample, batch_size=16, device=device, std=std, mean=mean)\n",
    "                reconstructed_data[time_start:time_end, lat - patch_size:lat, lon - patch_size:lon] = calculate_overlap(reconstructed_data[time_start:time_end, lat - patch_size:lat, lon - patch_size:lon], output)    \n",
    "        elif mode == 'all':\n",
    "            day_sample, half_day_sample, hour_sample, static_sample = split_dataset_filling_faster(vars_index_map, vars_day_argu, Day_time_step, vars_half_day_argu, Half_Day_time_step, vars_hour_argu, Hour_time_step, vars_static_argu, data_batch)\n",
    "            output = batch_process(model, day_sample, half_day_sample, hour_sample, static_sample, batch_size=2, device=device, std=std, mean=mean)\n",
    "            reconstructed_data[time_start:time_end, :, :] = output\n",
    "        batch_end_time = time.time()  # 结束计时\n",
    "        print(f\"处理时间段 {time_start} 到 {time_end} 完成. 总耗时：{batch_end_time - batch_start_time:.2f}s\")\n",
    "        reconstructed_data = np.clip(reconstructed_data, 0, 1)\n",
    "        np.save(file_path, reconstructed_data)\n",
    "    return reconstructed_data\n",
    "\n",
    "def main(args):\n",
    "    models_config = Util.load_config(args.models_config_path)\n",
    "    train_config = models_config['train_shared_parameter'] \n",
    "    path_config = Util.load_config(args.path_config_path)\n",
    "    dataloader_hyperparameter_config = Util.load_config(args.dataloader_hyperparameter_config_path)\n",
    "    vars_argu = Util.load_config(args.vars_config_path)\n",
    "    vars_day_argu = Util.load_config(args.vars_day_config_path)\n",
    "    vars_half_day_argu = Util.load_config(args.vars_half_day_config_path)\n",
    "    vars_hour_argu = Util.load_config(args.vars_hour_config_path)\n",
    "    vars_static_argu = Util.load_config(args.vars_static_config_path)\n",
    "    logger = setup_logger(path_config['log'],append=True)\n",
    "    var_list = list(vars_argu.keys())\n",
    "    return train_config, models_config, path_config, dataloader_hyperparameter_config, logger, var_list, vars_argu, vars_day_argu, vars_half_day_argu, vars_hour_argu, vars_static_argu\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--models_config_path', type=str, default='../config/models_config.yaml')\n",
    "    parser.add_argument('--path_config_path', type=str, default='../config/path_config.yaml')\n",
    "    parser.add_argument('--dataloader_hyperparameter_config_path', type=str, default='../config/dataloader_hyperparameter_config.yaml')\n",
    "    parser.add_argument('--vars_config_path', type=str, default='../config/construct_Data/vars_config.yaml')\n",
    "    parser.add_argument('--vars_day_config_path', type=str, default='../config/construct_Data/vars_day_config.yaml')\n",
    "    parser.add_argument('--vars_half_day_config_path', type=str, default='../config/construct_Data/vars_half_day_config.yaml')\n",
    "    parser.add_argument('--vars_hour_config_path', type=str, default='../config/construct_Data/vars_hour_config.yaml')\n",
    "    parser.add_argument('--vars_static_config_path', type=str, default='../config/construct_Data/vars_static_config.yaml')\n",
    "    args = parser.parse_known_args()[0]\n",
    "\n",
    "    train_dict, models_config, path_config, dataloader_hyperparameter_config, logger, var_list, vars_argu, vars_day_argu, vars_half_day_argu, vars_hour_argu, vars_static_argu = main(args)\n",
    "    Util.random_seed(seed=train_dict['seed'])\n",
    "    data = reorder_variables(path_config['0.37_normal_Data'], r\"/root/ST-Conv/config/construct_Data/vars_config.yaml\")\n",
    "    vars_index_map = {var: i for i, var in enumerate(data.data_vars)}\n",
    "    # 解包model_config\n",
    "    experiment_groups = models_config['experiment_groups']      # 实验组参数-实验组共享参数+实验组模型参数\n",
    "    std = np.load(\"/root/autodl-fs/ST_Data/std_mean/std.npy\")\n",
    "    mean = np.load(\"/root/autodl-fs/ST_Data/std_mean/mean.npy\")\n",
    "\n",
    "    for group in experiment_groups:\n",
    "        if group['group_name'] == \"Experiment Group 7\":\n",
    "            print(f\"Running experiments for group: {group['group_name']}\")\n",
    "            shared_parameters = models_config['Shared_parameter'] \n",
    "            experiment_shared_params = group.get('experiment_shared_parameter', {})\n",
    "\n",
    "            # 训练实验组中每个模型\n",
    "            for model_config in group['models']:\n",
    "                model = STConvNet(**shared_parameters, **experiment_shared_params, **model_config['parameters'])\n",
    "                device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "                model = model.to(device)\n",
    "                if torch.cuda.device_count() > 1:\n",
    "                    print(f\"Let's use {torch.cuda.device_count()} GPUs!\")\n",
    "                model = nn.DataParallel(model)\n",
    "                # 设置优化器\n",
    "                optimizer = optim.AdamW(model.parameters(), lr=0, weight_decay=0)  # 优化器\n",
    "                Util.load_model_and_optimizer(model, optimizer, device, experiment_shared_params['best_model_path'], logger=logger, model_index=-1,model_name=model_config['model_name'])\n",
    "                file_path = os.path.join(path_config['reconstruct_data_path'], model_config['model_name'] + '16.npy')\n",
    "                reconstructed_data = reconstruct_data(file_path, vars_index_map, vars_day_argu, shared_parameters['day_step'], vars_half_day_argu, shared_parameters['half_day_step'], vars_hour_argu,shared_parameters['hour_step'], vars_static_argu, data, model, patch_size=16, stride=16, std = std[0],mean = mean[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from models.block import FFN\n",
    "class TemporalDilatedConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, padding, dilation):\n",
    "        super(TemporalDilatedConvBlock, self).__init__()\n",
    "        self.expand_ratio = 6\n",
    "        self.conv_time = nn.Sequential(nn.Conv3d(in_channels, out_channels, kernel_size=(kernel_size, 1, 1), padding=(padding, 0, 0), dilation=(dilation, 1, 1)),\n",
    "                                    nn.GELU())\n",
    "        self.conv1 = nn.Sequential(nn.Conv3d(out_channels, out_channels*self.expand_ratio, kernel_size=(1, 1, 1)),\n",
    "                                   nn.GELU())\n",
    "        self.conv2 = nn.Sequential(nn.Conv3d(out_channels*self.expand_ratio, out_channels, kernel_size=(1, 1, 1)),\n",
    "                                   nn.GELU())\n",
    "    def forward(self, x):\n",
    "        x = self.conv_time(x)\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        return x\n",
    "\n",
    "class TemporalDilatedConv(nn.Module):\n",
    "    def __init__(self, hour_in_channels,hour_step,day_in_channels,day_step,half_day_in_channels,half_day_step,static_in_channels, kernel_size, target_time_steps,temporal_usage_max_avg=True,temporal_usage_position_embedding=True,reduction_ratio=8):\n",
    "        super(TemporalDilatedConv, self).__init__()\n",
    "        # 时间维度的卷积核大小为2，不使用填充\n",
    "        self.expand_ratio = 6 \n",
    "        self.temporal_usage_max_avg = temporal_usage_max_avg\n",
    "        self.temporal_usage_position_embedding = temporal_usage_position_embedding\n",
    "        self.target_time_steps = target_time_steps\n",
    "        if temporal_usage_position_embedding == True:\n",
    "            hour_in_channels -= 1\n",
    "            day_in_channels -= 1\n",
    "            half_day_in_channels -= 1\n",
    "            static_in_channels -= 2\n",
    "\n",
    "        self.hour_block = self._create_block(hour_in_channels, hour_in_channels, hour_step, kernel_size)\n",
    "        self.day_block = self._create_block(day_in_channels, day_in_channels, day_step, kernel_size)\n",
    "        self.half_day_block = self._create_block(half_day_in_channels, half_day_in_channels, half_day_step, kernel_size)  # 数据输入通道只有一个\n",
    "\n",
    "        self.out_channels = hour_in_channels + day_in_channels + half_day_in_channels \n",
    "\n",
    "        self.final_conv = FFN(self.out_channels, self.out_channels, num_layers=6, expansion_ratio=6)\n",
    "\n",
    "        self.out_channels = self.out_channels  + static_in_channels + self.extra_channel if temporal_usage_max_avg == True else self.out_channels + static_in_channels\n",
    "        \n",
    "        self.conv_2d = nn.Sequential(nn.Conv2d(self.out_channels, self.out_channels*self.expand_ratio, kernel_size=1),\n",
    "                                 nn.GELU(),\n",
    "                                 nn.Conv2d(self.out_channels*self.expand_ratio, self.out_channels, kernel_size=1))\n",
    "        \n",
    "    def _create_block(self, in_channels, out_channels, time_steps, kernel_size):\n",
    "        layers = []\n",
    "        current_time_steps = time_steps\n",
    "        dilation = 1\n",
    "        while current_time_steps > self.target_time_steps:\n",
    "            required_reduction = current_time_steps - self.target_time_steps\n",
    "            adjusted_kernel_size = required_reduction + 1 if required_reduction + 1 < kernel_size else kernel_size\n",
    "            padding = 0\n",
    "\n",
    "            layers.append(TemporalDilatedConvBlock(in_channels, out_channels, adjusted_kernel_size, padding, dilation))\n",
    "            current_time_steps = current_time_steps - dilation * (adjusted_kernel_size - 1)\n",
    "            in_channels = out_channels\n",
    "            dilation *= 2\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def process_temporal_data(self, input_data):\n",
    "        processed_data = input_data[:, :-1, :, :, :]\n",
    "        # 将时间通道加到其他所有通道上#time_channel = input_data[:, -1:, :, :, :]\n",
    "        processed_data += input_data[:, -1:, :, :, :]\n",
    "\n",
    "        return processed_data\n",
    "\n",
    "    def process_static_data(self, static_input, hour_input, day_input, half_day_input):\n",
    "        # 提取经度和纬度通道\n",
    "        lon = static_input[:, -2:-1, :, :]  # [batch_size, 1, height, width]\n",
    "        lat = static_input[:, -1:, :, :]    # [batch_size, 1, height, width]\n",
    "\n",
    "        # 从原始数据中移除经度和纬度通道\n",
    "        processed_static = static_input[:, :-2, :, :]\n",
    "\n",
    "        # 扩展经度和纬度\n",
    "        lon_expanded_hour = lon.unsqueeze(2).expand(-1, -1, hour_input.shape[2], -1, -1)\n",
    "        lat_expanded_hour = lat.unsqueeze(2).expand(-1, -1, hour_input.shape[2], -1, -1)\n",
    "\n",
    "        lon_expanded_day = lon.unsqueeze(2).expand(-1, -1, day_input.shape[2], -1, -1)\n",
    "        lat_expanded_day = lat.unsqueeze(2).expand(-1, -1, day_input.shape[2], -1, -1)\n",
    "\n",
    "        lon_expanded_half_day = lon.unsqueeze(2).expand(-1, -1, half_day_input.shape[2], -1, -1)\n",
    "        lat_expanded_half_day = lat.unsqueeze(2).expand(-1, -1, half_day_input.shape[2], -1, -1)\n",
    "\n",
    "        # 将经度和纬度加到其他所有通道上\n",
    "        hour_input += lon_expanded_hour + lat_expanded_hour\n",
    "        day_input += lon_expanded_day + lat_expanded_day\n",
    "        half_day_input += lon_expanded_half_day + lat_expanded_half_day\n",
    "\n",
    "        return processed_static, hour_input, day_input, half_day_input\n",
    "\n",
    "    def get_out_channels(self):\n",
    "        return self.out_channels\n",
    "\n",
    "    def forward(self, hour_input, day_input, half_day_input, static_input):\n",
    "        if self.temporal_usage_position_embedding == True:\n",
    "\n",
    "            # 绝对时间编码\n",
    "            hour_input = self.process_temporal_data(hour_input)\n",
    "            day_input = self.process_temporal_data(day_input)\n",
    "            half_day_input = self.process_temporal_data(half_day_input)\n",
    "\n",
    "            # 绝对位置编码\n",
    "            static_input,hour_input,day_input,half_day_input = self.process_static_data(static_input,hour_input,day_input,half_day_input)\n",
    "\n",
    "        hour_output = self.hour_block(hour_input)\n",
    "        day_output = self.day_block(day_input)\n",
    "        half_day_output = self.half_day_block(half_day_input)\n",
    "        \n",
    "        combined_output = torch.cat((hour_output, day_output, half_day_output), dim=1)\n",
    "        final_output = self.final_conv(combined_output)\n",
    "        final_output = final_output.squeeze(2)\n",
    "\n",
    "        final_output = torch.cat((final_output, static_input), dim=1)\n",
    "        final_outconv = self.conv_2d(final_output)\n",
    "        return final_outconv\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from models.block import FFN\n",
    "\n",
    "\n",
    "class SqueezeLayer(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super(SqueezeLayer, self).__init__()\n",
    "        self.dim = dim\n",
    "    def forward(self, x):\n",
    "        return x.squeeze(self.dim)\n",
    "    \n",
    "class TemporalDilatedConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, padding, dilation):\n",
    "        super(TemporalDilatedConvBlock, self).__init__()\n",
    "        self.expand_ratio = 6\n",
    "        self.conv_first = nn.Sequential(nn.Conv3d(in_channels, out_channels*self.expand_ratio, kernel_size=(kernel_size, 1, 1), padding=(padding, 0, 0), dilation=(dilation, 1, 1)),\n",
    "                                    nn.GELU())\n",
    "        self.conv1= nn.Sequential(nn.Conv3d(out_channels*self.expand_ratio, out_channels, kernel_size=(1, 1, 1)),\n",
    "                                nn.GELU())\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x_initial = self.conv_first(x)\n",
    "        x = self.conv1(x_initial)\n",
    "        return x\n",
    "\n",
    "class TemporalDilatedConv(nn.Module):\n",
    "    def __init__(self, hour_in_channels,hour_step,day_in_channels,day_step,half_day_in_channels,half_day_step,static_in_channels, kernel_size, target_time_steps,temporal_usage_max_avg=True,temporal_usage_position_embedding=True,reduction_ratio=8):\n",
    "        super(TemporalDilatedConv, self).__init__()\n",
    "        # 时间维度的卷积核大小为2，不使用填充\n",
    "        self.expand_ratio = 6 \n",
    "        self.temporal_usage_max_avg = temporal_usage_max_avg\n",
    "        self.temporal_usage_position_embedding = temporal_usage_position_embedding\n",
    "        self.target_time_steps = target_time_steps\n",
    "        if temporal_usage_position_embedding == True:\n",
    "            hour_in_channels -= 1\n",
    "            day_in_channels -= 1\n",
    "            half_day_in_channels -= 1\n",
    "            static_in_channels -= 2\n",
    "        self.num_layers = 3\n",
    "        self.hour_blocks = self._create_block(hour_in_channels, hour_in_channels, hour_step, kernel_size, num_layers = self.num_layers)\n",
    "        self.day_blocks = self._create_block(day_in_channels, day_in_channels, day_step, kernel_size, num_layers = self.num_layers)\n",
    "        self.half_day_blocks = self._create_block(half_day_in_channels, half_day_in_channels, half_day_step, kernel_size, num_layers = self.num_layers)  # 数据输入通道只有一个\n",
    "        self.out_channels = hour_in_channels + day_in_channels + half_day_in_channels + static_in_channels\n",
    "\n",
    "        self.final_conv = FFN(self.out_channels, self.out_channels, num_layers=4, expansion_ratio=6)\n",
    "\n",
    "    def _create_block(self, in_channels, out_channels, time_steps, kernel_size, num_layers):\n",
    "        TCN_layers = []\n",
    "        current_time_steps = time_steps\n",
    "        dilation = 1\n",
    "        layer_count = 0\n",
    "        while current_time_steps > self.target_time_steps or layer_count < num_layers:\n",
    "            padding = (kernel_size - 1) * dilation // 2 if layer_count < num_layers else 0\n",
    "            TCN_layers.append(TemporalDilatedConvBlock(in_channels, out_channels, kernel_size, padding, dilation))\n",
    "\n",
    "            current_time_steps = max(current_time_steps - dilation * (kernel_size - 1), 1) if layer_count >= num_layers else time_steps-1\n",
    "            layer_count += 1\n",
    "            dilation = 1 if layer_count == num_layers else dilation * 2\n",
    "            \n",
    "        if current_time_steps > 1:\n",
    "            TCN_layers.append(nn.Sequential(nn.Conv3d(in_channels, out_channels, kernel_size=(current_time_steps, 1, 1),padding=(0, 0, 0)),\n",
    "                                            nn.GELU()))\n",
    "        return TCN_layers\n",
    "\n",
    "    def process_temporal_data(self, input_data):\n",
    "        processed_data = input_data[:, :-1, :, :, :]\n",
    "        # 将时间通道加到其他所有通道上#time_channel = input_data[:, -1:, :, :, :]\n",
    "        processed_data += input_data[:, -1:, :, :, :]\n",
    "\n",
    "        return processed_data\n",
    "\n",
    "    def process_static_data(self, static_input, hour_input, day_input, half_day_input):\n",
    "        # 提取经度和纬度通道\n",
    "        lon = static_input[:, -2:-1, :, :]  # [batch_size, 1, height, width]\n",
    "        lat = static_input[:, -1:, :, :]    # [batch_size, 1, height, width]\n",
    "\n",
    "        # 从原始数据中移除经度和纬度通道\n",
    "        processed_static = static_input[:, :-2, :, :]\n",
    "\n",
    "        # 扩展经度和纬度\n",
    "        lon_expanded_hour = lon.unsqueeze(2).expand(-1, -1, hour_input.shape[2], -1, -1)\n",
    "        lat_expanded_hour = lat.unsqueeze(2).expand(-1, -1, hour_input.shape[2], -1, -1)\n",
    "\n",
    "        lon_expanded_day = lon.unsqueeze(2).expand(-1, -1, day_input.shape[2], -1, -1)\n",
    "        lat_expanded_day = lat.unsqueeze(2).expand(-1, -1, day_input.shape[2], -1, -1)\n",
    "\n",
    "        lon_expanded_half_day = lon.unsqueeze(2).expand(-1, -1, half_day_input.shape[2], -1, -1)\n",
    "        lat_expanded_half_day = lat.unsqueeze(2).expand(-1, -1, half_day_input.shape[2], -1, -1)\n",
    "\n",
    "        # 将经度和纬度加到其他所有通道上\n",
    "        hour_input += lon_expanded_hour + lat_expanded_hour\n",
    "        day_input += lon_expanded_day + lat_expanded_day\n",
    "        half_day_input += lon_expanded_half_day + lat_expanded_half_day\n",
    "\n",
    "        return processed_static, hour_input, day_input, half_day_input\n",
    "\n",
    "    def get_out_channels(self):\n",
    "        return self.out_channels\n",
    "    \n",
    "    def apply_layers(self, layers, x):\n",
    "        for layer in layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "    \n",
    "    def forward(self, hour_input, day_input, half_day_input, static_input):\n",
    "        if self.temporal_usage_position_embedding == True:\n",
    "\n",
    "            # 绝对时间编码\n",
    "            hour_input = self.process_temporal_data(hour_input)\n",
    "            day_input = self.process_temporal_data(day_input)\n",
    "            half_day_input = self.process_temporal_data(half_day_input)\n",
    "\n",
    "            # 绝对位置编码\n",
    "            static_input,hour_input,day_input,half_day_input = self.process_static_data(static_input,hour_input,day_input,half_day_input)\n",
    "\n",
    "        hour_output = self.apply_layers(self.hour_blocks, hour_input, mode='TCN')\n",
    "        day_output = self.apply_layers(self.day_blocks, day_input, mode='TCN')\n",
    "        half_day_output = self.general_layers(self.half_day_blocks, half_day_input, mode='TCN')\n",
    "    \n",
    "\n",
    "        combined_out = torch.cat([hour_concat,day_concat,half_day_concat,static_input], dim=1)\n",
    "        final_out = self.final_conv(combined_out)\n",
    "\n",
    "        return final_out\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
